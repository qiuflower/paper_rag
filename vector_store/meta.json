[
  "The Model Context Protocol (MCP) is a standardized interface designed to enable seamless interaction between\nAI models and external tools and resources, breaking down data silos and facilitating interoperability across\ndiverse systems. This paper provides a comprehensive overview of MCP, focusing on its core components,\nworkflow, and the lifecycle of MCP servers, which consists of three key phases: creation, operation, and update.\nWe analyze the security and privacy risks associated with each phase and propose strategies to mitigate\npotential threats. The paper also examines the current MCP landscape, including its adoption by industry\nleaders and various use cases, as well as the tools and platforms supporting its integration. We explore future\ndirections for MCP, highlighting the challenges and opportunities that will influence its adoption and evolution\nwithin the broader AI ecosystem. Finally, we offer recommendations for MCP stakeholders to ensure its secure\nand sustainable development as the AI landscape continues to evolve.\n\nCCS Concepts: • **General and reference** → **Surveys and overviews** ; • **Security and privacy** → **Software**\n**and application security** ; • **Computing methodologies** → **Artificial intelligence** .\n\nAdditional Key Words and Phrases: Model Context Protocol, MCP, Vision paper, Security\n\n**ACM Reference Format:**\n\nXinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. 2025. Model Context Protocol (MCP): Landscape,\n[Security Threats, and Future Research Directions. 1, 1 (April 2025), 20 pages. https://doi.org/10.1145/nnnnnnn.](https://doi.org/10.1145/nnnnnnn.nnnnnnn)\n\n[nnnnnnn](https://doi.org/10.1145/nnnnnnn.nnnnnnn)\n\n**1** **INTRODUCTION**\n\nIn recent years, the vision of autonomous AI agents capable of interacting with a wide range of\ntools and data sources has gained significant momentum. This progress accelerated in 2023 with the\nintroduction of **function calling** by OpenAI, which allowed language models to invoke external\nAPIs in a structured way [ 38 ]. This advancement expanded the capabilities of LLMs, enabling\nthem to retrieve real-time data, perform computations, and interact with external systems. As\nfunction calling gained adoption, an ecosystem formed around it. OpenAI introduced the **ChatGPT**\n**plugin** [ 37 ], allowing developers to build callable tools for ChatGPT. LLM app stores such as Coze [ 4 ]\nand Yuanqi [ 50 ] have launched their **plugin stores**, supporting tools specifically designed for\n\n∗ Haoyu Wang is the corresponding author (haoyuwang@hust.edu.cn).\n\nAuthors’ addresses: Xinyi Hou, xinyihou@hust.edu.cn, Huazhong University of Science and Technology, Wuhan, China;\nYanjie Zhao, yanjie_zhao@hust.edu.cn, Huazhong University of Science and Technology, Wuhan, China; Shenao Wang,\nshenaowang@hust.edu.cn, Huazhong University of Science and Technology, Wuhan, China; Haoyu Wang, haoyuwang@\nhust.edu.cn, Huazhong University of Science and Technology, Wuhan, China.\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM XXXX-XXXX/2025/4-ART\n\n[https://doi.org/10.1145/nnnnnnn.nnnnnnn](https://doi.org/10.1145/nnnnnnn.nnnnnnn)\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\n2 X Hou, Y Zhao, S Wang, and H Wang\n\ntheir platforms. Frameworks like LangChain [ 26 ] and LlamaIndex [ 29 ] provided standardized **tool**\n**interfaces**, making it easier to integrate LLMs with external services. Other AI providers, including\nAnthropic, Google, and Meta, introduced similar mechanisms, further driving adoption. Despite\nthese advancements, **integrating tools remains fragmented** . Developers must manually define\ninterfaces, manage authentication, and handle execution logic for each service. Function calling\nmechanisms vary across platforms, requiring redundant implementations. Additionally, current\napproaches rely on **predefined workflows, limiting AI agents’ flexibility in dynamically**\n**discovering and orchestrating tools** .\nIn late 2024, Anthropic introduced the Model Context Protocol (MCP)[ 3 ], a general-purpose protocol standardizing AI-tool interactions. Inspired by the Language Server Protocol (LSP) [ 22 ], MCP\nprovides a flexible framework for AI applications to communicate with external tools dynamically.\nInstead of relying on predefined tool mappings, MCP allows AI agents to autonomously discover,\nselect, and orchestrate tools based on task context. It also supports human-in-the-loop mechanisms,\nenabling users to inject data or approve actions as needed. By unifying interfaces, MCP simplifies\nthe development of AI applications and improves their flexibility in handling complex workflows.\nSince its release, MCP has rapidly grown from a niche protocol to a key foundation for AI-native\napplication development. A thriving ecosystem has emerged, with thousands of community-driven\nMCP servers enabling model access to systems like GitHub [ 41 ], Slack [ 42 ], and even 3D design tools\nlike Blender [ 1 ]. Tools like Cursor [ 12 ] and Claude Desktop [ 2 ] demonstrate how MCP clients can\nextend their capabilities by installing new servers, turning developer tools, productivity platforms,\nand creative environments alike into multi-modal AI agents.\nDespite the rapid adoption of MCP, its ecosystem is still in the early stages, with key areas such\nas security, tool discoverability, and remote deployment lacking comprehensive solutions. These\nissues present untapped opportunities for further research and development. Although MCP is\nwidely recognized for its potential in the industry, it has not yet been extensively analyzed in\nacademic research. This gap in research motivates this paper, which provides the first analysis of\nthe MCP ecosystem, examining its architecture and workflow, defining the lifecycle of MCP servers,\nand identifying potential security risks at each stage, such as installer spoofing and tool name\nconflict. Through this study, we present a thorough exploration of MCP’s current landscape and\noffer a forward-looking vision that highlights key implications, outlines future research directions,\nand addresses the challenges that must be overcome to ensure its sustainable growth.\n**Our contributions are as follows:**\n\n(1) We provide the first analysis of the MCP ecosystem, detailing its architecture, components,\nand workflow.\n\n(2) We identify the key components of MCP servers and define their lifecycle, encompassing the\nstages of creation, operation, and update. We also highlight potential security risks associated\nwith each phase, offering insights into safeguarding AI-to-tool interactions.\n(3) We examine the current MCP ecosystem landscape, analyzing the adoption, diversity, and\nuse cases across various industries and platforms.\n(4) We discuss the implications of MCP’s rapid adoption, identifying key challenges for stakeholders, and outline future research directions on security, scalability, and governance to\nensure its sustainable growth.\n\nThe remainder of this paper is structured as follows: § 2 compares tool invocation with and\nwithout MCP, highlighting the motivation for this study. § 3 outlines the architecture of MCP,\ndetailing the roles of the MCP host, client, and server, as well as the lifecycle of the MCP server.\n§ 4 examines the current MCP landscape, focusing on key industry players and adoption trends.\n§ 5 analyzes security and privacy risks across the MCP server lifecycle and proposes mitigation\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\nModel Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions 3\n\nstrategies. § 6 explores implications, future challenges, and recommendations to enhance MCP’s\nscalability and security in dynamic AI environments. § 7 reviews prior work on tool integration\nand security in LLM applications. Finally, § 8 concludes the whole paper.\n\n**2** **BACKGROUND AND MOTIVATION**\n\n**2.1** **AI Tooling**\n\nBefore the introduction of MCP, AI applications relied on various methods, such as manual API\nwiring, plugin-based interfaces, and agent frameworks, to interact with external tools. As shown in\nFigure 1, these approaches required integrating each external service with a specific API, leading to\nincreased complexity and limited scalability. **MCP addresses these challenges by providing a**\n**standardized protocol that enables seamless and flexible interaction with multiple tools.**\n\n\n\n\n**vs.**\n\n\n\n\n\n\nFig. 1. Tool invocation with and without MCP.\n\n*2.1.1* *Manual API Wiring.* In traditional implementations, developers had to establish manual API\nconnections for each tool or service that an AI application interacted with. This process **required**\n**custom authentication, data transformation, and error handling for every integration** . As\nthe number of APIs increased, the maintenance burden became significant, often leading to tightly\ncoupled and fragile systems that were difficult to scale or modify. MCP eliminates this complexity\nby offering a unified interface, allowing AI models to connect with multiple tools dynamically\nwithout the need for custom API wiring.\n\n*2.1.2* *Standardized Plugin Interfaces.* To reduce the complexity of manual wiring, plugin-based\ninterfaces such as OpenAI ChatGPT Plugins, introduced in November 2023 [ 37 ], allowed AI models\nto connect with external tools through standardized API schemas like OpenAPI. For example, in\nthe OpenAI Plugin ecosystem, plugins like Zapier allowed models to perform predefined actions,\nsuch as sending emails or updating CRM records. However, these interactions were often **one-**\n**directional and could not maintain state or coordinate multiple steps in a task** . New LLM\napp stores [ 62 ] such as ByteDance Coze [ 4 ] and Tencent Yuanqi [ 50 ] have also emerged, offering a\nplugin store for web services. While these platforms expanded available tool options, they created\nisolated ecosystems where plugins are **platform-specific**, limiting cross-platform compatibility\nand requiring duplicate maintenance efforts. MCP stands out by being open-source and platformagnostic, enabling AI applications to engage in rich two-way interactions with external tools,\nfacilitating complex workflows.\n\n*2.1.3* *AI Agent Tool Integration.* The emergence of AI agent frameworks like LangChain [ 26 ] and\nsimilar tool orchestration frameworks provided a structured way for models to invoke external tools\nthrough predefined interfaces, improving automation and adaptability [ 55 ]. However, integrating\nand maintaining these tools remained largely manual, requiring custom implementations and\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\n4 X Hou, Y Zhao, S Wang, and H Wang\n\nincreasing complexity as the number of tools grew. MCP simplifies this process by **offering a**\n**standardized protocol that enables AI agents to seamlessly invoke, interact with, and**\n**chain multiple tools through a unified interface** . This reduces manual configuration and\nenhances task flexibility, allowing agents to perform complex operations without extensive custom\nintegration.\n\n*2.1.4* *Retrieval-Augmented Generation (RAG) and Vector Database.* Contextual information retrieval\nmethods, such as RAG, leverage vector-based search to retrieve relevant knowledge from databases\nor knowledge bases, enabling models to supplement responses with up-to-date information [ 11, 16 ].\nWhile this approach addressed the problem of knowledge cutoff and improved model accuracy, it\nwas limited to **passive retrieval of information** . It did not inherently allow models to perform\nactive operations, such as modifying data or triggering workflows. For example, a RAG-based\nsystem could retrieve relevant sections from a product documentation database to assist a customer\nsupport AI. However, if the AI needed to update customer records or escalate an issue to human\nsupport, it could not take action beyond providing textual responses. MCP extends beyond passive\ninformation retrieval by enabling AI models to interact with external data sources and tools actively,\nfacilitating both retrieval and action in a unified workflow.\n\n**2.2** **Motivation**\n\nMCP has rapidly gained traction in the AI community due to its ability to standardize how AI\nmodels interact with external tools, fetch data, and execute operations. By addressing the limitations\nof manual API wiring, plugin interfaces, and agent frameworks, MCP has the potential to redefine\nAI-to-tool interactions and enable more autonomous and intelligent agent workflows. Despite\nits growing adoption and promising potential, MCP is still in its early stages, with an evolving\necosystem that remains incomplete. Many key aspects, such as security and tool discoverability,\nare yet to be fully addressed, leaving ample room for future research and improvement. Moreover,\nwhile MCP has gained rapid adoption in the industry, it is still largely unexplored in academia.\nMotivated by this gap, this paper is **the first to analyze the current MCP landscape, examine**\n**its emerging ecosystem, and identify potential security risks** . Additionally, we outline a\nvision for MCP’s future development and highlight the key challenges that must be addressed to\nsupport its long-term success.\n\n**3** **MCP ARCHITECTURE**\n\n**3.1** **Core Components**\n\nThe MCP architecture is composed of three core components: ***MCP host***, ***MCP client***, and ***MCP***\n***server*** . These components collaborate to facilitate seamless communication between AI applications,\nexternal tools, and data sources, ensuring that operations are secure and properly managed. As\nshown in Figure 2, in a typical workflow, the user sends a prompt to the MCP client, which **analyzes**\n**the intent**, **selects the appropriate tools** via the MCP server, and **invokes external APIs** to\nretrieve and process the required information before **notifying** the user of the results.\n\n*3.1.1* *MCP Host.* The MCP host is an AI application that provides the environment for executing\nAI-based tasks while running the MCP client. It integrates interactive tools and data to enable\nsmooth communication with external services. Examples include Claude Desktop for AI-assisted\ncontent creation, Cursor, an AI-powered IDE for code completion and software development, and\nAI agents that function as autonomous systems for executing complex tasks. The MCP host hosts\nthe MCP client and ensures communication with external MCP servers.\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\nModel Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 2. The workflow of MCP.\n\n*3.1.2* *MCP Client.* The MCP client acts as an intermediary within the host environment, managing\ncommunication between the MCP host and one or more MCP servers. It initiates requests to MCP\nservers, queries available functions, and retrieves responses that describe the server’s capabilities.\nThis ensures seamless interaction between the host and external tools. In addition to managing\nrequests and responses, the MCP client processes **notifications** from MCP servers, providing\nreal-time updates about task progress and system status. It also performs **sampling** to gather data\non tool usage and performance, enabling optimization and informed decision-making. The MCP\nclient communicates through the transport layer with MCP servers, facilitating secure, reliable\ndata exchange and smooth interaction between the host and external resources.\n\n*3.1.3* *MCP Server.* The MCP server enables the MCP host and client to access external systems\nand execute operations, offering three core capabilities: ***tools, resources, and prompts*** .\n\n- **Tools: Enabling external operations** . Tools allow the MCP server to invoke external services\nand APIs to execute operations on behalf of AI models. When the client requests an operation, the\nMCP server identifies the appropriate tool, interacts with the service, and returns the result. For\ninstance, if an AI model requires real-time weather data or sentiment analysis, the MCP server\nconnects to the relevant API, retrieves the data, and delivers it to the host. Unlike traditional\nfunction calling, which requires multiple steps and separates invocation from execution, Tools of\nMCP servers streamline this process by allowing the model to autonomously select and invoke\nthe appropriate tool based on context. Once configured, these tools follow a standardized supplyand-consume model, making them modular, reusable, and easily accessible to other applications,\nenhancing system efficiency and flexibility.\n\n- **Resources: Exposing data to AI models** . Resources provide access to structured and unstructured datasets that the MCP server can expose to AI models. These datasets may come from\nlocal storage, databases, or cloud platforms. When an AI model requests specific data, the MCP\nserver retrieves and processes the relevant information, enabling the model to make data-driven\ndecisions. For example, a recommendation system may access customer interaction logs, or a\ndocument summarization task may query a text repository.\n\n- **Prompts: Reusable templates for workflow optimization** . Prompts are predefined templates\nand workflows that the MCP server generates and maintains to optimize AI responses and\nstreamline repetitive tasks. They ensure consistency in responses and improve task execution\nefficiency. For instance, a customer support chatbot may use prompt templates to provide uniform\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\n6 X Hou, Y Zhao, S Wang, and H Wang\n\nand accurate responses, while an annotation task may rely on predefined prompts to maintain\nconsistency in data labeling.\n\n**3.2** **Transport Layer and Communication**\n\nThe transport layer ensures secure, bidirectional communication, allowing for real-time interaction\nand efficient data exchange between the host environment and external systems. The transport\nlayer manages the transmission of initial requests from the client, the delivery of server responses\ndetailing available capabilities, and the exchange of notifications that keep the client informed of\nongoing updates. Communication between the MCP client and the MCP server follows a structured\nprocess, beginning with an **initial request** from the client to query the server’s functionalities.\nUpon receiving the request, the server responds with an **initial response** listing the available\ntools, resources, and prompts the client can leverage. Once the connection is established, the\nsystem maintains a continuous exchange of **notifications** to ensure that changes in server status\nor updates are communicated back to the client in real time. This structured communication\nensures high-performance interactions and keeps AI models synchronized with external resources,\nenhancing the effectiveness of AI applications.\n\n**3.3** **MCP Server Lifecycle**\n\nThe MCP server lifecycle as shown in Figure 3 consists of three key phases: ***creation***, ***operation***,\nand ***update*** . Each phase defines critical activities that ensure the secure and efficient functioning\nof the MCP server, enabling seamless interaction between AI models and external tools, resources,\nand prompts.\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 3. MCP servers components and lifecycle.\n\n*3.3.1* *MCP Server Components.* The MCP server is responsible for managing external tools, data\nsources, and workflows, providing AI models with the necessary resources to perform tasks\nefficiently and securely. It comprises several key components that ensure smooth and effective\noperations. **Metadata** includes essential information about the server, such as its name, version, and\ndescription, allowing clients to identify and interact with the appropriate server. **Configuration**\ninvolves the source code, configuration files, and manifest, which define the server’s operational\nparameters, environment settings, and security policies. **Tool list** stores a catalog of available tools,\ndetailing their functionalities, input-output formats, and access permissions, ensuring proper tool\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\nModel Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions 7\n\nmanagement and security. **Resources list** governs access to external data sources, including web\nAPIs, databases, and local files, specifying allowed endpoints and their associated permissions.\nFinally, **Prompts and Templates** include pre-configured task templates and workflows that\nenhance the efficiency of AI models in executing complex operations. Together, these components\nenable MCP servers to provide seamless tool integration, data retrieval, and task orchestration for\nAI-powered applications.\n\n*3.3.2* *Creation Phase.* The creation phase is the initial stage of the MCP server lifecycle, where the\nserver is registered, configured, and prepared for operation. This phase involves three key steps.\n**Server registration** assigns a unique name and identity to the MCP server, allowing clients to\ndiscover and connect to the appropriate server instance. **Installer deployment** involves installing\nthe MCP server and its associated components, ensuring that the correct configuration files, source\ncode, and manifests are in place. **Code integrity verification** validates the integrity of the server’s\ncodebase to prevent unauthorized modifications or tampering before the server becomes operational.\nSuccessful completion of the creation phase ensures that the MCP server is ready to handle requests\nand interact securely with external tools and data sources.\n\n*3.3.3* *Operation Phase.* The operation phase is where the MCP server actively processes requests,\nexecutes tool invocations, and facilitates seamless interaction between AI applications and external\nresources. **Tool execution** allows the MCP server to invoke the appropriate tools based on the AI\napplication’s requests, ensuring that the selected tools perform their intended operations. **Slash**\n**command handling** enables the server to interpret and execute multiple commands, including\nthose issued through user interfaces or AI agents, while managing potential command overlaps to\nprevent conflicts. **Sandbox mechanism** enforcement ensures that the execution environment is\nisolated and secure, preventing unauthorized access and mitigating potential risks. Throughout the\noperation phase, the MCP server maintains a stable and controlled environment, enabling reliable\nand secure task execution.\n\n*3.3.4* *Update Phase.* The update phase ensures that the MCP server remains secure, up-to-date, and\ncapable of adapting to evolving requirements. This phase includes three key tasks. **Authorization**\n**management** verifies that post-update access permissions remain valid, preventing unauthorized\nuse of server resources after updates. **Version control** maintains consistency between different\nserver versions, ensuring that new updates do not introduce vulnerabilities or conflicts. **Old version**\n**management** deactivates or removes outdated versions to prevent attackers from exploiting known\nvulnerabilities in previous versions.\nUnderstanding the MCP server lifecycle is essential for identifying potential vulnerabilities\nand designing effective security measures. Each phase introduces distinct challenges that must\nbe carefully addressed to maintain the security, efficiency, and adaptability of the MCP server in\ndynamic AI environments.\n\n**4** **CURRENT LANDSCAPE**\n\n**4.1** **Ecosystem Overview**\n\n*4.1.1* *Key Adopters.* Table 1 demonstrates how MCP has gained significant traction across diverse\nsectors, signaling its growing importance in enabling seamless AI-to-tool interactions. Notably,\nleading AI companies such as Anthropic [ 2 ] and OpenAI [ 39 ] have integrated MCP to enhance\nagent capabilities and improve multi-step task execution. This adoption by industry pioneers\nhas set a precedent, encouraging other major players to follow suit. Chinese tech giants like\nBaidu [ 31 ] have also incorporated MCP into their ecosystems, highlighting the protocol’s potential\nto standardize AI workflows across global markets. Developer tools and IDEs, including Replit [ 43 ],\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\n8 X Hou, Y Zhao, S Wang, and H Wang\n\nTable 1. Overview of MCP ecosystem adoption.\n\n**Category** **Company/Product** **Key Features or Use Cases**\n\nAnthropic (Claude) [2] Full MCP support in the desktop version, enabling interaction with external tools.\n**AI Models and Frameworks** OpenAI [39] MCP support in Agent SDK and API for seamless integration.\nBaidu Maps [31] API integration using MCP to access geolocation services.\nBlender MCP [33] Enables Blender and Unity 3D model generation via natural language commands.\n\nReplit [43] AI-assisted development environment with MCP tool integration.\nMicrosoft Copilot Studio [49] Extends Copilot Studio with MCP-based tool integration.\n**Developer Tools** Sourcegraph Cody [10] Implements MCP through OpenCTX for resource integration.\nCodeium [9] Adds MCP support for coding assistants to facilitate cross-system tasks.\nCursor [12] MCP tool integration in Cursor Composer for seamless code execution.\nCline [7] VS Code coding agent that manages MCP tools and servers.\n\nZed [60] Provides slash commands and tool integration based on MCP.\nJetBrains [24] Integrates MCP for IDE-based AI tooling.\n**IDEs/Editors** Windsurf Editor [14] AI-assisted IDE with MCP tool interaction.\nTheiaAI/TheiaIDE [52] Enables MCP server interaction for AI-powered tools.\nEmacs MCP [32] Enhances AI functionality in Emacs by supporting MCP tool invocation.\nOpenSumi [40] Supports MCP tools in IDEs and enables seamless AI tool integration.\n\nCloudflare [8] Provides remote MCP server hosting and OAuth integration.\n**Cloud Platforms and Services** Block (Square) [47] Uses MCP to enhance data processing efficiency for financial platforms.\nStripe [48] Exposes payment APIs via MCP for seamless AI integration.\n\nApify MCP Tester [51] Connects to any MCP server using SSE for API testing.\n**Web Automation and Data** LibreChat [28] Extends the current tool ecosystem through MCP integration.\nGoose [21] Allows building AI agents with integrated MCP server functionality.\n\nMicrosoft Copilot Studio [ 49 ], JetBrains [ 24 ], and TheiaIDE [ 52 ], leverage MCP to facilitate agentic\nworkflows and streamline cross-platform operations. This trend indicates a shift toward embedding\nMCP in developer environments to enhance productivity and reduce manual integration efforts.\nFurthermore, cloud platforms like Cloudflare [ 8 ] and financial service providers such as Block\n(Square) [ 47 ] and Stripe [ 48 ] are exploring MCP to improve security, scalability, and governance\nin multi-tenant environments. The widespread adoption of MCP by these industry leaders not\nonly highlights its growing relevance but also points to its potential as a foundational layer in\nAI-powered ecosystems. As more companies integrate MCP into their operations, the protocol is\nset to play a central role in shaping the future of AI tool integration. Looking ahead, MCP is poised\nto become a key enabler of AI-driven workflows, driving more secure, scalable, and efficient AI\necosystems across industries.\n\n*4.1.2* *Community-Driven MCP Servers.* Anthropic has not yet released an official MCP marketplace,\nbut the vibrant MCP community has stepped in to fill this gap by creating numerous independent\nserver collections and platforms. As shown in Table 2, platforms such as MCP.so [ 35 ], Glama [ 20 ],\nand PulseMCP [ 15 ] host thousands of servers, allowing users to discover and integrate a wide\nrange of tools and services. These community-driven platforms have significantly accelerated the\nadoption of MCP by providing accessible repositories where developers can publish, manage, and\nshare their MCP servers. Desktop-based solutions like Dockmaster [ 34 ] and Toolbase [ 19 ] further\nenhance local MCP deployment capabilities, empowering developers to manage and experiment\nwith servers in isolated environments. The rise of community-driven MCP server ecosystems\nreflects the growing enthusiasm for MCP and highlights the need for a formalized marketplace.\n\n*4.1.3* *SDKs and Tools.* With the continuous growth of community-driven tools and official SDKs,\nthe MCP ecosystem is becoming increasingly accessible, allowing developers to integrate MCP into\nvarious applications and workflows efficiently. Official SDKs are available in multiple languages,\nincluding *TypeScript*, *Python*, *Java*, *Kotlin*, and *C#*, providing developers with versatile options\nto implement MCP in different environments. In addition to official SDKs, the community has\ncontributed numerous frameworks and utilities that simplify MCP server development. Tools such\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\nModel Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions 9\n\nTable 2. Overview of MCP server collections and deployment modes (As of March 27, 2025).\n\n**Collection** **Author** **Mode** **",
  "MCP.so mcp.so Website 4774 [mcp.so](https://mcp.so/)\nGlama glama.ai Website 3356 [glama.ai](https://glama.ai/mcp/servers)\nPulseMCP Antanavicius et al. Website 3164 [pulsemcp.com](https://www.pulsemcp.com)\nSmithery Henry Mao Website 2942 [smithery.ai](https://smithery.ai/)\nDockmaster mcp-dockmaster Desktop App 517 [mcp-dockmaster.com](https://mcp-dockmaster.com)\n**Official Collection** **Anthropic** **GitHub Repo** **320** [modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers)\nAiMCP Hekmon Website 313 [aimcp.info](https://www.aimcp.info)\nMCP.run mcp.run Website 114 [mcp.run](https://mcp.run)\nAwesome MCP Servers Stephen Akinyemi GitHub Repo 88 [appcypher/mcp-servers](https://github.com/appcypher/awesome-mcp-servers)\nmcp-get registry Michael Latman Website 59 [mcp-get.com](https://mcp-get.com)\nAwesome MCP Servers wong2 Website 34 [mcpservers.org](https://mcpservers.org)\nOpenTools opentoolsteam Website 25 [opentools.com](https://opentools.com)\nToolbase gching Desktop App 24 [gettoolbase.ai](https://gettoolbase.ai)\nmake inference mkinf Website 20 [mkinf.io](https://mkinf.io)\n\nAwesome Crypto MCP Servers Luke Fan GitHub Repo 13 [badkk/crypto-mcp-servers](https://github.com/badkk/awesome-crypto-mcp-servers)\n\nas *EasyMCP* and *FastMCP* offer lightweight TypeScript-based solutions for quickly building MCP\nservers, while *FastAPI to MCP Auto Generator* enables the seamless exposure of FastAPI endpoints\nas MCP tools. For more complex scenarios, *Foxy Contexts* provides a Golang-based library to\nbuild MCP servers, and *Higress MCP Server Hosting* extends the API Gateway (based on Envoy)\nto host MCP servers with wasm plugins. Server generation and management platforms such as\n*Mintlify*, *Speakeasy*, and *Stainless* further enhance the ecosystem by **automating MCP server**\n**generation**, providing curated MCP server lists, and enabling faster deployment with minimal\nmanual intervention. These platforms empower organizations to rapidly create and manage secure\nand well-documented MCP servers.\n\n**4.2** **Use Cases**\n\nMCP has become a vital tool for AI applications to effectively communicate with external tools,\nAPIs, and systems. By standardizing interactions, MCP simplifies complex workflows, boosting the\nefficiency of AI-driven applications. Below, we explore three key platforms (i.e., OpenAI, Cursor,\nand Cloudflare) that have successfully integrated MCP, highlighting their distinct use cases.\n\n*4.2.1* *OpenAI: MCP Integration in AI Agents and SDKs.* OpenAI has adopted MCP to standardize\nAI-to-tool communication, recognizing its potential to enhance integration with external tools.\nRecently, OpenAI introduced MCP support in its Agent SDK, enabling developers to create AI\nagents that seamlessly interact with external tools. In a typical workflow, developers use the Agent\nSDK to define tasks that require external tool invocation. When an AI agent encounters a task\nlike retrieving data from an API or querying a database, the SDK routes the request through an\nMCP server. The request is transmitted via the MCP protocol, ensuring proper formatting and\nreal-time response delivery to the agent. OpenAI’s plan to integrate MCP into the Responses API\nwill streamline AI-to-tool communication, allowing AI models like ChatGPT to interact with tools\ndynamically without extra configuration. Additionally, OpenAI aims to extend MCP support to\nChatGPT desktop applications, enabling AI assistants to handle various user tasks by connecting\nto remote MCP servers, further bridging the gap between AI models and external systems.\n\n*4.2.2* *Cursor: Enhancing Software Development with MCP-Powered Code Assistants.* Cursor uses\nMCP to enhance software development by enabling AI-powered code assistants that automate\ncomplex tasks. With MCP, Cursor allows AI agents to interact with external APIs, access code\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\n10 X Hou, Y Zhao, S Wang, and H Wang\n\nrepositories, and automate workflows directly within the integrated development environment.\nWhen a developer issues a command within the IDE, the AI agent evaluates whether external tools\nare needed. If so, the agent sends a request to an MCP server, which identifies the appropriate\ntool and processes the task, such as running API tests, modifying files, or analyzing code. The\nresults are then returned to the agent for further action. This integration helps automate repetitive\ntasks, minimizing errors and enhancing overall development efficiency. By simplifying complex\nprocesses, Cursor boosts both productivity and accuracy, allowing developers to execute multi-step\noperations effortlessly.\n\n*4.2.3* *Cloudflare: Remote MCP Server Hosting and Scalability.* Cloudflare has played a pivotal role\nin transforming MCP from a local deployment model to a cloud-hosted architecture by introducing\nremote MCP server hosting. This approach eliminates the complexities associated with configuring\nMCP servers locally, allowing clients to connect to secure, cloud-hosted MCP servers seamlessly.\nThe workflow begins with Cloudflare hosting MCP servers in secure cloud environments that are\naccessible via authenticated API calls. AI agents initiate requests to the Cloudflare MCP server\nusing OAuth-based authentication, ensuring that only authorized entities can access the server.\nOnce authenticated, the agent dynamically invokes external tools and APIs through the MCP server,\nexecuting tasks such as data retrieval, document processing, or API integration. This approach\nnot only reduces the risk of misconfiguration but also ensures seamless execution of AI-powered\nworkflows across distributed environments. Furthermore, Cloudflare’s multi-tenant architecture\nallows multiple users to securely access and manage their own MCP instances, ensuring isolation\nand preventing data leakage. Cloudflare’s solution thus extends MCP’s capabilities by enabling\nenterprise-grade scalability and secure multi-device interoperability.\nThe adoption of MCP by platforms like OpenAI, Cursor, and Cloudflare highlights its flexibility\nand growing role in AI-driven workflows, enhancing efficiency, adaptability, and scalability across\ndevelopment tools, enterprise applications, and cloud services.\n\n**5** **SECURITY AND PRIVACY ANALYSIS**\n\nMCP servers, as open and extensible platforms, introduce various security risks throughout their\nlifecycle. In this section, we analyze security threats across different phases: ***creation***, ***operation***,\nand ***update*** . Each phase of the MCP server lifecycle presents unique challenges that, if not properly\nmitigated, can compromise system integrity, data security, and user privacy.\n\n**5.1** **Security Risks in the Creation Phase**\n\nThe creation phase of an MCP server involves registering the server, deploying the installer, and\nverifying code integrity. This phase introduces three key risks: name collision, installer spoofing,\nand code injection/backdoor.\n\n*5.1.1* *Name Collision.* Server name collision occurs when a malicious entity registers an MCP\nserver with an identical or deceptively similar name to a legitimate server, deceiving users during the\ninstallation phase. Since MCP clients primarily **rely on the server’s name and description when**\n**selecting servers**, they are vulnerable to such impersonation attacks. Once a compromised server is\ninstalled, it can mislead AI agents and clients into invoking the malicious server, potentially exposing\nsensitive data, executing unauthorized commands, or disrupting workflows. For example, an attacker\ncould register a server named mcp-github that mimics the legitimate github-mcp server, allowing\nthem to intercept and manipulate sensitive interactions between AI agents and trusted services.\nAlthough MCP currently operates primarily in local environments, **future adoption in multi-**\n**tenant environments** introduces additional risks of name collision. In these scenarios, where\nmultiple organizations or users might register servers with similar names, the lack of centralized\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\nModel Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions 11\n\nnaming control can increase the likelihood of confusion and impersonation attacks. Additionally,\nas **MCP marketplaces grow to support public server listings, supply chain attacks may**\n**become a critical concern**, where malicious servers can replace legitimate ones. To mitigate\nthese risks, future research can focus on establishing strict namespace policies, implementing\ncryptographic server verification, and designing reputation-based trust systems to secure MCP\nserver registrations.\n\n*5.1.2* *Installer Spoofing.* Installer spoofing occurs when attackers distribute modified MCP server\ninstallers that introduce malicious code or backdoors during the installation process. Each MCP\nserver requires a unique configuration that users must manually set up in their local environments\nbefore the client can invoke the server. This manual configuration process creates a barrier for\nless technical users, prompting the emergence of **unofficial auto-installers** that automate the\nsetup process. As shown in Table 3, tools such as Smithery-CLI, mcp-get, and mcp-installer\nstreamline the installation process, allowing users to quickly configure MCP servers without dealing\nwith intricate server settings.\n\nTable 3. Unofficial MCP auto installers (As of March 27, 2025).\n\n**Tool** **Author** **",
  "Smithery CLI Henry Mao 170 2942 [smithery.ai](https://smithery.ai/)\nmcp.run Dylibso / 118 [docs.mcp.run](https://docs.mcp.run/)\nmcp-get Michael Latman 318 59 [mcp-get.com](https://mcp-get.com)\nToolbase gching / 24 [gettoolbase.ai](https://gettoolbase.ai/)\nmcp-installer Ani Betts 767 NL [1] [mcp-installer](https://github.com/anaisbetts/mcp-installer)\n\n1 Enables MCP server installation through natural language interaction with the\nclient.\n\nHowever, while these auto-installers enhance usability, they also introduce new attack surfaces\nby potentially distributing compromised packages. Since these unofficial installers are often sourced\nfrom unverified repositories or community-driven platforms, they may inadvertently expose users\nto security risks such as installing tampered servers or misconfigured environments. Attackers\ncan **exploit these auto-installers by embedding malware that grants unauthorized access,**\n**modifies system configurations, or creates persistent backdoors** . Moreover, most users\nwho opt for one-click installations **rarely review the underlying code** for potential security\nvulnerabilities, making it easier for attackers to distribute compromised versions undetected.\nAddressing these challenges requires developing a standardized, secure installation framework\nfor MCP servers, enforcing package integrity checks, and establishing reputation-based trust\nmechanisms to assess the credibility of auto-installers in the MCP ecosystem.\n\n*5.1.3* *Code Injection/Backdoor.* Code injection attacks occur when malicious code is surreptitiously\nembedded into the MCP server’s codebase during the creation phase, often bypassing traditional\nsecurity checks. It targets the server’s source code or configuration files, embedding hidden backdoors that persist even after updates or security patches. These backdoors allow attackers to\nsilently maintain control over the server, enabling actions such as unauthorized data exfiltration,\nprivilege escalation, or command manipulation. Code injection is particularly insidious because\nit can be introduced by compromised dependencies, vulnerable build pipelines, or unauthorized\nmodifications to the server’s source code. Since MCP servers often rely on community-maintained\ncomponents and open-source libraries, ensuring the integrity of these dependencies is critical.\nTo mitigate this risk, **rigorous code integrity verification, strict dependency management,**\n**and regular security audits should be implemented to detect unauthorized modifications**\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\n12 X Hou, Y Zhao, S Wang, and H Wang\n\n**and prevent the introduction of malicious code** . Additionally, adopting reproducible builds\nand enforcing checksum validation during deployment can further safeguard MCP servers from\ninjection-based threats.\n\n**5.2** **Security Risks in the Operation Phase**\n\nThe operation phase is when the MCP server actively executes tools, processes slash commands,\nand interacts with external APIs. This phase introduces three major risks: tool name conflicts, slash\ncommand overlap, and sandbox escape.\n\n*5.2.1* *Tool Name Conflicts.* Tool name conflicts arise when multiple tools within the MCP ecosystem\nshare identical or similar names, leading to ambiguity and confusion during tool selection and\nexecution. This can result in AI applications inadvertently invoking the wrong tool, potentially\nexecuting malicious commands or leaking sensitive information. A common attack scenario involves\na malicious actor registering a tool named send_email that mimics a legitimate email-sending tool.\nIf the MCP client invokes the malicious version, sensitive information intended for trusted recipients\nmay be redirected to an attacker-controlled endpoint, compromising data confidentiality. Beyond\nname similarity, our experiments revealed that malicious actors can further **manipulate tool**\n**selection by embedding deceptive phrases** in tool descriptions. Specifically, we observed that if a\ntool’s description explicitly contains directives like “this tool should be prioritized” or “prefer using\nthis tool first”, the MCP client is more likely to select that tool, even when its functionality is inferior\nor potentially harmful. This introduces a severe risk of **toolflow hijacking**, where attackers can\nleverage misleading descriptions to influence tool selection and gain control over critical workflows.\nThis underscores the need for researchers to develop advanced validation and anomaly detection\ntechniques to identify and mitigate deceptive tool descriptions, ensuring accurate and secure AI\ntool selection.\n\n*5.2.2* *Slash Command Overlap.* Slash command overlap occurs when multiple tools define identical\nor similar commands, leading to ambiguity during command execution. This overlap introduces\nthe risk of executing unintended actions, especially when AI applications dynamically select and\ninvoke tools based on contextual cues. Malicious actors can exploit this ambiguity by introducing\nconflicting commands that manipulate tool behavior, potentially compromising system integrity or\nexposing sensitive data. For instance, if one tool registers a /delete command to remove temporary\nfiles while another uses the same command to erase critical system logs, an AI application may\nmistakenly execute the incorrect command, potentially causing data loss or system instability.\nSimilar issues have been observed in team chat systems such as Slack, where overlapping command\nregistrations allowed unauthorized tools to hijack legitimate invocations, resulting in security\nbreaches and operational disruptions [ 61 ]. Since slash commands are often **surfaced as user-**\n**facing shortcuts in client interfaces, misinterpreted or conflicting commands can lead to**\n**dangerous outcomes**, especially in multi-tool environments. To minimize this risk, MCP clients\nshould establish context-aware command resolution, apply command disambiguation techniques,\nand prioritize execution based on verified tool metadata.\n\n*5.2.3* *Sandbox Escape.* Sandboxing isolates the execution environment of MCP tools, restricting\ntheir access to critical system resources and protecting the host system from potentially harmful\noperations. However, sandbox escape vulnerabilities arise when attackers exploit flaws in the\nsandbox implementation, enabling them to break out of the restricted environment and gain\nunauthorized access to the host system. Once outside the sandbox, attackers can execute arbitrary\ncode, manipulate sensitive data, or escalate privileges, compromising the security and stability of the\nMCP ecosystem. Common attack vectors include exploiting weaknesses in system calls, improperly\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\nModel Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions 13\n\nhandled exceptions, and vulnerabilities in third-party libraries. For instance, a malicious MCP tool\ncould exploit unpatched vulnerabilities in the underlying container runtime to bypass confinement\nand execute commands with elevated privileges. Similarly, side-channel attacks may allow attackers\nto extract sensitive data, undermining the intended isolation of the sandbox. Examining real-world\nsandbox escape scenarios in MCP environments can provide valuable insights for strengthening\nsandbox security and preventing future exploitation.\n\n**5.3** **Security Risks in the Update Phase**\n\nThe update phase involves managing server versions, modifying configurations, and adjusting\naccess controls. This phase introduces three critical risks: post-update privilege persistence, redeployment of vulnerable versions, and configuration drift.\n\n*5.3.1* *Post-Update Privilege Persistence.* Post-update privilege persistence occurs when outdated\nor revoked privileges remain active after an MCP server update, allowing previously authorized\nusers or malicious actors to retain elevated privileges. This vulnerability arises when privilege\nmodifications, such as **API key revocations or permission changes, are not properly synchro-**\n**nized or invalidated following server updates** . If these outdated privileges persist, attackers\nmay exploit them to maintain unauthorized access to sensitive resources or perform malicious\noperations. For example, in API-driven environments like GitHub or AWS, privilege persistence\nhas been observed when outdated OAuth tokens or IAM session tokens remain valid after privilege\nrevocation. Similarly, in MCP ecosystems, if a revoked API key or modified role configuration is\nnot promptly invalidated after an update, an attacker could continue invoking privileged actions,\npotentially compromising the integrity of the system. Enforcing strict privilege revocation policies,\nensuring privilege changes propagate consistently across all server instances, and implementing\nautomatic expiration for API keys and session tokens are essential to reducing the likelihood\nof privilege persistence. Comprehensive logging and auditing of privilege modifications further\nenhance visibility and help detect inconsistencies that could indicate privilege persistence.\n\n*5.3.2* *Re-deployment of Vulnerable Versions.* MCP servers, being open-source and **maintained by**\n**individual developers or community contributors**, lack a centralized platform for auditing\nand enforcing security updates. Users typically download MCP server packages from repositories\nlike GitHub, npm, or PyPi and configure them locally, often without formal review processes.\nThis decentralized model increases the risk of re-deploying vulnerable versions, either due to\ndelayed updates, version rollbacks, or reliance on unverified package sources. When users update\nMCP servers, they may unintentionally roll back to older, vulnerable versions to address compatibility issues or maintain stability. Additionally, unofficial auto-installers, such as mcp-get and\nmcp-installer, which streamline server installation, may default to cached or outdated versions,\nexposing systems to previously patched vulnerabilities. Since these tools often **prioritize ease of**\n**use over security**, they may lack version verification or fail to notify users about critical updates.\nBecause security patches in the MCP ecosystem rely on community-driven maintenance, **delays**\n**between vulnerability disclosure and patch availability are common** . Users who do not\nactively track updates or security advisories may unknowingly continue using vulnerable versions,\ncreating opportunities for attackers to exploit known flaws. For example, an attacker could exploit\nan outdated MCP server to gain unauthorized access or manipulate server operations. From a\nresearch perspective, analyzing version management practices in MCP environments can identify\npotential gaps and highlight the need for automated vulnerability detection and mitigation. On the\nother hand, there is also a pressing need to establish an **official package management system**\n**with a standardized packaging format** for MCP servers and a **centralized server registry to**\n**facilitate secure discovery and verification** of available MCP servers.\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\n14 X Hou, Y Zhao, S Wang, and H Wang\n\n*5.3.3* *Configuration Drift.* Configuration drift occurs when unintended changes accumulate in\nthe system configuration over time, deviating from the original security baseline. These deviations often arise due to manual adjustments, overlooked updates, or conflicting modifications\nmade by different tools or users. In MCP environments, where servers are typically configured\nand maintained locally by end-users, such inconsistencies can introduce exploitable gaps and\nundermine the overall security posture. With the emergence of remote MCP server support, such\nas Cloudflare’s hosted MCP environments, configuration drift becomes an even more pressing\nconcern. Unlike local MCP deployments, where configuration issues may only affect a single user’s\nenvironment, configuration drift in remote or cloud-based MCP servers can impact multiple users\nor organizations simultaneously. Misconfigurations in multi-tenant environments may expose\nsensitive data, lead to privilege escalation, or inadvertently grant malicious actors broader access\nthan intended. Addressing this issue requires the implementation of automated configuration\nvalidation mechanisms and regular consistency checks to ensure that both local and remote MCP\nenvironments adhere to secure baseline configurations.\n\n**6** **DISCUSSION**\n\n**6.1** **Implications**\n\nThe rapid adoption of MCP is transforming the AI application ecosystem, introducing new opportunities and challenges that have significant implications for developers, users, MCP ecosystem\nmaintainers, and the broader AI community.\n**For developers**, MCP reduces the complexity of integrating external tools, enabling the creation\nof more versatile and capable AI agents that can perform complex, multi-step tasks. By providing a\nstandardized interface for invoking tools, MCP shifts the focus from managing intricate integrations\nto enhancing agent logic and functionality. However, this increased efficiency comes with the\nresponsibility to ensure that MCP implementations are secure, version-controlled, and aligned with\nbest practices. Developers must remain vigilant about maintaining secure tool configurations and\npreventing potential misconfigurations that could expose systems to vulnerabilities.\n**For users**, MCP enhances the experience by enabling seamless interactions between AI agents and\nexternal tools, automating workflows across platforms such as enterprise data management and IoT\nintegration. It reduces the need for manual operations and improves efficiency in handling complex\ntasks. However, as MCP servers gain deeper access to sensitive data and critical operations, users\nmust remain vigilant about the risks posed by unverified tools and misconfigured servers. Careless\ninstallation or untrusted sources may cause data leaks, unauthorized actions, or system instability.\n**For MCP ecosystem maintainers**, the decentralized nature of MCP server development and\ndistribution introduces a fragmented security landscape. MCP servers are often hosted on opensource platforms, where updates and patches are community-driven and may vary in quality and\nfrequency. Without centralized oversight, inconsistencies in server configurations and outdated\nversions can introduce potential vulnerabilities. As the MCP ecosystem evolves to support remote\nhosting and multi-tenant environments, maintainers must remain attentive to potential risks\nassociated with configuration drift, privilege persistence, and re-deployment of vulnerable versions.\n**For the broader AI community**, MCP unlocks new possibilities by enhancing agentic workflows\nthrough cross-system coordination, dynamic tool invocation, and collaborative multi-agent systems.\nMCP’s ability to standardize interactions between agents and tools has the potential to accelerate AI\nadoption across industries, driving innovation in fields such as healthcare, finance, and enterprise\nautomation. However, as MCP adoption grows, the AI community must address emerging ethical\nand operational concerns, such as ensuring fair and unbiased tool selection, safeguarding sensitive\nuser data, and preventing potential misuse of AI capabilities. Balancing these considerations will be\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\nModel Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions 15\n\nessential to ensuring that MCP’s benefits are widely distributed while maintaining accountability\nand trust within the AI ecosystem.\n\n**6.2** **Challenges**\n\nDespite its potential, MCP’s adoption brings forth a range of challenges that need to be addressed\nto ensure its sustainable growth and responsible development:\n**Lack of centralized security oversight.** Since MCP servers are managed by independent developers and contributors, there is no centralized platform to audit, enforce, or validate security standards.\nThis decentralized model increases the likelihood of inconsistencies in security practices, making it\ndifficult to ensure that all MCP servers adhere to secure development principles. Moreover, the\nabsence of a unified package management system for MCP servers complicates the installation and\nmaintenance process, increasing the likelihood of deploying outdated or misconfigured versions.\nThe use of unofficial installation tools across different MCP clients further introduces variability in\nserver deployment, making it harder to maintain consistent security standards.\n**Authentication and authorization gaps.** MCP currently lacks a standardized framework for\nmanaging authentication and authorization across different clients and servers. Without a unified\nmechanism to verify identities and regulate access, it becomes difficult to enforce granular permissions, especially in multi-tenant environments where multiple users and agents may interact with\nthe same MCP server. The absence of robust authentication protocols increases the risk of unauthorized tool invocation and exposes sensitive data to malicious actors. Moreover, inconsistencies in\nhow different MCP clients handle user credentials further exacerbate these security challenges,\nmaking it difficult to maintain a consistent access control policy across deployments.\n**Insufficient debugging and monitoring mechanisms.** MCP lacks comprehensive debugging\nand monitoring mechanisms, making it difficult for developers to diagnose errors, trace tool\ninteractions, and assess system behavior during tool invocation. Since MCP clients and servers\noperate independently, inconsistencies in error handling and logging can obscure critical security\nevents or operational failures. Without robust monitoring frameworks and standardized logging\nmechanisms, identifying anomalies, preventing system failures, and mitigating potential security\nincidents becomes challenging, hindering the development of more resilient MCP ecosystems.\n**Maintaining consistency in multi-step, cross-system workflows.** MCP allows AI agents to\nexecute multi-step workflows by invoking multiple tools across different systems through a unified\ninterface. Ensuring consistent context across successive tool interactions is inherently difficult\ndue to the distributed nature of these systems. Without effective state management and error\nrecovery mechanisms, MCP risks propagating errors or losing intermediate results, leading to\nincomplete or inconsistent workflows. Additionally, dynamic coordination across diverse platforms\ncan introduce delays and conflicts, further complicating the seamless execution of workflows within\nMCP environments.\n\n**Scalability challenges in multi-tenant environments.** As MCP evolves to support remote server\nhosting and multi-tenant environments, maintaining consistent performance, security, and tenant\nisolation becomes increasingly complex. Without robust mechanisms for resource management\nand tenant-specific configuration policies, misconfigurations can lead to data leakage, performance\nissues, and privilege escalation. Ensuring scalability and isolation is critical for MCP’s reliability in\nenterprise deployments.\n**Challenges in embedding MCP in smart environments.** Integrating MCP into smart environments, such as smart homes, industrial IoT systems, or enterprise automation platforms, introduces\nunique challenges related to real-time responsiveness, interoperability, and security. MCP servers\nin these environments must handle continuous streams of data from multiple sensors and devices\nwhile maintaining low-latency responses. Moreover, ensuring seamless interaction between AI\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\n16 X Hou, Y Zhao, S Wang, and H Wang\n\nagents and heterogeneous device ecosystems often requires custom adaptations, increasing development complexity. Compromised MCP servers in smart environments can lead to unauthorized\ncontrol over critical systems, threatening both safety and data integrity.\n\n**6.3** **Recommendations for MCP stakeholders**\n\nTo safeguard the long-term success and security of MCP, all stakeholders, including MCP maintainers, developers, researchers, and end-users, should implement best practices and proactively\naddress evolving challenges within the ecosystem.\n**Recommendations for MCP maintainers.** MCP maintainers play a critical role in establishing\nsecurity standards, improving version control, and ensuring ecosystem stability. To reduce the\nrisk of security vulnerabilities, maintainers should establish a formal package management system\nthat enforces strict version control and ensures that only verified updates are distributed to users.\nAdditionally, introducing a centralized server registry would enable users to discover and validate\nMCP servers more securely, reducing the risk of interacting with malicious or misconfigured\nservers. To further enhance security, maintainers should promote the adoption of cryptographic\nsignatures for verifying MCP packages and encourage periodic security audits to identify and\nmitigate vulnerabilities. Moreover, implementing a secure sandboxing framework can help prevent\nprivilege escalation and protect host environments from malicious tool executions.\n**Recommendations for developers.** Developers integrating MCP into AI applications should\nprioritize security and resilience by adhering to secure coding practices and maintaining thorough\ndocumentation. Enforcing version management policies can prevent rollbacks to vulnerable versions,\nwhile thorough testing ensures reliable MCP integrations before deployment. To mitigate configuration drift, developers should automate configuration management and adopt infrastructure-as-code\n(IaC) practices. Additionally, implementing robust tool name validation and disambiguation techniques can prevent conflicts that lead to unintended behavior. Leveraging runtime monitoring and\nlogging helps track tool invocations, detect anomalies, and mitigate threats effectively.\n**Recommendations for researchers.** Given the decentralized nature of MCP server deployment\nand the evolving threat landscape, researchers should focus on conducting systematic security\nanalyses to uncover potential vulnerabilities in tool invocation, sandbox implementations, and\nprivilege management. Exploring techniques to enhance sandbox security, mitigate privilege\npersistence, and prevent configuration drift can significantly strengthen MCP’s security posture. In\naddition, researchers should investigate more effective approaches for version control and package\nmanagement in decentralized ecosystems to reduce the likelihood of re-deploying vulnerable\nversions. Researchers can help MCP maintainers and developers stay ahead of emerging threats\nby developing automated vulnerability detection methods and proposing secure update pipelines.\nAnother critical area for research is the exploration of context-aware agent orchestration in multitool environments. As MCP increasingly supports multi-step, cross-system workflows, ensuring\nstate consistency and preventing tool invocation conflicts becomes paramount. Researchers can\nexplore techniques for dynamic state management, error recovery, and workflow validation to\nensure seamless operation in complex environments.\n**Recommendations for end-users.** End-users should remain vigilant about security risks and\nadopt practices to safeguard their environments. They should prioritize using verified MCP servers\nand avoid unofficial installers that may introduce vulnerabilities. Regularly updating MCP servers\nand monitoring configuration changes can prevent misconfigurations and reduce exposure to\nknown exploits. Properly configuring access control policies helps prevent privilege escalation\nand unauthorized tool usage. For users relying on remote MCP servers, choosing providers that\nfollow strict security standards can minimize risks in multi-tenant environments. Promoting user\nawareness and encouraging best practices will enhance overall security and resilience.\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\nModel Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions 17\n\n**7** **RELATED WORK**\n\n**7.1** **Tool Integration in LLM Applications**\n\nEquipping LLMs with external tools has become a key paradigm for enhancing their capabilities\nin real-world tasks. This approach enables LLMs to transcend the limitations of static knowledge\nand interact dynamically with external systems. Recent studies have proposed frameworks to\nsupport such integration, focusing on tool representation, selection, invocation, and reasoning.\nShen et al.[ 44 ] provide a comprehensive survey outlining a standard LLM-tool integration paradigm,\nidentifying key challenges in user intent understanding, tool selection, and execution planning.\nBuilding on this, AutoTools[45, 46] introduces an automated framework that transforms raw tool\ndocumentation into executable functions, reducing reliance on manual engineering. EasyTool [ 59 ]\nfurther streamlines this process by distilling diverse and verbose tool documentation into concise\nand unified instructions, improving tool usability and efficiency. From an evaluation perspective,\nseveral benchmarks have emerged. ToolSandbox [ 30 ] emphasizes stateful and interactive tool usage\nwith implicit dependencies, while UltraTool [ 23 ] focuses on complex, multi-step tasks involving\nplanning, creation, and execution. These efforts reveal significant performance gaps and motivate\nbetter evaluations for LLM-agent capabilities. To improve agent decision-making and prompt\nquality, AvaTaR [ 54 ] proposes contrastive reasoning techniques, while Toolken+[ 56 ] incorporates\nreranking and rejection mechanisms for more precise tool use. Additionally, some works explore\nLLMs not just as tool users but as tool creators—ToolMaker[ 53 ] autonomously converts code\nrepositories into callable tools, moving toward fully automated agents. To unify this expanding\nlandscape, Li [ 27 ] proposes a taxonomy that situates tool use alongside planning and feedback\nlearning as three core agent paradigms.\nAs tool-augmented LLMs continue to evolve, the lack of a standardized, secure, and extensible\ncontext protocol has become a key bottleneck. MCP, with its potential to unify tool interaction across\ndiverse systems, is poised to become the foundational layer for next-generation LLM applications,\nmaking it critical to examine its landscape, limitations, and risks.\n\n**7.2** **Security Risks in LLM-Tool Interactions**\n\nThe integration of tool-use capabilities into LLM agents significantly expands their functionality,\nbut also introduces new and more severe security risks. Fu et al. [ 17 ] demonstrate that obfuscated\nadversarial prompts can lead LLM agents to misuse tools, enabling attacks such as data exfiltration\nand unauthorized command execution. These vulnerabilities are particularly concerning as they\ngeneralize across models and modalities. A growing body of work has begun to categorize and\nanalyze these risks. Gan et al.[ 18 ] and Yu et al.[ 58 ] propose taxonomies for threats across agent\ncomponents and stages, while the OWASP Agentic Security Initiative [ 25 ] provides practical threat\nmodeling frameworks. To support detection and mitigation, Chen et al.[ 5 ] introduce AgentGuard,\nwhich automatically discovers unsafe workflows and generates safety constraints, and ToolFuzz[ 36 ]\nidentifies failures stemming from ambiguous or underspecified tool documentation. On the alignment front, Chen et al.[ 6 ] propose the H2A principle, which encourages LLMs to behave with\nhelpfulness, harmlessness, and autonomy, and introduce the ToolAlign dataset to guide safer tool\nusage. Ye et al.[ 57 ] further analyze safety risks throughout the tool-use pipeline, including malicious\nqueries, execution misdirection, and unsafe outputs. Deng et al. [ 13 ] highlight broader systemic\nrisks such as unpredictable inputs, environmental variability, and untrusted tool endpoints.\nThese security threats may be mitigated through the structured design of MCP, but they can also\npersist or even evolve under this new integration paradigm. As MCP simplifies tool orchestration\nin LLM applications, it simultaneously introduces new potential attack surfaces, warranting deeper\ninvestigation into its security implications.\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\n18 X Hou, Y Zhao, S Wang, and H Wang\n\n**8** **CONCLUSION**\n\nThis paper presents the first comprehensive analysis of the MCP ecosystem landscape. We examine\nits architecture, core components, operational workflows, and server lifecycle stages. Furthermore,\nwe explore the adoption, diversity, and use cases, while identifying potential security threats\nthroughout the creation, operation, and update phases. We also highlight the implications and\nrisks associated with MCP adoption and propose actionable recommendations for stakeholders\nto enhance security and governance. Additionally, we outline future research directions to tackle\nemerging risks and improve MCP’s resilience. As MCP continues to gain traction with industry\nleaders such as OpenAI and Cloudflare, addressing these challenges is key to its long-term success\nand to enabling secure, efficient interaction with diverse external tools and services.\n\n**REFERENCES**\n\n[1] [ahujasid. 2025. BlenderMCP - Blender Model Context Protocol Integration. https://github.com/ahujasid/blender-mcp.](https://github.com/ahujasid/blender-mcp)\n\n[[2] Anthropic. 2024. For Claude Desktop Users. https://modelcontextprotocol.io/quickstart/user.](https://modelcontextprotocol.io/quickstart/user)\n\n[3] [Anthropic. 2024. Introducing the Model Context Protocol. https://www.anthropic.com/news/model-context-protocol.](https://www.anthropic.com/news/model-context-protocol)\n\n[[4] ByteDance. 2024. Coze plugin store. https://www.coze.com/store/plugin.](https://www.coze.com/store/plugin)\n\n[5] Jizhou Chen and Samuel Lee Cong. 2025. AgentGuard: Repurposing Agentic Orchestrator for Safety Evaluation of\nTool Orchestration. *CoRR* [abs/2502.09809 (2025). https://doi.org/10.48550/ARXIV.2502.09809 arXiv:2502.09809](https://doi.org/10.48550/ARXIV.2502.09809)\n\n[6] Zhi-Yuan Chen, Shiqi Shen, Guangyao Shen, Gong Zhi, Xu Chen, and Yankai Lin. 2024. Towards Tool Use Alignment\nof Large Language Models. In *Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing* .\n\n1382–1400.\n\n[[7] Cline. 2025. Cline. https://github.com/cline/cline.](https://github.com/cline/cline)\n\n[[8] Cloudflare. 2025. Cloudflare. https://www.cloudflare.com.](https://www.cloudflare.com)\n\n[[9] Codeium. 2025. Codeium. https://codeium.com.](https://codeium.com)\n\n[10] [Sourcegraph Cody. 2025. Cody supports additional context through Anthropic’s Model Context Protocol. https:](https://sourcegraph.com/blog/cody-supports-anthropic-model-context-protocol)\n[//sourcegraph.com/blog/cody-supports-anthropic-model-context-protocol.](https://sourcegraph.com/blog/cody-supports-anthropic-model-context-protocol)\n\n[11] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola\nTonellotto, and Fabrizio Silvestri. 2024. The Power of Noise: Redefining Retrieval for RAG Systems. *CoRR* abs/2401.14887\n[(2024). https://doi.org/10.48550/ARXIV.2401.14887 arXiv:2401.14887](https://doi.org/10.48550/ARXIV.2401.14887)\n\n[12] [Cursor. 2025. Learn how to add and use custom MCP tools within Cursor. https://docs.cursor.com/context/model-](https://docs.cursor.com/context/model-context-protocol)\n[context-protocol.](https://docs.cursor.com/context/model-context-protocol)\n\n[13] Zehang Deng, Yongjian Guo, Changzhou Han, Wanlun Ma, Junwu Xiong, Sheng Wen, and Yang Xiang. 2024. AI\nAgents Under Threat: A Survey of Key Security Challenges and Future Pathways. *CoRR* abs/2406.02630 (2024).\n[https://doi.org/10.48550/ARXIV.2406.02630 arXiv:2406.02630](https://doi.org/10.48550/ARXIV.2406.02630)\n\n[[14] Windsurf Editor. 2025. Windsurf Editor. https://windsurf.com.](https://windsurf.com)\n\n[[15] Antanavicius et al. 2025. PulseMCP. https://www.pulsemcp.com.](https://www.pulsemcp.com)\n\n[16] Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. A\nSurvey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models. In *Proceedings of the 30th ACM*\n*SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2024, Barcelona, Spain, August 25-29, 2024*, Ricardo\n[Baeza-Yates and Francesco Bonchi (Eds.). ACM, 6491–6501. https://doi.org/10.1145/3637528.3671470](https://doi.org/10.1145/3637528.3671470)\n\n[17] Xiaohan Fu, Shuheng Li, Zihan Wang, Yihao Liu, Rajesh K. Gupta, Taylor Berg-Kirkpatrick, and Earlence Fernandes.\n2024. Imprompter: Tricking LLM Agents into Improper Tool Use. *CoRR* [abs/2410.14923 (2024). https://doi.org/10.](https://doi.org/10.48550/ARXIV.2410.14923)\n[48550/ARXIV.2410.14923 arXiv:2410.14923](https://doi.org/10.48550/ARXIV.2410.14923)\n\n[18] Yuyou Gan, Yong Yang, Zhe Ma, Ping He, Rui Zeng, Yiming Wang, Qingming Li, Chunyi Zhou, Songze Li, Ting Wang,\nYunjun Gao, Yingcai Wu, and Shouling Ji. 2024. Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats\nin LLM-Based Agents. *CoRR* [abs/2411.09523 (2024). https://doi.org/10.48550/ARXIV.2411.09523 arXiv:2411.09523](https://doi.org/10.48550/ARXIV.2411.09523)\n\n[[19] gching. 2025. Toolbase. https://gettoolbase.ai.](https://gettoolbase.ai)\n\n[[20] glama.ai. 2025. Glama MCP Servers. https://glama.ai/mcp/servers.](https://glama.ai/mcp/servers)\n\n[[21] Goose. 2025. Goose. https://goose.ai.](https://goose.ai)\n\n[22] Nadeeshaan Gunasinghe and Nipuna Marcus. 2021. *Language Server Protocol and Implementation* . Springer.\n\n[23] Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng, Yasheng\nWang, Lifeng Shang, Xin Jiang, Ruifeng Xu, and Qun Liu. 2024. Planning, Creation, Usage: Benchmarking LLMs for\nComprehensive Tool Utilization in Real-World Complex Scenarios. *CoRR* [abs/2401.17167 (2024). https://doi.org/10.](https://doi.org/10.48550/ARXIV.2401.17167)\n[48550/ARXIV.2401.17167 arXiv:2401.17167](https://doi.org/10.48550/ARXIV.2401.17167)\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\nModel Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions 19\n\n[[24] JetBrains. 2025. JetBrains MCP Server. https://plugins.jetbrains.com/plugin/26071-mcp-server.](https://plugins.jetbrains.com/plugin/26071-mcp-server)\n\n[25] Sotiropoulos John, Rosario Ron F Del, Kokuykin Evgeniy, Oakley Helen, Habler Idan, Underkoffler Kayla, Huang Ken,\nSteffensen Peter, Aralimatti Rakshith, Bitton Ron, et al . 2025. *OWASP Top 10 for LLM Apps & Gen AI Agentic Security*\n*Initiative* . Ph. D. Dissertation. OWASP.\n\n[26] [LangChain. 2022. LangChain: Framework for developing applications powered by language models. https://github.](https://github.com/langchain-ai/langchain)\n[com/langchain-ai/langchain.](https://github.com/langchain-ai/langchain)\n\n[27] Xinzhe Li. 2025. A Review of Prominent Paradigms for LLM-Based Agents: Tool Use, Planning (Including RAG),\nand Feedback Learning. In *Proceedings of the 31st International Conference on Computational Linguistics, COLING*\n*2025, Abu Dhabi, UAE, January 19-24, 2025*, Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa,\nBarbara Di Eugenio, and Steven Schockaert (Eds.). Association for Computational Linguistics, 9760–9779. [https:](https://aclanthology.org/2025.coling-main.652/)\n[//aclanthology.org/2025.coling-main.652/](https://aclanthology.org/2025.coling-main.652/)\n\n[[28] LibreChat. 2025. LibreChat. https://librechat.ai.](https://librechat.ai)\n\n[[29] Jerry Liu. 2022. LlamaIndex: A data framework for LLM applications. https://github.com/run-llama/llama_index.](https://github.com/run-llama/llama_index)\n\n[30] Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix Bai, Shuang Ma, Shen Ma, Mengyu Li,\nGuoli Yin, Zirui Wang, and Ruoming Pang. 2024. ToolSandbox: A Stateful, Conversational, Interactive Evaluation\nBenchmark for LLM Tool Use Capabilities. *CoRR* [abs/2408.04682 (2024). https://doi.org/10.48550/ARXIV.2408.04682](https://doi.org/10.48550/ARXIV.2408.04682)\n\n[arXiv:2408.04682](https://arxiv.org/abs/2408.04682)\n\n[[31] Baidu Maps. 2025. Baidu Maps MCP Servers. https://lbs.baidu.com/faq/api?title=mcpserver/base.](https://lbs.baidu.com/faq/api?title=mcpserver/base)\n\n[[32] Emacs MCP. 2025. Emacs MCP. https://github.com/lizqwerscott/mcp.el.](https://github.com/lizqwerscott/mcp.el)\n\n[[33] Tripo3D MCP. 2025. Tripo3D MCP. https://blender-mcp.com/.](https://blender-mcp.com/)\n\n[[34] mcp dockmater. 2025. Dockmaster. https://mcp-dockmaster.com.](https://mcp-dockmaster.com)\n\n[[35] mcp.so. 2025. MCP.so. https://mcp.so/.](https://mcp.so/)\n\n[36] Ivan Milev, Mislav Balunović, Maximilian Baader, and Martin Vechev. 2025. ToolFuzz–Automated Agent Tool Testing.\n*arXiv preprint arXiv:2503.04479* (2025).\n\n[[37] OpenAI. 2023. ChatGPT plugins. https://openai.com/index/chatgpt-plugins/.](https://openai.com/index/chatgpt-plugins/)\n\n[[38] OpenAI. 2023. Funcation Calling. https://platform.openai.com/docs/guides/function-calling?api-mode=responses.](https://platform.openai.com/docs/guides/function-calling?api-mode=responses)\n\n[39] [OpenAI. 2025. OpenAI Agents SDK - Model context protocol (MCP). https://openai.github.io/openai-agents-python/](https://openai.github.io/openai-agents-python/mcp/)\n[mcp/.](https://openai.github.io/openai-agents-python/mcp/)\n\n[[40] OpenSumi. 2025. OpenSumi. https://github.com/opensumi/core.](https://github.com/opensumi/core)\n\n[41] [Model Context Protocol. 2024. GitHub MCP Server. https://github.com/modelcontextprotocol/servers/tree/main/src/](https://github.com/modelcontextprotocol/servers/tree/main/src/github)\n[github.](https://github.com/modelcontextprotocol/servers/tree/main/src/github)\n\n[42] [Model Context Protocol. 2024. Slack MCP Server. https://github.com/modelcontextprotocol/servers/tree/main/src/](https://github.com/modelcontextprotocol/servers/tree/main/src/slack)\n[slack.](https://github.com/modelcontextprotocol/servers/tree/main/src/slack)\n\n[[43] Replit. 2025. Replit. https://replit.com.](https://replit.com)\n\n[44] Zhuocheng Shen. 2024. LLM With Tools: A Survey. *CoRR* [abs/2409.18807 (2024). https://doi.org/10.48550/ARXIV.2409.](https://doi.org/10.48550/ARXIV.2409.18807)\n\n[18807 arXiv:2409.18807](https://doi.org/10.48550/ARXIV.2409.18807)\n\n[45] Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Suzan Verberne, and Zhaochun Ren. 2024. Chain of Tools: Large Language Model is an Automatic Multi-tool Learner. *CoRR*\n[abs/2405.16533 (2024). https://doi.org/10.48550/ARXIV.2405.16533 arXiv:2405.16533](https://doi.org/10.48550/ARXIV.2405.16533)\n\n[46] Zhengliang Shi, Shen Gao, Lingyong Yan, Yue Feng, Xiuyi Chen, Zhumin Chen, Dawei Yin, Suzan Verberne, and\nZhaochun Ren. 2025. Tool Learning in the Wild: Empowering Language Models as Automatic Tool Agents. In *THE*\n*WEB CONFERENCE 2025* [. https://openreview.net/forum?id=T4wMdeFEjX](https://openreview.net/forum?id=T4wMdeFEjX)\n\n[[47] Block (Square). 2025. Block (Square). https://glama.ai/mcp/servers/atblock/square-mcp/tools/team.](https://glama.ai/mcp/servers/ at block/square-mcp/tools/team)\n\n[[48] Stripe. 2025. Stripe. https://stripe.com.](https://stripe.com)\n\n[49] Microsoft Copilot Studio. 2025. Introducing Model Context Protocol (MCP) in Copilot Studio: Simplified Integration\n[with AI Apps and Agents. https://www.microsoft.com/en-us/microsoft-copilot/blog/copilot-studio/introducing-model-](https://www.microsoft.com/en-us/microsoft-copilot/blog/copilot-studio/introducing-model-context-protocol-mcp-in-copilot-studio-simplified-integration-with-ai-apps-and-agents/)\n[context-protocol-mcp-in-copilot-studio-simplified-integration-with-ai-apps-and-agents/.](https://www.microsoft.com/en-us/microsoft-copilot/blog/copilot-studio/introducing-model-context-protocol-mcp-in-copilot-studio-simplified-integration-with-ai-apps-and-agents/)\n\n[[50] Tencent. 2024. Tencent plugin shop. https://yuanqi.tencent.com/plugin-shop.](https://yuanqi.tencent.com/plugin-shop)\n\n[[51] Apify MCP Tester. 2025. Apify MCP Tester. https://apify.com/jiri.spilka/tester-mcp-client.](https://apify.com/jiri.spilka/tester-mcp-client)\n\n[[52] TheiaAI/TheiaIDE. 2025. TheiaAI/TheiaIDE. https://theia-ide.org/docs/user_ai/.](https://theia-ide.org/docs/user_ai/)\n\n[53] Georg Wölflein, Dyke Ferber, Daniel Truhn, Ognjen Arandjelovic, and Jakob Nikolas Kather. 2025. LLM Agents Making\nAgent Tools. *CoRR* [abs/2502.11705 (2025). https://doi.org/10.48550/ARXIV.2502.11705 arXiv:2502.11705](https://doi.org/10.48550/ARXIV.2502.11705)\n\n[54] Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Kaidi Cao, Vassilis N. Ioannidis, Karthik\nSubbian, Jure Leskovec, and James Y. Zou. 2024. AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive\nReasoning. In *Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing*\n*Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024*, Amir Globersons, Lester Mackey, Danielle\n[Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/paper_files/](http://papers.nips.cc/paper_files/paper/2024/hash/2db8ce969b000fe0b3fb172490c33ce8-Abstract-Conference.html)\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----\n\n20 X Hou, Y Zhao, S Wang, and H Wang\n\n[paper/2024/hash/2db8ce969b000fe0b3fb172490c33ce8-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2024/hash/2db8ce969b000fe0b3fb172490c33ce8-Abstract-Conference.html)\n\n[55] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin,\nEnyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng\nZou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan\nZheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. 2023. The Rise and Potential of Large Language Model Based\nAgents: A Survey. *CoRR* [abs/2309.07864 (2023). https://doi.org/10.48550/ARXIV.2309.07864 arXiv:2309.07864](https://doi.org/10.48550/ARXIV.2309.07864)\n\n[56] Konstantin Yakovlev, Sergey I. Nikolenko, and Andrey Bout. 2024. Toolken+: Improving LLM Tool Usage with Reranking\nand a Reject Option. *CoRR* [abs/2410.12004 (2024). https://doi.org/10.48550/ARXIV.2410.12004 arXiv:2410.12004](https://doi.org/10.48550/ARXIV.2410.12004)\n\n[57] Junjie Ye, Sixian Li, Guanyu Li, Caishuang Huang, Songyang Gao, Yilong Wu, Qi Zhang, Tao Gui, and Xuanjing Huang.\n2024. ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages. *CoRR*\n[abs/2402.10753 (2024). https://doi.org/10.48550/ARXIV.2402.10753 arXiv:2402.10753](https://doi.org/10.48550/ARXIV.2402.10753)\n\n[58] Miao Yu, Fanci Meng, Xinyun Zhou, Shilong Wang, Junyuan Mao, Linsey Pang, Tianlong Chen, Kun Wang, Xinfeng\nLi, Yongfeng Zhang, et al . 2025. A Survey on Trustworthy LLM Agents: Threats and Countermeasures. *arXiv preprint*\n*arXiv:2503.09648* (2025).\n\n[59] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Kan Ren, Dongsheng Li, and Deqing Yang. 2024.\nEASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction. *CoRR* abs/2401.06201 (2024). [https:](https://doi.org/10.48550/ARXIV.2401.06201)\n[//doi.org/10.48550/ARXIV.2401.06201 arXiv:2401.06201](https://doi.org/10.48550/ARXIV.2401.06201)\n\n[[60] Zed. 2025. Zed - Model Context Protocol. https://zed.dev/docs/assistant/model-context-protocol.](https://zed.dev/docs/assistant/model-context-protocol)\n\n[61] Mingming Zha, Jice Wang, Yuhong Nan, Xiaofeng Wang, Yuqing Zhang, and Zelin Yang. 2022. Hazard Integrated:\nUnderstanding Security Risks in App Extensions to Team Chat Systems. In *29th Annual Network and Distributed System*\n*Security Symposium, NDSS 2022, San Diego, California, USA, April 24-28, 2022* [. The Internet Society. https://www.ndss-](https://www.ndss-symposium.org/ndss-paper/auto-draft-262/)\n[symposium.org/ndss-paper/auto-draft-262/](https://www.ndss-symposium.org/ndss-paper/auto-draft-262/)\n\n[62] Yanjie Zhao, Xinyi Hou, Shenao Wang, and Haoyu Wang. 2024. LLM App Store Analysis: A Vision and Roadmap.\n*CoRR* [abs/2404.12737 (2024). https://doi.org/10.48550/ARXIV.2404.12737 arXiv:2404.12737](https://doi.org/10.48550/ARXIV.2404.12737)\n\n, Vol. 1, No. 1, Article . Publication date: April 2025.\n\n\n-----",
  "1 *Instituto Nacional de Astrofísica, Óptica y Electrónica, Luis Enrique Erro 1, CP 72840, Tonantzintla, Puebla, Mexico*\n2 *Bundesdeutsche Arbeitsgemeinschaft für Veränderliche Sterne e.V. (BAV), Munsterdamm 90, 12169 Berlin, Germany*\n3 *American Association of Variable Star Observers (AAVSO), 49 Bay State Rd, Cambridge, MA 02138, USA*\n4 *Faculty of Science, Masaryk University, Department of Theoretical Physics and Astrophysics, Kotlářská 2, 611 37 Brno, Czechia*\n5 *Universidad Autónoma de Nuevo León, Pedro de Alba S/N, 66455 San Nicolś de los Garza, Nuevo León, Mexico*\n6 *Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, Pasadena CA 91109, USA*\n7 *University of Rochester, Department of Physics & Astronomy, Rochester, NY 14627-0171, USA*\n\nAccepted XXX. Received YYY; in original form ZZZ\n\n**ABSTRACT**",
  "**1 INTRODUCTION**\n\nAp/CP2 stars are upper main-sequence objects characterised by peculiar atmospheric abundances of elements such as Si, Fe, Sr, Cr,\nEu, and the rare-earth elements as compared to the solar composition (e.g. Preston 1974; Renson & Manfroid 2009; Ghazaryan et al.\n2018). They exhibit strong, stable, and globally organised magnetic\nfields with strengths of up to several tens of kilogauss (Babcock\n1947; Landstreet 1982; Aurière et al. 2007) and show a non-uniform\nsurface distribution of elements, which is associated with the characteristics of the magnetic field and results in the formation of spots of\nenhanced or depleted element abundance. In these ‘chemical spots’,\nflux is redistributed through line and continuum blanketing (e.g.\nWolff & Wolff 1971; Lanz et al. 1996; Shulyak et al. 2010; Krtička\net al. 2013). As a consequence of the magnetic field being inclined\nto the rotation axis (Stibbs 1950), most CP2 stars exhibit light, spectral, and magnetic field strength variations over the rotation period.\n\n*★* E-mail: ebertone@inaoep.mx\n\n- E-mail:mchavez@inaoep.mx\n\n© 2024 The Authors\n\n\nPhotometrically variable CP2 stars are conventionally referred to as\n*𝛼* [2] Canum Venaticorum (ACV) variables (Samus et al. 2017).\n\nThe He-rich and the He-weak/CP4 stars (e.g. Preston 1974; Renson\n& Manfroid 2009; Ghazaryan et al. 2019) are in many respects the\nhotter analogues of the CP2 stars. Except for one subgroup (the\nHe-weak PGa stars), they also possess strong and stable magnetic\nfields and present an inhomogeneous chemical surface composition,\nwhich leads to the same kind of variability as observed in the CP2\nstars. Together with the CP2 stars, the He-peculiar stars are generally\nreferred to by convention as magnetic chemically peculiar (mCP)\n\nstars.\n\nThe observed amplitude and shape of the light curves of ACV\nvariables depend on the investigated wavelength regions. The light\nvariations may appear in phase in different photometric passbands\nor vary in antiphase to the flux at other wavelengths (e.g. Manfroid\n& Mathys 1986; Shulyak et al. 2010). A limited number of ACV\nstars have been studied at space-ultraviolet (UV) wavelengths. Antiphase variations of the flux between the far-UV and the optical\nwavelength regions is a common property, as it has been found in\nCU Vir (Sokolov 2000; Krtička et al. 2019), *𝛼* [2] CVn (Molnar 1973;\n\n\n-----",
  "Sokolov 2011), a Cen (Sokolov 2012), *𝜄* Cas (Molnar et al. 1976),\nHD 215441 (Leckrone 1974), *𝜙* Dra (Jamar 1977; Prvák et al. 2015),\n*𝜃* Aur (Krtička et al. 2015), and *𝜖* UMa (Molnar 1975). It is due\nto a global flux redistribution to longer wavelengths, caused by the\nphase-dependent absorption. However, the pattern of this redistribution can be different from star to star. The wavelength interval\nwhere the flux remains almost unchanged over the rotational cycle\nis called the ‘null-wavelength region’ (Molnar 1975) and may be\nlocated at different wavelengths. Therefore, the antiphase variations\nmay not only occur between the far-UV and optical wavelengths. In\nparticular, the near-UV interval shows non-unique behaviour: its flux\nvaries in phase with the visible in CU Vir (Sokolov 2000; Krtička\net al. 2019), a Cen (Sokolov 2012; Krtička et al. 2020), *𝜙* Dra (Jamar\n1977; Prvák et al. 2015), and *𝜃* Aur (Krtička et al. 2015), while it\nshows antiphase variation in *𝛼* [2] CVn (Molnar 1973; Sokolov 2011)\nand *𝜄* Cas (Molnar et al. 1976). In the case of HD 215441, Leckrone\n(1974) finds a null wavelength at ∼2460 Å, but does not present other\nNUV intervals, while *𝜖* UMa has not been studied in the space NUV\ninterval.\n\nAntiphase variations may sometimes also be observed in the optical region, where, in general, the light changes are in phase in\nthe different photometric passbands. For example, antiphase variation of the *𝐵* and *𝑉* light curves of the CP2 star HD 240121 have\nbeen reported by Gröbel et al. (2017). Faltová et al. (2021) reported\nantiphase variability in the Zwicky Transient Facility (Bellm et al.\n2019; Masci et al. 2019) *𝑔* and *𝑟* filters and employed this unique\ncharacteristic for the search for new CP2 star candidates. In general,\nACV variables show an amazing diversity of light curve patterns,\nwhose characteristics depend on the surface distribution of spots and\nthe elements involved (Mikulášek et al. 2007).\nWe here present a study of the properties of the photometric variability of a sample of mCP stars (mostly CP2 stars), mCP star candidates and several non-CP stars in the ultraviolet (UV) and visible\nregions based on observational data of the space telescope GALEX\n(Martin & GALEX Science Team 2003; Bianchi et al. 2017) and\nthe *Kepler* prime mission (Borucki et al. 2010). We investigate the\npresence of a correlation of the variability amplitudes with stellar\nparameters that affect the intensity of the absorption, in particular\neffective temperature, and calculate synthetic light curves, model atmospheres, and spectral energy distribution (SED) profiles of diverse\nchemical compositions to connect our findings to theoretical models.\nData sets are described in Sect. 2. The objects of our study, the\nsample of mCP stars in the *Kepler* prime field, are discussed in\nSect. 3. Next, we illustrate the procedure used to determine the nearultraviolet (NUV) variability amplitude and its phase difference to\nthe light changes observed in the visible region (Sect. 4). Results are\npresented in Sect. 5, notes on some individual objects in Sect. 6 and\nfinal conclusions are exposed in Sect. 7.\n\n**2 OBSERVATIONAL DATA BASES**\n\n**2.1 The GALEX NUV observations**\n\nThe prime field of the *Kepler* mission (Borucki et al. 2010) has\nrapidly become one of the most observed and studied stellar fields.\nIt is located in the northern hemisphere and is centred at R.A. =\n19h22m40s and Dec =+44 [◦] 30 [′] 00 [′′] (J2000), not far from Vega. Its\nmore than 100 square degrees have been observed by the *GALEX*\nspace telescope during the Complete All-Sky UV Survey Extension\n(CAUSE), as part of a programme funded by the Cornell University\n(P.I. James Lloyd). The observations have been carried out with the\n\nMNRAS **000**, 1–14 (2024)\n\n\nNUV detector (Martin & GALEX Science Team 2003; Siegmund\net al. 2004) only (the FUV detector stopped working in May 2009),\nduring 47 days in August-September 2012, a period that coincides\nwith Quarter 14 of the *Kepler* observations.\nMore details on the *GALEX* CAUSE survey of the *Kepler* prime\nfield are provided by Olmedo et al. (2015). This article also\npresents the *GALEX* -CAUSE *Kepler* (GCK) catalogue [1] of about\n660,000 NUV point sources, down to a limiting magnitude of\nNUV≃22.6 mag. Most GCK sources are included in the *Kepler* Input\nCatalog (KIC; Brown et al. 2011).\nDue to the modality used by *GALEX* in the observations of the\n*Kepler* field (multiple scans along a larger circle), each GCK source\nhas been observed several times. The *GALEX* processing pipeline\ndivided the whole area into 180 tiles: each tile was visited 17 times,\non average, with a maximum of 22, for a total of 3,251 images.\nTherefore, besides measuring the brightness of the sources on the\nco-added images as it was done by Olmedo et al. (2015), it is also\npossible to extract their light curves, by separately reducing each tile\nand perform the photometry of all detected sources. This has already\nbeen done by our group (Bertone et al. 2020) and the catalogue of all\nNUV light curves of point sources in the *Kepler* field will be presented\nin a future publication (Olmedo et al., in preparation). Data reduction\nhas followed the same procedure as in Olmedo et al. (2015), with\nthe only difference of using the background image of the *GALEX*\npipeline [2], rather than computing our own. However, in the NUV\nband, the detector background is negligible and the sky background\nlevel is very low: it amounts to a count rate of 10 [−][3] cts sec [−][1] arcsec [−][2]\n\n(Morrissey et al. 2007) [3] ; therefore, for a source of NUV=20 mag (AB\nsystem) and FWHM=5 [′′], it would correspond to ∼ 1% of the total\nflux.\n\n**2.2 The Kepler light curves**\n\nThe *Kepler* spacecraft used a differential photometer with a 115\nsquare-degrees field of view and an aperture of 0.95 m (Borucki et al.\n2010). The detectors consisted of 21 modules each equipped with\ntwo 2200x1024 pixel CCDs. The *Kepler* telescope produced single\n∼\npassband light curves in the visible range ( 4200–9000 Å) with\ntwo different integration times in the long-cadence (LC), 29.4 minsampling mode (Jenkins et al. 2010) and the short-cadence (SC),\n∼1 min-sampling mode (Gilliland et al. 2010).\nThe prime *Kepler* mission’s main goal was the detection of transiting planets to determine the frequency of Earth-like planets in or\nnear the habitable zone of Sun-like stars (Borucki et al. 2008, 2010).\nIt was in operation for four years (2009 May 2 to 2013 May 8), until\nthe loss of a second reaction wheel on the spacecraft. After that,\n*Kepler* entered a redefined mission called K2 (Howell et al. 2014)\nthat lasted another four years until the satellite’s official retirement in\nOctober 2018.\n\n*Kepler* produced long, quasi-uninterrupted and high-quality time\nseries data. To optimise solar panel efficiency, the spacecraft completed a 90 degree-roll every three months; therefore, *Kepler* data\nare divided into four quarters each year. The final Data Release 25\n\n1 Also available at `http://vizier.u-strasbg.fr/viz-bin/`\n```\nVizieR-3?-source=J/ApJ/813/100\n\n```\n2 `http://galex.stsci.edu/GR6/?page=scanmode`\n3 See also the *GALEX* technical documentation at `http://www.galex.`\n`caltech.edu/researcher/faq.html` and `https://asd.gsfc.nasa.`\n```\ngov/archive/galex/FAQ/counts_background.html\n\n```\n\n-----\n\nincludes 197,096 stars (Mathur et al. 2017), which were observed in\nsome or all Quarters 1-17.\n*Kepler* has been very successful in its main goal and discovered\nthousands of exoplanet candidates. It has also enabled stellar variability analyses with unprecedented detail. For more information on\nthe *Kepler* spacecraft, we refer to Borucki et al. (2010) and Koch\net al. (2010).\n\n**3 SAMPLE SELECTION, CLASSIFICATION AND LIGHT**\n**CURVE DATA**\n\nWe initially collected a sample of mCP stars and candidate mCP\nstars in and near the *Kepler* prime field that was chosen from the\nlists of Hümmerich et al. (2018) and Bauer-Fasching et al. (2024).\nIt was then supplemented with another set of mCP star candidates\nidentified by S. Hümmerich (private communication).\nThis initial number of more than 100 objects decreased by applying\nthe following selection criteria: (1) the stars must have been observed\nby both *GALEX* and *Kepler* ; (2) the number of points in the *GALEX*\nlight curve must be greater than three; (3) the star must be fainter\nthan 14.7 mag in NUV (GCK catalogue).\nWe imposed the latter requisite as the photon counting *GALEX*\ndetector suffers from loss of linearity at high count rates (Morrissey\net al. 2007; see also Olmedo et al. 2015). This problem also affects the\nrepeatability of the measurements and thus it may create a spurious\nvariability. With respect to criterion (1), we comment that some of\nthe stars in the *Kepler* field observed by *GALEX* have no reliable\nphotometric data due to the presence of a nearby very bright object\nor because of other instrumental artifacts.\n\nApplying these criteria, we ended up with a final sample of 28\nstars, of which 21 are present in the sample of Hümmerich et al.\n(2018), who presented an investigation of the light variability of mCP\nstars using *Kepler* data. These authors identified candidate mCP stars\nvia light-curve properties (in particular monoperiodic variability and\nlight-curve stability) and used newly acquired and archival spectra to\ninvestigate these candidates.\nThe subsample of the 21 stars from Hümmerich et al. (2018)\nwhich entered our sample consists of 14 spectroscopically confirmed\nmCP stars (KIC 2853320, 3945892, 5739204, 5774743, 6715809,\n6950556, 7628336, 8773445, 9541567, 10685175, 10905824,\n10959320, 11154043, 11465134), three candidate mCP stars\n(KIC 2969628, 3326428, 8362546), and four stars in which these\nauthors could not establish chemical peculiarities (non-CP stars;\nKIC 5213466, 5727964, 8569986, 10082844). The non-CP stars\nhave been selected as ‘control sample’ and to check the spectroscopic results with the here described method.\nThe stars KIC 4171302, 5000179, 8386865, 9665384, 10090722,\nand 10096019 are from an unpublished list of mCP star candidates (Hümmerich, private communication). Two of these objects\n(KIC 8386865 and KIC 10090722) were subsequently confirmed as\nmCP stars by Hümmerich et al. (2020).\nFinally, the star KIC 7976845 was identified as an mCP star on\nthe basis of its light variability properties (e.g. Faltová et al. 2021)\nand a significantly positive Δ *𝑎* value calculated from Gaia BP/RP\nlow-resolution spectra (Paunzen et al. 2005; Carrasco et al. 2021;\nPaunzen & Prišegen 2022). It is part of a recently published sample\nof mCP stars and candidates (Bauer-Fasching et al. 2024).\nAs described in the following section (Section 3.1), we use here\nour own spectroscopic observations and, where available, spectra\nfrom the Large Sky Area Multi-Object Fiber Spectroscopic Telescope (LAMOST) to investigate and confirm the mCP star candidates",
  "KIC 2969628, 3326428, 4171302, 5000179, 9665384, and 10096019\n\nas bona-fide CP2 stars.\n\nIn summary, the final sample of stars that was investigated with\nGALEX UV and *Kepler* photometry in this study consists of 22\nconfirmed CP2 stars, one photometrically confirmed mCP star\n(KIC 7976845), one mCP star candidate (KIC 8362546), and four\nnon-CP stars (KIC 5213466, 5727964, 8569986, 10082844).\nRelevant parameters of our sample stars are reported in Table 1.\nThe first two columns contain the KIC and the GCK identifiers.\n\nColumns three and four report another identifier from the SIMBAD\nor VizieR databases (Wenger et al. 2000) and the stellar spectral type\n(sources are identified in the table). Column five denotes the CP star\nclassification (CP2 = CP2 star; cand = candidate mCP star; nonCP\n= non-CP star; photCP = mCP star identified by photometric criteria\nalone). J2000 coordinates from Gaia DR3 (Gaia Collaboration et al.\n2016; Babusiaux et al. 2022; Gaia Collaboration et al. 2022) are\nprovided in the next two columns. The rotational periods presented\nin column eight are based on literature values (as indicated in the\nnotes to the table) and are presented here rounded to five decimal\nplaces. The last two columns contain the brightness in the *Kepler*\nand *GALEX* NUV passbands.\n\n**3.1 Spectral classification**\n\nSpectral classification was performed using own spectroscopic observations and spectra from LAMOST.\nWe acquired observations of KIC 3326428 and KIC 2969628\nin July and September 2023 at the 2.1-meter telescope of the Observatorio Astrofıısico Guillermo Haro, located in Sonora, Mexico,\nwith a Böller & Chivens spectrograph, equipped with an e2v 4240 2040×2040 px CCD. The instrument setup consisted of a 600\nline mm [−][1] grating, at a blaze angle of 8.3 [◦], and a 200 *𝜇* m wide slit;\nthis configuration provided a wavelength coverage between about\n3870 and 4800 Å, with a spectral resolution of 3 Å for KIC 3326428\nand 2.3 Å for KIC 2969628, due to an improved focusing of the spectrograph. Multiple images of each spectra were co-added to reach\na signal-to-noise ratio of about 10 for KIC 3326428 (observed during cloudy weather), and about 30 for KIC 2969628. The spectra\nwere reduced using IRAF (Tody 1986, 1993), following the standard\nprocedure for spectroscopic data: bias subtraction, flat-field correction, cosmic-ray removal, wavelength calibration (through an internal\nHeAr lamp) and flux calibration, using standard stars from the European Southern Observatory list (Hamuy et al. 1992).\nThe LAMOST survey (Zhao et al. 2012; Cui et al. 2012) employs\na reflecting Schmidt telescope located at the Xinglong Observatory\nin Beĳing, China, with an effective aperture of 3.6−4.9 m and a field\nof view of 5 [◦] . LAMOST can take up to 4000 spectra in a single\nexposure (resolving power R∼1800; limiting magnitude r ∼ 19 mag;\nwavelength coverage 3700 Å to 9000 Å) and is therefore particularly\nsuited for large-scale spectral surveys. Data products are released to\nthe public in consecutive data releases and can be accessed via the\nLAMOST spectral archive. [4] The present study used spectra released\nin LAMOST DR4 (Luo et al. 2018).\nSpectral classification was performed in the framework of the refined MK classification system following the methodology outlined\nin Gray & Garrison (1987, 1989a,b), Garrison & Gray (1994) and\nGray & Corbally (2009). For a precise classification and to iden\n−\ntify peculiarities, the blue-violet (3800 4600 Å) spectral region was\ncompared visually to, and overlaid with, MK standard star spectra,\n\n4 `http://www.lamost.org`\n\nMNRAS **000**, 1–14 (2024)\n\n\n-----",
  "**Table 1.** Relevant properties of the sample of mCP stars. Unless indicated otherwise in the notes to this table, spectral types and period values are from\nHümmerich et al. (2018).\n\nKIC GCK Other id. Sp.Type Class. R.A. Dec. Period Kepler NUV\n(deg) (deg) (d) (mag) (mag)\n\n2853320 GCK_J19263019+3802516 2MASS J19263017+3802518 A0 V Si CP2 291.625721 +38.047703 5.06533 13.70 15.67\n2969628 GCK_J19025476+3809570 TYC 3120-750-1 A7 V SrCr [(][1][)] CP2 285.728088 +38.165967 1.97361 11.70 15.02\n3326428 GCK_J19061861+3824207 2MASS J19061861+3824209 kB9:hA5 V SrCrEu [(][1][)] CP2 286.577542 +38.405769 7.70042 13.41 16.98\n\n3945892 GCK_J19160898+3900250 TYC 3121-127-1 A2 V SiSrCrEu CP2 289.037304 +39.006961 4.08323 12.23 15.35\n4171302 GCK_J19390868+3916088 TYC 3135-491-1 kA3hA5 V SrCrEu [(][2][)] CP2 294.785890 +39.269001 8.73500 [(] *[𝑎]* [)] 11.81 14.95\n5000179 GCK_J19132601+4010289 KOI-6485 kB9hA3 V SiCrSrEu [(][2][)] CP2 288.358280 +40.174660 3.63400 [(] *[𝑏]* [)] 13.76 16.39\n\n5213466 GCK_J19523121+4023597 2MASS J19523118+4023594 A1 V nonCP 298.129900 +40.399825 2.81951 13.07 16.51\n\n5727964 GCK_J19501553+4058358 2MASS J19501553+4058357 A6 V nonCP 297.564708 +40.976564 1.63014 12.93 16.41\n\n5739204 GCK_J19591601+4056163 2MASS J19591596+4056166 B9 V SiEu CP2 299.816529 +40.937928 1.81123 13.61 17.11\n\n5774743 GCK_J19042026+4101441 TYC 3124-443-1 A3 V SiCr CP2 286.084513 +41.029186 4.07357 12.14 15.41\n\n6715809 GCK_J19511202+4206261 2MASS J19511198+4206264 A1 V SiCrEu CP2 297.799937 +42.107297 4.19793 12.50 16.05\n\n6950556 GCK_J19294384+4229306 2MASS J19294376+4229306 A0 V Si CP2 292.432367 +42.491831 1.51179 12.75 15.02\n\n7628336 GCK_J19493626+4313081 TYC 3148-183-1 A3 V SiSrCrEu CP2 297.401025 +43.218967 2.53883 11.35 14.77\n7976845 GCK_J19484629+4343516 UCAC4 669-077857 n/a photCP 297.192930 +43.730991 1.83492 [(] *[𝑐]* [)] 15.52 18.87\n8362546 GCK_J19224728+4419142 2MASS J19224722+4419143 n/a cand 290.696825 +44.320619 1.10814 15.69 16.60\n8386865 GCK_J19534140+4419393 2MASS J19534139+4419399 A0 V CrEu [(][3][)] CP2 298.422490 +44.327751 1.25800 [(] *[𝑑]* [)] 12.02 15.22\n\n8569986 GCK_J19420666+4438591 2MASS J19420663+4438592 A2 V nonCP 295.527642 +44.649778 3.13318 13.43 16.36\n\n8773445 GCK_J19531423+4457123 2MASS J19531426+4457124 A0 IV SiCrSrEu CP2 298.309417 +44.953425 3.66078 13.84 17.25\n\n9541567 GCK_J19493708+4607121 KOI-7190 A9 V SrCrEu CP2 297.404237 +46.119989 2.24569 11.87 14.74\n9665384 GCK_J19504933+4622500 UCAC4 682-072593 kA0hB9 III Si [(][2][)] CP2 297.705410 +46.380520 1.04615 [(] *[𝑐]* [)] 14.58 16.03\n\n10082844 GCK_J19391261+4701086 2MASS J19391258+4701085 A0 V nonCP 294.802400 +47.019017 2.08338 13.69 15.96\n10090722 GCK_J19492598+4702164 UCAC4 686-073039 B9.5 II-III EuSi [(][3][)] CP2 297.357979 +47.038344 6.00962 [(] *[𝑒]* [)] 13.00 15.31\n10096019 GCK_J19552479+4704589 UCAC4 686-074397 kA3hA5 V SrCrEu [(][2][)] CP2 298.853240 +47.082790 6.87600 [(] *[𝑎]* [)] 12.39 17.23\n\n10685175 GCK_J19541720+4757500 KOI-7362 A4 V Eu CP2 298.571600 +47.963928 3.10199 12.07 15.03\n\n10905824 GCK_J18544470+4820245 2MASS J18544461+4820247 A1 V SiCr CP2 283.685917 +48.340231 2.71954 12.81 15.30\n\n10959320 GCK_J18481850+4828538 UCAC4 693-063535 A0 V SiCrSr CP2 282.076729 +48.481633 2.44558 13.19 15.51\n\n11154043 GCK_J19543527+4847042 KOI-7414 A0 V SiCr CP2 298.646833 +48.784622 4.52984 11.99 15.06\n\n11465134 GCK_J19453889+4922275 TYC 3565-508-1 A0 V Si CP2 296.411767 +49.374414 1.48781 12.40 14.74\n\nNotes on spectral types: [(][1][)] This study, own spectroscopic observation. [(][2][)] This study, LAMOST spectrum. [(][3][)] Hümmerich et al. (2020).\nNotes on period values: [(] *[𝑎]* [)] Nielsen et al. (2013). [(] *[𝑏]* [)] Gao et al. (2016). [(] *[𝑐]* [)] Bauer-Fasching et al. (2024). [(] *[𝑑]* [)] Conroy et al. (2014). [(] *[𝑒]* [)] Balona (2017).\n\n\nwhich were taken from the *libr18* collections available from R. O.\nGray’s MKCLASS website. [5]\n\nmCP stars exhibit several peculiarities that need to be taken into\naccount during the process of classification. They often show weak or\notherwise peculiar Ca ii K line profiles, weak Mg ii 4481 Å lines and\nare markedly deficient in He (Gray & Corbally 2009; Ghazaryan et al.\n2018). In addition, mCP stars generally show enhanced and peculiar\nmetallic lines. Therefore, the hydrogen-line profiles are generally the\nmost accurate indicators of the actual temperature of these objects\n(Gray & Corbally 2009). Where appropriate, spectral types based on\nthe Ca ii K line strength (the k-line type) and the hydrogen-line profile\n(the h-line type) (Osawa 1965) were determined. As the metallic\nlines of most mCP stars are so peculiar that they cannot be used for\nluminosity classification, luminosity types were based on the wings\nof the hydrogen lines (Gray & Corbally 2009).\nIn this way, KIC 2969628 and 3326428 were confirmed as CP2\nstars with own spectroscopic observations, while KIC 4171302,\n5000179, 9665384, and 10096019 were confirmed as CP2 stars on\nthe basis of LAMOST spectra. Figure 1 provides an example of this\nprocess. The final spectral types are included in Table 1.\n\n**3.2 Light curves and phase diagrams in the NUV and the visible**\n\nWe searched and downloaded the *Kepler* light curves of our sample\nstars from the Mikulski Archive for Space Telescopes (MAST) [6] .\nAs the *GALEX* -CAUSE observations of the *Kepler* prime field have\nbeen carried out from 2012 August 3 to September 20, we extracted\nthe *Kepler* data from the simultaneously acquired Quarter 14. We\n\n5 `http://www.appstate.edu/~grayro/mkclass/`\n6 `https://archive.stsci.edu/kepler/data_search/search.php`\n\nMNRAS **000**, 1–14 (2024)\n\n\ntransformed the *Kepler* time system, expressed in barycentric Julian\nDate, which is used to correct for the motion of the spacecraft with\nrespect to the center of mass of the Solar system, to the *GALEX*\nsystem, expressed in Julian Date (JD); we used the formula from\nThompson et al. (2016, p. 17). We also defined the time *𝑡* =0 d, which\ncorresponds to JD=2456143.0, and selected the *Kepler* data points\nacquired in the time interval 0 ⩽ *𝑡* ⩽ 47 d because the *GALEX* data\ncover the interval 0 *.* 8 ≲ *𝑡* ≲ 46 *.* 5. Two stars have not been observed\nby *Kepler* in Quarter 14: for KIC 4171302, we therefore extracted\ndata from Quarter 13, while for KIC 9665384, we selected data from\n\n∼\nQuarter 9 ( 1.2 yr before). To produce the working light curve for\neach object, we first transformed the PDCSAP flux to magnitudes (see\nThompson et al. 2016) and we then subtracted the average magnitude,\nsince we are only interested in analyzing the variability of the sources.\nEach light curve typically has about 2230 data points.\nAll *GALEX* light curves of our sample stars can be considered of\ngood quality since all points have a signal-to-noise ratio SNR *>* 10.\nFurthermore, we visually inspected a suitable region of the *GALEX*\nimages of each visit to each star, in order to check whether the\nphotometric measurements were affected by instrumental artifacts\n(Olmedo et al. 2015; Bianchi et al. 2017) [7] . This analysis did not\nshow any anomalies. For these light curves to be comparable with\nthe *Kepler* ones, we subtracted the average NUV magnitude. These\nlight curves have between 11 and 22 data points, with a median of\n18 points.\n\n7 We refer to the *GALEX* technical documentation for a more detailed\n\nexplanation of the instrumental artifacts; see, e.g., `http://www.galex.`\n```\ncaltech.edu/DATA/gr1_docs/GR1_Pipeline_and_advanced_data_\n\n```\n`description_v2.htm` and `http://www.galex.caltech.edu/wiki/`\n```\nPublic:Documentation/Chapter_8#Artifact_Flags\n\n```\n\n-----",
  "2.2\n\n2.0\n\n1.8\n\n1.6\n\n1.4\n\n1.2\n\n1.0\n\n0.8\n\n0.6\n\n0.4\n\n\n\n\n\n\n\n\n\n\n0.2\n\n0.0\n\n3800 3900 4000 4100 4200 4300 4400 4500 4600",
  "**Figure 1.** Blue-violet region of the LAMOST DR4 spectrum of the CP2 star KIC 10096019 = LAMOST J195524.77+470458.0 (middle spectrum), compared\nwith two standard star spectra taken from the *libr18_225* collection. Some prominent lines and blends relevant to the classification of CP2 stars are identified.\nWe note the peculiarly strong Cr ii, Sr ii, and Eu ii features and the weak and unusually broad Ca ii K line in the CP2 star.\n\n\nWe then folded the data of the light curves into phase diagrams\nby assigning phase *𝜙* =0 to time *𝑡* =0 d for all objects, assuming (as\nhas been well studied) that the variability periods coincide with\nthe rotation periods (see Table 1). This implies that *𝜙* =0 does not\nnecessarily coincide with a specific feature of the light curve, such\nas the maximum or minimum of the brightness.\nSome *Kepler* phase curves show outliers or instrumental drifts or a\nnon-negligible dispersion in the brightness at the same phase. For the\nlater analysis, we therefore had to produce an average curve. Hence,\nwe first proceeded with an iterative 3 *𝜎* clipping and then computed\nan average value by smoothing the data with a moving boxcar of 50\npoints. As examples, in Fig. 2, we show several light curves and the\ncorresponding phase diagrams.\n\n**4 ASSESSING THE NUV AMPLITUDE AND PHASE**\n\n**DIFFERENCE**\n\nThe number of the NUV points in the phase curves is too small for\ndirectly assessing the variability properties, such as the period and the\namplitude. Therefore, we made the assumption that the *GALEX* NUV\nlight curve has the same period and shape as the visible one, which\nhas been shown to hold true for most ACV variables (e.g. Molnar\n1973, 1975, cf. also Section 1), and used two different methods to\ndetermine the amplitude of the NUV variation and the phase shift\nbetween the NUV and the visible curves. The first method makes use\n\nof an Asexual Genetic Algorithm (AGA; Cantó et al. 2009), while\nin the second we fit the phase curves with harmonic polynomials,\nfollowing the model of Mikulášek et al. (2007).\n\n**4.1 The asexual genetic algorithm**\n\nFor implementing the AGA, we described the NUV phase curve of a\nmCP star with the model:\n\nΔ *𝑚* NUV model ( *𝜙* ) = *𝛼* Δ *𝑚* Kep ( *𝜙* + *𝜏* ) + *𝛽,* (1)\n\n\nwhere Δ *𝑚* Kep is the *Kepler* average phase curve, Δ *𝑚* NUV model is the\n*GALEX* phase curve, while the three free parameters are: *𝛼*, the\namplitude ratio of the NUV curve to the visible one; *𝜏*, the phase\ndifference (in the interval 0–1); and *𝛽*, the offset between the mean\nof two curves.\n\nThe AGA will find the best *𝛼, 𝜏, 𝛽* values by minimizing the reduced chi-square, defined as:\n\n\nwhere Δ *𝑚* NUV obs *,𝑖* is the value of the *𝑖* -th point of the *GALEX* observed data, with error *𝜎* NUV [2] obs *,𝑖* [,][ Δ] *[𝑚]* [NUV] [model] *[,𝑖]* [is the value of the]\n*𝑖* -th point of the computed model of the *GALEX* phase curve, at the\nsame *𝜙* as the observed point, and *𝑛* is the total number of points.\nAs a first step, for each free parameter ( *𝛼, 𝜏, 𝛽* ), an initial random\npopulation of *𝑁* 0 =3050 values (i.e. individuals) is generated in a\ndefined interval and the model for the NUV phase curve (Equation 1)\nis computed. Then, the fitness of each individual is evaluated by\nmeans of a merit function, which in this case is the *𝜒* *𝜈* [2] (Equation 2).\nThe *𝑁* 1 =50 individuals with the highest fitness (i.e., smaller *𝜒* *𝜈* [2] ) are\nselected and passed to the next generation (this is the reason of the\nword \"asexual\" in the algorithm name). Then, for each free parameter,\na new and smaller interval is defined, centered at the parameter value\nof each *𝑁* 1 selected individual, where a new generation of *𝑁* 2 =60\nrandom individuals is created. Again, the fitness of all *𝑁* 1 × *𝑁* 2 + *𝑁* 1 (=\n*𝑁* 0 ) individuals in this new generation is calculated and the process\nis iterated until a stopping criterion is achieved. Here, this criterion\nis that, for five successive generations, the *𝑁* 1 individuals do not\nchange and the difference between the minimum and maximum of\ntheir *𝜒* *𝜈* [2] is less than 10 [−][3] . Finally, the model (i.e., *𝛼, 𝜏* and *𝛽* ) that\nhas the minimum *𝜒* *𝜈* [2] is assumed as the best solution and adopted as\nthe NUV model of the phase curve of the star. Cantó et al. (2009)\nthoroughly described the algorithm and its performance.\nWe evaluated the error associated to the free parameters *𝛼, 𝜏* and\n*𝛽* following the recipe of Avni (1976). Using the *𝜒* [2] values of indi\nMNRAS **000**, 1–14 (2024)\n\n\n(Δ *𝑚* NUV model *,𝑖* − Δ *𝑚* NUV obs *,𝑖* ) [2] (2)\n\n*𝜎* [2]\nNUV obs *,𝑖*\n\n\n1\n*𝜒* *𝜈* [2] [=] *𝑛* − 3\n\n\n*𝑛*\n∑︁\n\n*𝑖* =1\n\n\n-----",
  "**Figure 2. Upper panels** : *Kepler* (red dots) and *GALEX* (blue dots with error bars) light curves of the confirmed CP2 stars KIC 3326428 (kB9:hA5 V SrCrEu)\nand KIC 8386865 (A0 V CrEu), and the candidate CP2 star KIC 8362546. **Lower panels** : phase diagrams for the same stars as in the upper panels using the\nperiods listed in Table 1. The (barely visible) orange dots represent *𝜎* -clipped *Kepler* data; the smoothed curve is shown in red. The blue curve is the best fit of\nthe *GALEX* measurements (blue dots with error bars), obtained from the genetic algorithm, as explained in Sect. 4.1. Note that the *GALEX* points are shifted\nhere by the offset values reported in Table 2.\n\n[!t]\n\n**Figure 3.** The *𝜒* [2] map of all combinations of *𝛼, 𝜏, 𝛽*, projected on two planes, of the CP2 stars KIC 3326428 (left panel) and KIC 8386865 (middle panel), and\nthe mCP star candidate KIC 8362546 (right panel). The black dot marks the location of the best solution. The colour scale (orange to violet) indicates the *𝜒* [2] in\nstandard deviation intervals (1 to 5 *𝜎* ), while the white colour covers the space where the *𝜒* [2] is more than 5 *𝜎* larger than the minimum or where there are no\ndata (i.e., no parameter combinations have been computed). As expected, the results for the CP2 stars indicate antiphase variability in the UV and *Kepler* light\ncurves. This is not the case for the mCP star candidate, which provides evidence that this star is not a mCP star after all.\n\n\nviduals from all AGA iterations, we identified the volume, in the 3D\nparameter space, where the *𝜒* [2] is lower than the MIN( *𝜒* [2] ) + 3 *.* 52674\n(for 1 *𝜎* confidence level and 3 free parameters). The extreme values\nof the volume in each dimension provide the error on each parameter. In order to further improve the error estimation, we added the\n*𝜒* [2] from an extra set of random *𝛼, 𝜏*, *𝛽* combinations, more homogeneously distributed in the parameters space, which increased the\ntotal number of individuals to more than 1 million, on average. This\nprocedure produces asymmetric statistical errors around the mean.\n\nFigure 3 illustrates, as an example, the *𝜒* [2] maps of the three stars\nshown in Fig. 2. For the CP2 star KIC 3326428 (left panel), the parameters are well determined and constrained. The phase difference\n*𝜏* is almost 1 and the amplitude ratio is negative: this implies that\nthe NUV and the visible curves are almost perfectly anticorrelated,\nwhich basically confirms the star as a CP2 star. (In fact, during the\n\nMNRAS **000**, 1–14 (2024)\n\n\ncourse of our study, we were able to confirm this candidate mCP star\nfrom Hümmerich et al. (2018) as a CP2 star by our own spectroscopic\nobservations, cf. Section 3.) For the CP2 star KIC 8386865, the presence of two maxima of similar amplitude during a rotational cycle is\nreflected in the presence of various relative minima also in the map,\nalmost equidistant along *𝜏* and with a similar value of | *𝛼* |. Again, this\nimplies that the NUV and the visible curves are anticorrelated, in\nexpectations for this spectroscopically confirmed CP2 star. The latter\nis the most common result provided by AGA; in these cases, we only\ntook into account the relative minimum where the best solution is\n\nlocated to estimate the errors on the parameters. Finally, in the case\nof the CP2 star candidate KIC 8362546, the AGA was not able to\nfind a solution for the phase shift *𝜏* nor to significantly constrain the\namplitude ratio *𝛼*, which can be viewed as evidence that the star is\nnot a CP2 star (cf. the discussion in Section 6.1).\n\n\n-----\n\n**4.2 The fit with harmonic polynomials**\n\nMikulášek et al. (2007) stated that the light curves of mCP stars\ncan be well described by real-valued harmonic polynomials of 2nd\ndegree. We adapted their definition for the case of the *Kepler* phase\n\ncurves:\n\nΔ *𝑚* Kep model ( *𝜙* ) = Δ *𝑚* Kep + *𝑐* 1 cos(2 *𝜋𝜙* ) + *𝑐* 2 sin(2 *𝜋𝜙* )\n\n+ *𝑐* 3 cos(4 *𝜋𝜙* ) + *𝑐* 4 sin(4 *𝜋𝜙* ) *.* (3)\n\nThe free parameters in this equation are five: the four coefficients\n*𝑐* *𝑖* and the average Δ *𝑚* Kep . We fitted the *Kepler* observations of all\nstars of the sample, using a robust least-squares minimization that\nmakes use of the Levenberg-Marquardt algorithm. We found that\nthe best fits reproduce very well the shapes of the *Kepler* phase\ncurves, even though the *𝜒* *𝜈* [2] is, in many cases, quite large, because\nthe *Kepler* observational errors are extremely small (the SNR ranges\nin the ∼2000-20,000 interval).\nThen, in order to reproduce a similar procedure that we used with\nthe AGA fitting, we assumed the NUV phase curve to have the same\nshape as the *Kepler* curve. So, we defined the function:\n\nΔ *𝑚* NUV model ( *𝜙* ) = *𝛼* hf Δ *𝑚* Kep model ( *𝜙* + *𝜏* hf ) + *𝛽* hf (4)\n\nwhere the three free parameters *𝛼* hf *, 𝜏* hf *, 𝛽* hf have the same meaning\nas in the AGA fit. We then fitted the NUV observations, using the\nsame least-squares minimization algorithm as before, to find the best\nfitting model of equation 4. As the number of free parameters is the\nsame as in the AGA fitting, the *𝜒* *𝜈* [2] values of the two methods are\ndirectly comparable.\n\n**5 THE RESULTS OF THE FITTING PROCEDURE**\n\n**5.1 Results from the AGA**\n\nWe report the results provided by the AGA in Table 2: the columns\nreport the KIC ID number, the parameters ( *𝛼*, *𝛽*, *𝜏* ) of the best fit,\nalong with their respective positive and negative 1 *𝜎* errors, the *𝜒* *𝜈* [2]\nof the best fit, the number of *GALEX* points, the amplitude ( *𝐴* ) of\nthe *Kepler* light curve and the amplitude of the *GALEX* one, with its\npositive and negative 1 *𝜎* errors (see Sect. 5.3 for the definition of the\namplitude), the classification of the *Kepler* light curve and, finally, the\nPearson’s linear correlation coefficient. These aspects are discussed\nbelow. We summarise these results in Fig. 4, where we show the\ndistributions of the best solutions for the three free parameters.\nThe distribution of the absolute values of *𝛼* implies that the variability in the NUV is typically quite larger than in the visible: the\naverage is | *𝛼* | = 5 *.* 0 and only one star, KIC 9665384, has a slightly\nlower amplitude in the NUV ( *𝛼* = −0 *.* 95 [+] − [2] 1 *[.]* *.* [75] 16 [), however it is one of]\nthe stars with the poorest fit (see below), as the error of the *GALEX*\nobservations is quite high with respect to the flux variation (cf. also\nthe corresponding phase plot in Fig. A1).\nThe *𝜏* distribution is characterised by a peak around a phase difference of 0 (at both *𝜏* = 0 or 1). This means that the NUV curve is\nalmost perfectly in phase (if *𝛼>* 0) or antiphase (if *𝛼<* 0) with the\nvisible one. Another broader peak is centred at about *𝜏* = 0 *.* 5, which\nmay also imply a strong positive or negative correlation, depending\non the shape of the light curve.\nAs expected, the distribution of the brightness offset *𝛽* is centered\naround zero, but in many cases the value of *𝛽* is significant with\nrespect to the NUV amplitude. This shift is due to the much lower\nnumber of data points in the *GALEX* light curves as compared to the\n*Kepler* ones.",
  "In Fig. 5, we present the plot of *𝛼* vs. *𝜏* . If the periodic variability\nin the visible range is anticorrelated with that in NUV, as we should\nexpect for CP2 stars when the null wavelength is located somewhere\nin between the two intervals, we would expect the points to be located in specific places in the plot. These locations also depend on\nthe distribution of the spots on the stellar surface that can produce\nsingle-wave or double-wave light curves. Assuming that the curves,\nat different wavelengths, have the same shape, a perfect anticorrelation is always obtained for *𝜏* = 0 or 1 and *𝛼<* 0. Then, in the case\nof single-wave light curves, a strong anticorrelation is also present\nwhen *𝜏* ∼ 0 *.* 5 and *𝛼>* 0. Regarding the stars with double-wave\nlight curves, anticorrelation can also be achieved at *𝜏* ∼ 0 *.* 5, but for\nnegative values of *𝛼* ; furthermore, also the combinations of *𝜏* ∼ 0 *.* 25\nand *𝜏* ∼ 0 *.* 75, with positive *𝛼*, may indicate anticorrelation. These\nregions are highlighted in Fig. 5. We have divided the sample in stars\nwith single-wave or double-wave light curves (see Table 2) and we\nobserve in the plot that most of the stars lie in or not far from the\ncorrespondent regions of anticorrelation.\nThere are, however, some notable exceptions that we discuss also\nwith the help of Fig. 6, where we show the plot of the Pearson’s\nlinear coefficient *𝑟* that quantifies the correlation between the *GALEX*\nobserved points and the smoothed *Kepler* light curve (at the values\nof *𝜏* of the NUV points).\nFirst, there are three stars (the confirmed CP2 stars KIC 2853320\nand 9665384 and the CP2 star candidate KIC 8362546) whose 1 *𝜎*\nerror covers the entire *𝜏* valid range. We can therefore consider\nthat the AGA left this parameter undetermined. These stars also\nhave very large errors on *𝛼* because the *GALEX* observational errors\nare very large with respect to the maximum flux variation. Furthermore, they show a poor correlation between the NUV and visible\ncurves. Other four stars have *𝑟* ∼ 0: the non-CP stars KIC 10082844\n\nand KIC 5727964 and the confirmed CP2 stars KIC 8773445 and\n\nKIC 1009601. For the three latter cases, the lack of correlation can be\nexplained by the large NUV observational errors with respect to the\namplitude of the flux variation and, in the case of KIC 5727964, also\nfor the small number of *GALEX* data (11). Regarding KIC 10082844,\nthe small | *𝑟* | is mainly caused by the large dispersion of the NUV flux\nvalues in the interval 0 *< 𝜙<* 0 *.* 3; this spread also produces a large\n*𝜒* *𝜈* [2] of the AGA fit. Non-CP stars will be discussed in more detail in\nSection 6.2.\n\nWe also recall the fact that while for the majority of mCP stars, the\nnull-wavelength region lies somewhere between the *GALEX* UV and\n*Kepler* passbands (and hence these stars are expected to show the\ncorresponding anticorrelated light curve behaviour), for some mCP\nstars, the null wavelength is situated in the optical region, for example\nbetween the ZTF *𝑔* and *𝑟* filters (e.g. Faltová et al. 2021).\nFinally, the non-CP star KIC 5213466 shows a strong positive\ncorrelation between the *Kepler* and the *GALEX* NUV bands, with\nthe only caveat that it is one of the stars with the smallest number of *GALEX* observations. This finding corroborates the result of\nHümmerich et al. (2018) that this star is not an ACV variable.\n\nAll the other stars show a clear anticorrelation between the flux of\n\nthe *GALEX* NUV and *Kepler* wavelength intervals.\n\nMNRAS **000**, 1–14 (2024)\n\n\n-----\n\n**Table 2.** Results from the AGA.\n\nKIC *𝛼* + *𝜎* *𝛼* − *𝜎* *𝛼* *𝜏* + *𝜎* *𝜏* − *𝜎* *𝜏* *𝛽* + *𝜎* *𝛽* − *𝜎* *𝛽* *𝜒* *𝜈* [2] n *𝐴* Kepler *𝐴* NUV + *𝜎* *𝐴* NUV − *𝜎* *𝐴* NUV wave *𝑟*\n(mmag) (mmag) (mmag) (mmag) (mmag) (mmag) (mmag)\n\n2853320 2.33 2.74 5.90 0.535 0.465 0.535 -2.7 8.0 8.1 0.74 18 5.10 11.86 13.97 11.86 1 -0.29\n\n2969628 3.06 0.86 0.91 0.551 0.034 0.047 1.4 3.5 3.7 2.63 21 5.99 18.37 5.13 5.43 1 -0.52\n\n3326428 -10.24 1.21 1.15 0.996 0.009 0.008 -49.8 12.2 11.2 3.48 18 16.65 170.58 19.22 20.15 2 -0.92\n\n3945892 2.50 0.69 0.66 0.517 0.045 0.046 8.2 6.9 6.9 2.13 20 13.61 33.98 9.46 8.99 1 -0.75\n\n4171302 -8.15 1.83 1.83 0.025 0.027 0.026 -5.7 5.2 5.4 2.28 18 4.34 35.37 7.95 7.93 1 -0.79\n\n5000179 -4.43 1.20 1.19 0.497 0.026 0.019 -5.1 11.2 11.2 1.43 15 14.07 62.39 16.74 16.84 2 -0.77\n\n5213466 3.95 1.85 1.87 0.935 0.081 0.093 5.9 20.0 17.9 1.17 11 12.81 50.63 23.75 23.95 1 0.72\n\n5727964 -8.79 5.79 5.53 0.830 0.046 0.036 5.9 17.3 16.3 0.65 11 4.79 42.12 26.47 27.74 2 0.20\n\n5739204 -2.06 1.81 1.83 0.979 0.168 0.121 0.0 18.9 18.5 1.04 18 16.76 34.45 30.68 30.38 1 -0.47\n\n5774743 2.35 1.29 1.30 0.687 0.079 0.074 7.4 7.6 7.7 0.47 15 6.97 16.35 8.98 9.06 1 -0.58\n\n6715809 8.66 1.86 1.83 0.628 0.028 0.032 -17.4 10.4 10.7 1.21 18 9.95 86.23 18.56 18.17 1 -0.79\n\n6950556 1.78 0.75 0.73 0.284 0.042 0.032 -5.0 5.5 5.7 0.82 19 11.62 20.73 8.74 8.49 2 -0.72\n\n7628336 7.48 2.57 2.47 0.394 0.016 0.041 -4.8 5.3 5.2 3.80 18 3.35 25.04 8.59 8.28 1 -0.43\n\n7976845 -4.74 2.49 2.61 0.015 0.049 0.062 -0.1 36.1 35.9 1.29 18 24.83 117.70 64.86 61.74 1 -0.62\n\n8362546 15.09 29.18 47.14 0.685 0.315 0.685 2.6 13.5 20.6 0.96 18 0.96 14.44 27.92 14.44 1 0.13\n\n8386865 5.16 1.07 1.08 0.773 0.020 0.019 -2.8 5.0 4.9 2.95 16 6.35 32.77 6.80 6.84 2 -0.72\n\n8569986 -4.82 2.58 2.63 0.016 0.054 0.052 -18.6 16.6 17.3 1.17 12 9.74 46.90 25.60 25.09 1 -0.68\n\n8773445 -4.46 2.92 2.87 0.300 0.056 0.122 -14.3 16.0 15.9 0.61 17 9.68 43.19 27.77 28.28 1 -0.21\n\n9541567 -2.50 0.29 0.32 0.971 0.011 0.013 -0.6 4.0 3.8 4.44 16 24.15 60.44 7.66 6.94 1 -0.89\n\n9665384 -0.95 2.75 1.16 0.853 0.147 0.853 -3.5 11.8 8.8 0.55 16 15.60 14.86 18.05 14.86 1 0.03\n\n10082844 1.59 0.73 0.73 0.412 0.052 0.060 -4.1 10.7 10.7 4.23 17 18.00 28.59 13.11 13.06 1 -0.16\n\n10090722 6.44 2.06 2.05 0.395 0.044 0.041 4.4 6.5 6.6 2.41 15 4.37 28.12 8.98 8.97 1 -0.57\n\n10096019 -6.51 4.49 4.51 0.589 0.096 0.088 -1.1 12.5 12.4 0.86 22 3.75 24.42 16.94 16.84 2 0.01\n\n10685175 5.62 1.47 1.36 0.449 0.040 0.029 1.8 5.8 5.2 2.44 20 5.82 32.70 8.58 7.90 1 -0.74\n\n10905824 -3.06 2.17 3.57 0.081 0.147 0.081 -0.9 9.5 9.9 2.16 12 7.35 22.50 26.22 15.93 2 -0.36\n\n10959320 -5.43 1.92 1.88 0.995 0.026 0.037 15.4 12.0 12.2 2.50 17 10.57 57.38 19.82 20.26 2 -0.66\n\n11154043 -4.85 1.11 1.10 0.035 0.045 0.034 0.5 8.0 6.7 4.69 20 6.34 30.72 6.95 7.04 1 -0.62\n\n11465134 2.69 0.68 0.64 0.551 0.039 0.042 3.5 4.9 4.6 2.46 15 8.79 23.68 5.98 5.59 1 -0.66\n\n\n-----\n\n**5.2 Comparison with the results from the harmonic fitting**\n\nThe results of the harmonic fitting (HF) method are very similar to\nthose obtained with the AGA, as shown in Fig. 7. The only significant\ndifference is the value of *𝛼* of the candidate CP2 star KIC 8362546,\nwhich changes from 15.1, in the AGA fit, to 6.6, with the HF; however,\nthis star is one of the three objects that have a value of *𝜏* completely\nundetermined.\nFor some of the stars, some of the minimum *𝜒* *𝜈* [2] obtained with\nthe HF are slightly lower than the corresponding AGA values; this is\npossibly because the shape of the *Kepler* curve, used as a reference,\nis different in the two cases: for the AGA, it is a running average of\nthe data points, while for the HF, it is the harmonic curve of Eq. 3.\nConsidering the very high level of agreement between the two\nfitting methods, in the rest of this work we only use the results from\nthe AGA, as the assumption of equal shape for the visible and NUV\nlight curves is more rigorous.\n\n**5.3 Correlation of the light curve amplitudes with stellar**\n**parameters**\n\nSince the variability of CP2 stars is due to the different intensity of\nthe line blanketing, at different wavelength intervals and with the\nphase of the rotation period, it is interesting to investigate if there is\na correlation of the variability amplitude with the stellar parameters\nthat affect the intensity of the absorption, in particular with effective\n\ntemperature.\nWith that goal in mind, we adopt the main atmospheric parameters\n(the effective temperature *𝑇* eff and the surface gravity log *𝑔* ) derived\nby using the calibrations of Paunzen (2024). These are based on\nfour commonly used references (Stassun et al. 2019; Anders et al.\n2022; Fouesneau et al. 2023; Zhang et al. 2023) of astrophysical\nparameters, which were then refined for the different subgroups of\nCP stars. We report the *𝑇* eff and log *𝑔* for the whole sample, along\nwith their associated uncertainties, in Table 3: the sample of 28\nstars covers, quite evenly, the *𝑇* eff range 7500–13000 K, where mainsequence A-type to late B-type objects are located, and the log *𝑔*\nvalues are consistent with main-sequence objects. Note that the three\nstars with undetermined *𝜏* (the confirmed CP2 stars KIC 2853320\nand KIC 9665384 and the CP2 star candidate KIC 8362546) are the\nhottest of the sample and the catalogue of Mathur et al. (2017) places\nthem as the farthest of the sample.\nWe define the amplitude of the optical variability as half the difference between the maximum and the minimum values of the smoothed\n\n*Kepler* phase curve, while the NUV amplitude is obtained by multiplying the optical amplitude by *𝛼* . In Fig. 8, we show the plot of\nthe optical amplitude, the amplitude ratio *𝛼*, and the NUV amplitude\nwith respect to the *𝑇* eff . The Pearson’s linear correlation coefficient\n*𝑟*, calculated excluding the 4 non-CP2 stars and indicated in each\npanel, shows that there is no correlation between each of the amplitudes and *𝑇* eff . This result does not change significantly if we also\nexclude the three stars with undetermined *𝜏* : only the correlation\nbetween *𝑇* eff and the amplitude of the *Kepler* variability becomes\nmildly positive ( *𝑟* = +0 *.* 51), while the *𝑇* eff correlation with the other\ntwo quantities remains insignificant (| *𝑟* | *<* 0 *.* 23). The lack of significant correlation of the amplitudes also stands with log *𝑔* : in all cases\n| *𝑟* | *<* 0 *.* 22. Likewise, no correlation is found with the radius and the\nmass (| *𝑟* | *<* 0 *.* 23), taken from the Mathur et al. (2017) catalogue.\nWe also searched for correlations with stellar age. Berger et al.\n(2020) determined the age of a large number of stars in the *Ke-*\n*pler* prime field, collecting photometric and spectroscopic data from\nseveral sources and the parallaxes from Gaia DR2 (e.g., Gaia Collab",
  "**Table 3.** Effective temperatures and surface gravities of the sample stars,\nderived using the calibration of Paunzen (2024).\n\nKIC *𝑇* eff *𝜎* *𝑇* eff log *𝑔* *𝜎* log *𝑔*\n(K) (K) (dex) (dex)\n\n2853320 11483 1427 4.075 0.181\n\n2969628 8713 119 3.989 0.023\n\n3326428 8792 833 3.872 0.048\n\n3945892 7885 306 4.014 0.070\n\n4171302 8504 469 4.052 0.088\n\n5000179 10424 902 3.992 0.071\n\n5213466 8390 240 3.913 0.110\n\n5727964 7982 176 3.995 0.047\n\n5739204 9367 325 4.047 0.037\n\n5774743 8067 150 3.946 0.048\n\n6715809 9046 320 4.207 0.036\n\n6950556 10202 390 3.990 0.077\n\n7628336 7829 233 4.029 0.045\n\n7976845 9975 1469 4.057 0.135\n\n8362546 13057 1404 4.138 0.081\n\n8386865 9120 85 4.038 0.044\n\n8569986 8740 194 4.048 0.053\n\n8773445 9521 495 3.993 0.034\n\n9541567 11332 1788 3.949 0.088\n\n9665384 12491 566 4.005 0.172\n\n10082844 9076 502 4.041 0.159\n\n10090722 11009 1258 3.785 0.096\n\n10096019 7785 32 4.067 0.078\n\n10685175 8143 192 4.179 0.103\n\n10905824 9178 238 4.136 0.085\n\n10959320 9941 777 4.037 0.116\n\n11154043 9091 326 3.949 0.046\n\n11465134 9171 383 3.906 0.078\n\noration et al. 2018) and making use of the isochrone fitting method.\nFrom their catalogue, we acquired the age and the fraction of the\nage on the main sequence for 23 stars. We computed *𝑟* for both agerelated quantities versus the *Kepler* amplitude, the NUV amplitude\nand their ratio *𝛼* : in no case the value of | *𝑟* | is larger than 0.33, indicating, once again, that there is no significant correlation between\nthese quantities.\n\n*5.3.1 Exploring the lack of correlation with theoretical models*\n\nSince, as we already said, the variability of the CP2 stars depends\non the differential line blanketing on the stellar surface, the lack of\nsignificant correlations between the amplitude of the light curves and\nthe investigated stellar parameters (especially *𝑇* eff ) is quite puzzling.\nAimed at understanding the reasons behind that absence of correlations, we conducted an experiment using a small library of model\natmospheres and theoretical spectral energy distributions in the *𝑇* eff\nrange 7000–12000 K, compatible with our target parameters. We\nconsidered the case in which the variability is due to the presence of\na spot, with typical abundances of a CP2 star, on the surface of a star\nwith otherwise solar chemical composition. [8]\n\n8 We note that the bulk chemical composition of CP2 stars is unknown and\ntherefore the choice of an appropriate chemical composition for theoretical\ntracks remains an open question (cf. the discussion in Bagnulo et al. 2006).\nHowever, if atomic diffusion is accepted as the main mechanism for producing\nthe observed chemical peculiarities, the assumption of an overall abundance\nclose to solar seems reasonable (cf. also Hümmerich et al. 2020).\n\nMNRAS **000**, 1–14 (2024)\n\n\n-----",
  "**Figure 4.** Distributions of the best parameters provided by the AGA for the 28 sample stars. *Left panel:* distribution of the absolute value of *𝛼* (amplitude ratio).\n*Central panel:* distribution of *𝜏* (phase difference). *Right panel:* distribution of *𝛽* (magnitude offset). The light blue colour indicates the confirmed CP2 stars,\nthe dark blue shows the candidate CP2 object, while the red displays non-CP2 stars.\n\n\n**Figure 5.** Amplitude ratio *𝛼* vs. phase difference *𝜏*, as obtained with the AGA.\nThe colour code is the same as in Fig. 4: the light blue symbols indicate\nthe confirmed CP2 stars, the dark blue symbol marks the CP2 candidate\n(KIC 8362546), while the red symbols locate the non-CP2 stars. Stars with\nsingle-wave light curves are indicated by circles, while the triangles mark\nthe double-wave stars (see the classification in Table 2). The three stars that\nhave an undetermined value of *𝜏* are identified with a yellow cross on the\nsymbol. The striped regions show the locations in the plane where a strong\nanticorrelation between the *Kepler* and the *GALEX* light curves is to be\nexpected: cyan for single-wave light curves, red for the double-wave light\ncurves. Some notable objects discussed in the text are identified with their\n\nKIC number.\n\nWe first construct a chemical composition that may represent a\n‘generic CP2 star’. For this type of stars, we extracted the spectroscopic abundance of individual chemical elements, relative to the\nSun, from the catalogue of Ghazaryan et al. (2018). The total number of CP2 stars in this catalogue is 188. 71 chemical elements have\nmeasurements, but 25 elements have no more than three values, while\nsix elements have more than 100 data points (O, Si, Cr, Fe, Pr, Nd).\n\nMNRAS **000**, 1–14 (2024)\n\n\n**Figure 6.** The plot of the Pearson’s linear coefficient *𝑟* vs. *𝜏* . colours and\nsymbols are the same as in Fig. 5.\n\n**Figure 7.** Comparison of the best fit parameters and the *𝜒* *𝜈* [2] from AGA and\nharmonic fit. In all panels, the red cross indicates the location of the star\nKIC 8362546.\n\nFor each chemical element, we computed the mean and the standard\ndeviation of the abundance values.\n\nFor the average chemical composition, we adopted the mean value\nof each element with four or more measurements or the value of a\n\nlinear fit for the other elements, as shown in Fig. 9. These abundances\nare lower than in the Sun for just 9 elements, most notably for He and\n\n\n-----\n\n**Figure 8.** Plots of the *Kepler* light-curve amplitude (upper panel), the absolute\nvalue of the amplitude ratio *𝛼* (middle panel) and the NUV light-curve\namplitude (lower panel) vs. *𝑇* eff . In the upper panel, the amplitude error is\nomitted, while in the middle and lower panels, for the sake of clarity, the error\non *𝑇* eff is not plotted. Light blue symbols indicate confirmed CP2 stars, the\ndark blue one displays the CP2 candidate, while the red symbols show the non\nCP2 stars. The three stars that have an undetermined value of *𝜏* are identified\n\nwith a yellow cross on the symbol. In the middle panel, the diamonds mark\nthe locations of the theoretical *𝛼* values obtained with synthetic spectra, as\nexplained in Sect. 5.3.1. In each panel the linear correlation coefficient value\n\nis also indicated.\n\nO, but also for Be, B, C, N, Mg, Al and Zn. As the reference solar\nchemical composition, we considered the abundances of Asplund\net al. (2021).\nWe then computed, using the Fortran code DFSYNTHE [9] by\nRobert Kurucz (see, e.g., Castelli 2005), the opacity distribution\nfunctions (ODFs; i.e. the pre-computed library of opacity for suitable intervals in temperature, pressure, and microturbulent velocity)\nfor the solar and for the generic CP2 chemical compositions. In\nthis way, we were able to quickly compute the model atmospheres\nand SEDs for both chemical compositions, a fixed log *𝑔* = 4 dex\nand *𝑇* eff = 7000 *,* 8000 *,* 9000 *,* 10000 *,* 11000 *,* 12000 K with Kurucz’s\nATLAS9 code (e.g., Kurucz 2005). Convection is described by the\nmixing-length theory (MLT; Böhm-Vitense 1958); to treat its limited\ncontribution to energy transport in A-type stars, we follow the indication of Smalley (2004) to adopt the MLT parameter *𝛼* = *𝑙* / *𝐻* *𝑝* = 0 *.* 5.\nThe models have 72 layers and the SEDs are computed in 1221\nwavelength intervals. In Fig. 10, we present the differences of the\ntemperature profiles between the CP2 and the solar model atmospheres and their ratio of the surface flux, for each *𝑇* eff . The hotter\nlayers, at log *𝜏* Ross *>* 0 *.* 5, of the 7000 K CP2 model are caused by a\nlower level of convective flux, producing a steeper temperature gradi\n9 `http://wwwuser.oats.inaf.it/castelli/sources/dfsynthe.`\n```\nhtml\n\n```",
  "ent, while the models at warmer *𝑇* eff have quite similar temperatures\nin the optical thick regime. All the CP2 models are slightly warmer\nthan the corresponding solar ones in the range −2 ≲ log *𝜏* Ross ≲ 0,\nwhile they become colder in the upper layers.\nThe low-resolution theoretical SEDs (Fig. 10) show that, with\nrespect to the solar reference, the CP2 models produce lower emission\nin the *GALEX* NUV passband over the whole *𝑇* eff interval. This\ndepression is compensated by a positive excess in the optical regime\nand all through the *Kepler* passband. This results imply that in our\nscenario (a spot with CP2 abundances on a star with solar chemical\ncomposition), the theoretical *GALEX* NUV and *Kepler* light curves\nare anticorrelated. Furthermore, since in the NUV the flux decreases\nmore strongly, with decreasing *𝑇* eff, than it increases in the visible,\nwe expect a significant dependence on *𝑇* eff .\nIn order to quantify this dependence, we constructed synthetic light\ncurves at each *𝑇* eff . We described the time-invariant distribution of\nspots with CP2 abundances by a filling factor *𝜔* ( *𝜙* ), that indicates the\nfraction of the observed surface occupied by the spots and depends\non the phase *𝜙* . Therefore, the flux can be written as *𝑓* = *𝜔𝑓* CP2 +(1−\n*𝜔* ) *𝑓* ⊙ . While the amplitude of the light curve in a single passband\ndepends on *𝜔* ( *𝜙* ), the amplitude ratio *𝛼* does not.\nWe measured *𝛼* between the synthetic NUV and the *Kepler* light\ncurves at all *𝑇* eff . The results confirm that *𝛼* strongly depends on *𝑇* eff :\nas shown in the middle panel of Fig. 8 by the black diamonds, the\nvalue of | *𝛼* | smoothly decreases with *𝑇* eff ; it has a maximum value\nof | *𝛼* | = 4 *.* 98 at 7000 K, while it becomes lower than 1 for the two\nhotter models (0.71 at 11000 K and 0.48 at 12000 K).\nWe note that the vast majority of the stars in our sample have\n| *𝛼* | larger than the theoretical value from our experiment. This suggests that the assumed chemical compositions in our scenario are\ninadequate to describe the average properties of the sample.\nThe experiment also shows that, at fixed abundances, there is a\nclear correlation between the light curve amplitude ratios and *𝑇* eff,\nwhile our sample shows neither a correlation with this quantity nor\nwith the other stellar properties that we explored. The data from\nthe catalogue of Ghazaryan et al. (2018) show a large dispersion\namong the CP2 stars, sometimes up to four orders of magnitude,\nin the abundance of individual elements. Therefore, each individual\nchemical composition may be quite different from one star to the\nother, and it is the dominant factor that generates the *𝛼* value, as it\ndetermines the line blanketing in the different wavelength bands.\n\n**6 NOTES ON SINGLE OBJECTS**\n\nThis section briefly discusses some noteworthy objects.\n\n**6.1 The CP2 star candidates KIC 2969628, 3326428 and**\n\n**8362546**\n\nInitially, three candidate CP2 stars from Hümmerich et al. (2018)\nentered our sample (KIC 2969628, 3326428 and 8362546). Two\nof them, KIC 2969628 and 3326428 clearly show anticorrelation\nbetween the NUV and visible light curves, which points to these\nobjects being indeed CP2 stars. At a subsequent point in our study,\nwe were able to confirm both objects as classical CP2 stars by our\nown spectroscopic observations (cf. Section 3), which shows that the\nassumptions drawn from the variability pattern have been valid.\nOn the other hand, the mCP star candidate KIC 8362546 does not\nshow any correlation between the NUV and visible light curves and\nhas an undetermined value of *𝜏*, which favours the scenario that this\nstar is not a mCP star or that the null wavelength region is located at\n\nMNRAS **000**, 1–14 (2024)\n\n\n-----",
  "**Figure 9.** Abundances of the CP2 stars in the catalogue of Ghazaryan et al. (2018) (black diamonds). The red dots indicate the adopted abundance values for a\ngeneric CP2 star, as explained in the text.\n\nbluer wavelengths than the *GALEX* NUV passband. To distinguish\nbetween the two hypotheses, spectroscopic or far-UV observations\nare needed.\n\n**6.2 The non-mCP stars KIC 5213466, 5727964, 8569986, and**\n\n**10082844**\n\nThe four non-CP stars in our sample should not show the distinctive\nanti-correlation between the NUV and visible light curves as ACV\nvariables do. In agreement with this expectation, the non-CP star\nKIC 5213466 (spectral type A1 V; Hümmerich et al. 2018) shows\na strong positive correlation between both wavelength bands. It is,\nhowever, one of the stars with the minimum number of GALEX\nobservations. Nevertheless, this finding corroborates the result of\nHümmerich et al. (2018) that this star is not an ACV star. Its *Kepler*\nlight curve shows large variations in amplitude and shape which\nare not expected in this type of variables. This is also true for the\nnon-CP stars KIC 5727964 (A6 V; Hümmerich et al. 2018) and\nKIC 10082844 (A0 V; Hümmerich et al. 2018), which, in agreement\nwith this, do not show any significant correlation between the NUV\nand visible light curves. However, the amplitudes of the variability\nare small in both objects and the NUV errors quite large. Lastly,\nthe non-CP star KIC 8569986 (A2 V; Hümmerich et al. 2018) has\na significant Pearson’s *𝑟* = −0 *.* 68; nevertheless, the low number\nof the *GALEX* points, their dispersion and a large phase gap in its\nphase diagram (see Appendix A) cast doubts on the real presence of\nan anticorrelation. A campaign of simultaneous photometric visible\nand ultraviolet (preferably in the far-UV) observations and/or highresolution spectra of this star would be needed to clarify the nature\nof this star.\n\n\n**Figure 10. Upper panel** : profiles of the temperature difference between\nmodel atmospheres with CP2 and solar chemical composition, at each *𝑇* eff .\nThe colour scale is indicated in the plot. The 7000 K model has the largest\ndifference of 3485 K at log *𝜏* Ross = 1 *.* 875. **Lower panel** : surface flux ratio\nbetween the CP2 and the solar theoretical SEDs, which were smoothed with\na five-point boxcar. The colour scale is the same as in the upper panel. The\ndash-dotted lines represent the *GALEX* NUV and the *Kepler* filter responses,\nin an arbitrary scale.\n\nMNRAS **000**, 1–14 (2024)\n\n\n**7 CONCLUSIONS**\n\nBased on observational data from the GALEX and *Kepler* prime\nmissions, we carried out a study of the properties of the photometric\nvariability of a sample of 22 spectroscopically confirmed CP2 stars,\none photometrically confirmed mCP star (KIC 7976845), one mCP\nstar candidate (KIC 8362546), and four non-CP stars (KIC 5213466,\n5727964, 8569986, 10082844) in the NUV and visible wavelength\n\n\n-----\n\nregions. To date, this is the largest sample of mCP stars studied in\nthe NUV wavelength region. We furthermore investigated the presence of a correlation of the variability amplitudes in both wavelength\nregions with stellar parameters such as effective temperature and surface gravity. To connect our findings to theoretical considerations, we\ncalculated model atmospheres, spectral energy distribution profiles\nand synthetic light curves.\nThe main findings are summarised in the following:\n\n - We observe antiphase variations between the NUV and optical\nlight curves in the majority of mCP stars. This indicates that the\npresence of this particular variability pattern is common also in the\nNUV wavelength interval and not only at FUV wavelengths. It also\nmeans that the combination of NUV and visible observations are\n\nsuitable for identifying mCP star candidates.\n\n - While the theoretical calculations show that, at fixed abundances, a clear correlation between the light curve amplitude ratios\nand *𝑇* eff is expected, our sample does not show a correlation with\nany of the investigated properties. This may be due to the highly\nindividualistic abundance patterns of CP2 stars, which are the main\ncontributors to the line blanketing in different wavelength bands.\n\n**ACKNOWLEDGEMENTS**\n\nThis research has made use of the SIMBAD database, operated at\nCDS, Strasbourg, France.\n\n**DATA AVAILABILITY**\n\nThe *GALEX* NUV data, used in this paper, will be published in\na future publication, but can be requested in advance to the first\nauthor. The *Kepler* light curves and the LAMOST spectra are publicly\navailable online.\n\n**REFERENCES**\n\n[Anders F., et al., 2022, A&A, 658, A91](http://dx.doi.org/10.1051/0004-6361/202142369)\n[Asplund M., Amarsi A. M., Grevesse N., 2021, A&A, 653, A141](http://dx.doi.org/10.1051/0004-6361/202140445)\n[Aurière M., et al., 2007, A&A, 475, 1053](http://dx.doi.org/10.1051/0004-6361:20078189)\n[Avni Y., 1976, ApJ, 210, 642](http://dx.doi.org/10.1086/154870)\n[Babcock H. W., 1947, ApJ, 105, 105](http://dx.doi.org/10.1086/144887)\n[Babusiaux C., et al., 2022, arXiv e-prints, p. arXiv:2206.05989](https://ui.adsabs.harvard.edu/abs/2022arXiv220605989B)\nBagnulo S., Landstreet J. D., Mason E., Andretta V., Silaj J., Wade G. A.,\n[2006, A&A, 450, 777](http://dx.doi.org/10.1051/0004-6361:20054223)\n\n[Balona L. A., 2017, MNRAS, 467, 1830](http://dx.doi.org/10.1093/mnras/stx265)\n[Bauer-Fasching B., et al., 2024, A&A, 687, A211](http://dx.doi.org/10.1051/0004-6361/202347476)\n[Bellm E. C., et al., 2019, PASP, 131, 018002](http://dx.doi.org/10.1088/1538-3873/aaecbe)\nBerger T. A., Huber D., van Saders J. L., Gaidos E., Tayar J., Kraus A. L.,\n[2020, AJ, 159, 280](http://dx.doi.org/10.3847/1538-3881/159/6/280)\n\nBertone E., Sachkov M., Olmedo D., Olmedo M., Chavez M., 2020, in Neiner\n\nC., Weiss W. W., Baade D., Griffin R. E., Lovekin C. C., Moffat A. F. J.,\neds, Stars and their Variability Observed from Space. pp 199–200\n[Bianchi L., Shiao B., Thilker D., 2017, ApJS, 230, 24](http://dx.doi.org/10.3847/1538-4365/aa7053)\n[Böhm-Vitense E., 1958, Z. Astrophys., 46, 108](https://ui.adsabs.harvard.edu/abs/1958ZA.....46..108B)\nBorucki W., et al., 2008, in Sun Y.-S., Ferraz-Mello S., Zhou J.-L., eds, IAU\nSymposium Vol. 249, Exoplanets: Detection, Formation and Dynamics.\n[pp 17–24, doi:10.1017/S174392130801630X](http://dx.doi.org/10.1017/S174392130801630X)\n[Borucki W. J., et al., 2010, Science, 327, 977](http://dx.doi.org/10.1126/science.1185402)\n[Brown T. M., Latham D. W., Everett M. E., Esquerdo G. A., 2011, AJ, 142,](http://dx.doi.org/10.1088/0004-6256/142/4/112)\n\n[112](http://adsabs.harvard.edu/abs/2011AJ....142..112B)\n\n[Cantó J., Curiel S., Martínez-Gómez E., 2009, A&A, 501, 1259](http://dx.doi.org/10.1051/0004-6361/200911740)\n\n[Carrasco J. M., et al., 2021, A&A, 652, A86](http://dx.doi.org/10.1051/0004-6361/202141249)",
  "Castelli F., 2005, Memorie della Societa Astronomica Italiana Supplementi,\n\n[8, 34](https://ui.adsabs.harvard.edu/abs/2005MSAIS...8...34C)\nConroy K. E., Prša A., Stassun K. G., Orosz J. A., Fabrycky D. C., Welsh\n[W. F., 2014, AJ, 147, 45](http://dx.doi.org/10.1088/0004-6256/147/2/45)\nCui X.-Q., et al., 2012, Research in Astronomy and Astrophysics, 12, 1197\n[Faltová N., et al., 2021, A&A, 656, A125](http://dx.doi.org/10.1051/0004-6361/202141534)\n\n[Fouesneau M., et al., 2023, A&A, 674, A28](http://dx.doi.org/10.1051/0004-6361/202243919)\n\n[Gaia Collaboration et al., 2016, A&A, 595, A1](http://dx.doi.org/10.1051/0004-6361/201629272)\n\n[Gaia Collaboration et al., 2018, A&A, 616, A1](http://dx.doi.org/10.1051/0004-6361/201833051)\n[Gaia Collaboration et al., 2022, arXiv e-prints, p. arXiv:2208.00211](https://ui.adsabs.harvard.edu/abs/2022arXiv220800211G)\n[Gao Q., Xin Y., Liu J.-F., Zhang X.-B., Gao S., 2016, ApJS, 224, 37](http://dx.doi.org/10.3847/0067-0049/224/2/37)\n[Garrison R. F., Gray R. O., 1994, AJ, 107, 1556](http://dx.doi.org/10.1086/116967)\n[Ghazaryan S., Alecian G., Hakobyan A. A., 2018, MNRAS, 480, 2953](http://dx.doi.org/10.1093/mnras/sty1912)\n[Ghazaryan S., Alecian G., Hakobyan A. A., 2019, MNRAS, 487, 5922](http://dx.doi.org/10.1093/mnras/stz1678)\n[Gilliland R. L., et al., 2010, ApJ, 713, L160](http://dx.doi.org/10.1088/2041-8205/713/2/L160)\nGray R. O., Corbally C. J., 2009, Stellar Spectral Classification\n[Gray R. O., Garrison R. F., 1987, ApJS, 65, 581](http://dx.doi.org/10.1086/191237)\n[Gray R. O., Garrison R. F., 1989a, ApJS, 69, 301](http://dx.doi.org/10.1086/191315)\n[Gray R. O., Garrison R. F., 1989b, ApJS, 70, 623](http://dx.doi.org/10.1086/191349)\n[Gröbel R., Hümmerich S., Paunzen E., Bernhard K., 2017, New Astron., 50,](http://dx.doi.org/10.1016/j.newast.2016.07.012)\n\n[104](https://ui.adsabs.harvard.edu/abs/2017NewA...50..104G)\n\nHamuy M., Walker A. R., Suntzeff N. B., Gigoux P., Heathcote S. R., Phillips\n[M. M., 1992, PASP, 104, 533](http://dx.doi.org/10.1086/133028)\n\n[Howell S. B., et al., 2014, PASP, 126, 398](http://dx.doi.org/10.1086/676406)\n\n[Hümmerich S., et al., 2018, A&A, 619, A98](http://dx.doi.org/10.1051/0004-6361/201832938)\n\n[Hümmerich S., Paunzen E., Bernhard K., 2020, A&A, 640, A40](http://dx.doi.org/10.1051/0004-6361/202037750)\n\n[Jamar C., 1977, A&A, 56, 413](https://ui.adsabs.harvard.edu/abs/1977A&A....56..413J)\n[Jenkins J. M., et al., 2010, ApJ, 713, L120](http://dx.doi.org/10.1088/2041-8205/713/2/L120)\n[Koch D. G., et al., 2010, ApJ, 713, L79](http://dx.doi.org/10.1088/2041-8205/713/2/L79)\nKrtička J., Janík J., Marková H., Mikulášek Z., Zverko J., Prvák M., Skarka\n\n[M., 2013, A&A, 556, A18](http://dx.doi.org/10.1051/0004-6361/201221018)\n[Krtička J., Mikulášek Z., Lüftinger T., Jagelka M., 2015, A&A, 576, A82](http://dx.doi.org/10.1051/0004-6361/201425097)\n[Krtička J., et al., 2019, A&A, 625, A34](http://dx.doi.org/10.1051/0004-6361/201834937)\n\nKrtička J., Mikulášek Z., Prvák M., Niemczura E., Leone F., Wade G., 2020,\n\n[MNRAS, 493, 2140](http://dx.doi.org/10.1093/mnras/staa378)\nKurucz R. L., 2005, Memorie della Societa Astronomica Italiana Supple[menti, 8, 14](https://ui.adsabs.harvard.edu/abs/2005MSAIS...8...14K)\n[Landstreet J. D., 1982, ApJ, 258, 639](http://dx.doi.org/10.1086/160114)\n[Lanz T., Artru M. C., Le Dourneuf M., Hubeny I., 1996, A&A, 309, 218](https://ui.adsabs.harvard.edu/abs/1996A&A...309..218L)\n[Leckrone D. S., 1974, ApJ, 190, 319](http://dx.doi.org/10.1086/152879)\nLuo A. L., Zhao Y. H., Zhao G., et al. 2018, VizieR Online Data Catalog,\n\n[5153, 0](https://ui.adsabs.harvard.edu/abs/2018yCat.5153....0L)\n[Manfroid J., Mathys G., 1986, A&AS, 64, 9](https://ui.adsabs.harvard.edu/abs/1986A&AS...64....9M)\nMartin C., GALEX Science Team 2003, in American Astronomical Society\nMeeting Abstracts. p. 96.01\n[Masci F. J., et al., 2019, PASP, 131, 018003](http://dx.doi.org/10.1088/1538-3873/aae8ac)\n[Mathur S., et al., 2017, ApJS, 229, 30](http://dx.doi.org/10.3847/1538-4365/229/2/30)\nMikulášek M., Zverko J., Krtička J., Janík J., Žižńovský J., Zejda M., 2007, in\nRomanyuk I. I., Kudryavtsev D. O., Neizvestnaya O. M., Shapoval V. M.,\neds, Physics of Magnetic Stars. pp 300–309\n[Molnar M. R., 1973, ApJ, 179, 527](http://dx.doi.org/10.1086/151892)\n[Molnar M. R., 1975, AJ, 80, 137](http://dx.doi.org/10.1086/111725)\n[Molnar M. R., Mallama A. D., Holm A. V., Soskey D. G., 1976, ApJ, 209,](http://dx.doi.org/10.1086/154703)\n[146](https://ui.adsabs.harvard.edu/abs/1976ApJ...209..146M)\n\n[Morrissey P., et al., 2007, ApJS, 173, 682](http://dx.doi.org/10.1086/520512)\n[Nielsen M. B., Gizon L., Schunker H., Karoff C., 2013, A&A, 557, L10](http://dx.doi.org/10.1051/0004-6361/201321912)\nOlmedo M., Lloyd J., Mamajek E. E., Chávez M., Bertone E., Martin D. C.,\n[Neill J. D., 2015, ApJ, 813, 100](http://dx.doi.org/10.1088/0004-637X/813/2/100)\n[Osawa K., 1965, Annals of the Tokyo Astronomical Observatory, 9, 121](https://ui.adsabs.harvard.edu/abs/1965AnTok...9..121O)\n[Paunzen E., 2024, A&A, 683, L7](http://dx.doi.org/10.1051/0004-6361/202348086)\n[Paunzen E., Prišegen M., 2022, A&A, 667, L10](http://dx.doi.org/10.1051/0004-6361/202244839)\n[Paunzen E., Stütz C., Maitzen H. M., 2005, A&A, 441, 631](http://dx.doi.org/10.1051/0004-6361:20053001)\n\n[Preston G. W., 1974, ARA&A, 12, 257](http://dx.doi.org/10.1146/annurev.aa.12.090174.001353)\n[Prvák M., Liška J., Krtička J., Mikulášek Z., Lüftinger T., 2015, A&A, 584,](http://dx.doi.org/10.1051/0004-6361/201526647)\n\n[A17](https://ui.adsabs.harvard.edu/abs/2015A&A...584A..17P)\n\n[Renson P., Manfroid J., 2009, A&A, 498, 961](http://dx.doi.org/10.1051/0004-6361/200810788)\n\nSamus N. N., Kazarovets E. V., Durlevich O. V., Kireeva N. N., Pastukhova\n[E. N., 2017, Astronomy Reports, 61, 80](http://dx.doi.org/10.1134/S1063772917010085)\n\nMNRAS **000**, 1–14 (2024)\n\n\n-----",
  "Shulyak D., Krtička J., Mikulášek Z., Kochukhov O., Lüftinger T., 2010,\n\n[A&A, 524, A66](http://dx.doi.org/10.1051/0004-6361/201015094)\nSiegmund O. H. W., et al., 2004, in Hasinger G., Turner M. J. L., eds, Society\nof Photo-Optical Instrumentation Engineers (SPIE) Conference Series\nVol. 5488, UV and Gamma-Ray Space Telescope Systems. pp 13–24,\n[doi:10.1117/12.561488](http://dx.doi.org/10.1117/12.561488)\n\nSmalley B., 2004, in Zverko J., Ziznovsky J., Adelman S. J., Weiss\nW. W., eds, IAU Symposium Vol. 224, The A-Star Puzzle. pp 131–138\n( `arXiv:astro-ph/0408222` [), doi:10.1017/S1743921304004478](http://dx.doi.org/10.1017/S1743921304004478)\n[Sokolov N. A., 2000, A&A, 353, 707](https://ui.adsabs.harvard.edu/abs/2000A&A...353..707S)\nSokolov N. A., 2011, in Magnetic Stars. pp 390–398 ( `arXiv:1104.1547` ),\n\n[doi:10.48550/arXiv.1104.1547](http://dx.doi.org/10.48550/arXiv.1104.1547)\n\n[Sokolov N. A., 2012, MNRAS, 426, 2819](http://dx.doi.org/10.1111/j.1365-2966.2011.19926.x)\n\n[Stassun K. G., et al., 2019, AJ, 158, 138](http://dx.doi.org/10.3847/1538-3881/ab3467)\n\n[Stibbs D. W. N., 1950, MNRAS, 110, 395](http://dx.doi.org/10.1093/mnras/110.4.395)\nThompson S. E., Fraquelli D., Van Cleve J. E., Caldwell D. A., 2016, Kepler\nArchive Manual, Kepler Science Document KDMC-10008-006, id. 9.\nEdited by Faith Abney, Dwight Sanderfer, Michael R. Haas, and Steve B.\n\nHowell\n\nTody D., 1986, in Crawford D. L., ed., Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series Vol. 627, Instrumentation in\n[astronomy VI. p. 733, doi:10.1117/12.968154](http://dx.doi.org/10.1117/12.968154)\nTody D., 1993, in Hanisch R. J., Brissenden R. J. V., Barnes J., eds, Astronomical Society of the Pacific Conference Series Vol. 52, Astronomical\nData Analysis Software and Systems II. p. 173\n[Wenger M., et al., 2000, A&AS, 143, 9](http://dx.doi.org/10.1051/aas:2000332)\n[Wolff S. C., Wolff R. J., 1971, AJ, 76, 422](http://dx.doi.org/10.1086/111138)\n[Zhang X., Green G. M., Rix H.-W., 2023, MNRAS, 524, 1855](http://dx.doi.org/10.1093/mnras/stad1941)\nZhao G., Zhao Y.-H., Chu Y.-Q., Jing Y.-P., Deng L.-C., 2012, Research in\nAstronomy and Astrophysics, 12, 723\n\n**APPENDIX A: THE PHASE DIAGRAMS**\n\nIn this Section, we present the phase diagrams of all the 28 stars of\nour sample.\n\nThis paper has been typeset from a TEX/L [A] TEX file prepared by the author.\n\nMNRAS **000**, 1–14 (2024)\n\n\n-----",
  "**Figure A1.** Phase diagrams for the stars of our sample. Orange dots are the *𝜎* -clipped *Kepler* data, while the smoothed curve is shown in red. The blue curve is\nthe best fit of the *GALEX* points (large blue dots), obtained from the genetic algorithm, as explained in Sect. 4.1. On top of each panel, the KIC ID and some of\nthe results of Table 2 are reported.\n\nMNRAS **000**, 1–14 (2024)\n\n\n-----",
  "MNRAS **000**, 1–14 (2024)\n\n\n**Figure A2.** Same as in Fig. A1, but for the remaining stars of the sample.\n\n\n-----",
  "**Brandon Radosevich**\nJohn T. Halloran\nLeidos\nhalloranjt@leidos.com",
  "To reduce development overhead and enable seamless integration between\npotential components comprising any given generative AI application,\nthe Model Context Protocol (MCP) (Anthropic, 2025d) has recently been\nreleased and, subsequently, widely adapted. The MCP is an open protocol which standardizes API calls to large language models (LLMs), data\nsources, and agentic tools. Thus, by connecting multiple MCP servers–each\ndefined with a set of tools, resources, and prompts–users are able to define\nautomated workflows fully driven by LLMs. However, we show that the\ncurrent MCP design carries a wide range of security risks for end-users.\nIn particular, we show that industry-leading LLMs may be coerced to use\nMCP tools and compromise an AI developer’s system through a wide\nrange of attacks, e.g., malicious code execution, remote access control, and\ncredential theft. In order to proactively mitigate the demonstrated (and\nrelated) attacks, we introduce a safety auditing tool, **McpSafetyScanner**, **the**\n**first** such **agentic tool to assess the security of an arbitrary MCP server** .\nMcpSafetyScanner uses several agents to: a) automatically determine adversarial samples given an MCP server’s tools and resources, (b) search\nfor related vulnerabilities and remediations given such samples, and (c)\ngenerate a security report detailing all findings. Our work thus sheds light\non serious security issues with general purpose agentic workflows, while\nalso providing a proactive tool to audit the safety of MCP servers and\naddress detected vulnerabilities prior to deployment.",
  "With the rise of large language models (LLMs) and agentic workflows, AI is being developed\nand adapted at unprecedented rates. Anticipating such growth, as well as the ensuing\ncomplexity resulting from AI-powered assistants and services communicating with one\nanother, Anthropic has recently introduced the Model Context Protocol (MCP) (Anthropic,\n2025d). This protocol seeks to integrate LLMs and agents with various external systems and\nservices in an easily adaptable framework.\n\nSince it’s debut, the MCP has been widely adapted across a large number of commonly\nused open-source libraries, e.g., default MCP servers are natively packaged with Claude\nDesktop Anthropic (2025a), and official integrations include OpenAI’s Agents (OpenAI),\nCopilot (Microsoft, 2025), Stripe (Stripe, 2025), Slack (Anthropic, 2025g), and IBM’s\nWatson (IBM, 2025), to name a few. Furthermore, the MCP has garnered significant community interest and support; in just four months (at the time of this writing), the official MCP\ngithub repository has accrued over 27k stars (Anthropic, 2025e) and has been forked over\n2.8k times. Indeed, as the use of AI agents and AI-powered applications continues to grow,\nit is expected that the MCP will similarly continue to grow as a unifying framework for the\ngrowing AI-based ecoysytem (Forbes, 2025). However, we show that the current design of\nthe MCP poses significant security risks for users developing generative AI solutions.\n\n1\n\n\n-----\n\nHerein, we demonstrate that industry-leading LLMs may be coerced to use tools from\nstandard MCP servers and directly compromise user systems. In particular, we show that\nClaude 3.7 and Llama-3.3-70B may be prompted to use tools from default MCP servers\nwhich allow three different types of attacks: 1) *malicious code execution*, (2) *remote access*\n*control*, and (3) *credential theft* . Furthermore, we introduce a new multi-MCP server attack\nwhich enables both remote access control and credential theft. Through these attacks, bad\nactors are able to gain access to an AI developer’s system and/or procure sensitive user\ndata (e.g., API keys).\n\nWith the successful demonstration of all three attacks, we then show that **Claude** **is aware of**\nthe **underlying security issues** concerning prompts which enable these attacks **and refuses**\n**such requests some of the time, but may be coerced to successfully carry out requests**\n**by simple prompt changes.** Directly testing the ability of **an LLM’s guardrails** to prevent\nthese attacks **can thus produce false positives which**, in turn, **may provide a false sense of**\n**security against such attacks.** Thus, we advocate that an LLM’s guardrails [1] should not be\nsolely relied upon for remediation. Rather, remediation should occur through the LLM (via\nits guardrails) as well as proactively through the design of the MCP server (via knowledge\nof the exploits made possible when an LLM is enabled with an MCP server’s tools and\nresources).\n\nTo proactively identify exploits for agentic MCP workflows, we introduce McpSafetyScanner,\nthe first tool to assess the security of an arbitrary MCP server. Given a particular MCP\nserver, McpSafetyScanner uses agents to automatically detect system vulnerabilities using\nthe server’s features (i.e., tools, prompts, and resources), automatically searches knowledge\nbases for related vulnerabilities, determines remediations for all vulnerabilities, and produces a detailed security report for MCP developers. **McpSafetyScanner** **thus allows MCP**\n**developers the ability to easily scan their MCP servers for vulnerabilities and release**\n**patches for exploits using returned remediations.**\n\nWe show that, for the standard MCP servers which enable our demonstrated attacks,\nMcpSafetyScanner is able to correctly identify these vulnerabilities, provide standard examples of these attacks, and remediations (as well as guardrail best-practices).",
  "**2.1** **Need for Standardized Generative AI APIs**\n\nCurrently, the generative AI landscape consists of a wide range of custom APIs tailored\ntowards specific goals and targeted solutions. E.g., for retrieval augmented generation\n(RAG) alone, widely used solutions include Chroma, LangChain, Haystack, LlamaIndex,\nChatGPT’s retrieval plugin, Huggingface’s retrieval plugin, and Azure’s Machine Learning\npipeline, to name name a few. Furthermore, the aforementioned RAG solutions may\ninternally call several other generative AI APIs, differing based on the inference/LLMOps\nprovider (e.g., OpenAI, Azure OpenAI, Together AI, and DataBricks for API endpoints\nor local models through Huggingface, Ollama, vLLM, etc.). Such recursive API calls are\ninherited by practitioners when developing their own specific applications, who in turn\nbuild their own custom APIs. Thus, while a large number of generative AI solutions exist,\nadapting such solutions for a particular use case requires significant developer time and\neffort due to the current ad-hoc state of generative AI APIs.\n\n**2.2** **The Model Context Protocol**\n\nThe MCP is a streamlined solution to the current unstructured design of generative AI\nAPIs. At its core, the MCP defines a schema with which client- and server-side services\n\n1 In this context, we are not only referring to the guardrails enabled through safety fine-tuning\nthe LLM, i.e., *alignment* (Grattafiori et al., 2024). Rather, when discussing a closed-source LLM\naccessed via a server endpoint (e.g., Claude and GPT-4o ), we use the term *guardrails* to encompass\nall facets of the LLM-inference-endpoint’s refusal system, which may include malicious prompt\ndetectors (Inan et al., 2023) for either the input query or the LLM’s response.\n\n2\n\n\n-----\n\nmust structure their requests. Additionally, the MCP consists of an open-source SDK to\nenable quick adaptation across popular web-development languages (i.e., Python, Java,\nTypescript, and Kotlin ). Within the MCP client-serve message schema, requested services\nare described as *features* . Server-to-client (STC) features include resources (i.e., context and\ndata for the AI model to use), prompts (i.e., templated messages and workflows for users),\nand tools (i.e., functions for the AI model to execute). The client-to-service (CTS) may offer\nthe sampling feature, i.e., server-initiated agentic behaviors and recursive LLM interactions.\nBy standardizing messages, categorizing STC/CTS requests, and providing a centralized\ncodebase, developers may quickly integrate open source tooling and projects into their own\nGenerative AI solutions.",
  "As previously noted, we consider the following three types of attacks\n\n1. *Malicious code execution* (MCE) - an attacker inserts malicious code into a user’s\n\nsystem files.\n2. *Remote access control* (RAC) - an attacker is immediately granted remote access to\n\nthe victim’s system.\n3. *Credential Theft* (CT) - an attacker exploits access to system files or environment\n\nvariables, covertly extracting sensitive information from the victim’s system.\n\nHerein, we demonstrate MCE attacks which allow RAC every time the victim opens a new\nterminal. We note, however, that general MCE attacks are not limited to this specific end\ngoal.\n\n**Claude** **.** Figure 3 shows successful and unsuccessful MCE attempts by directly prompting\nClaude enabled with the MCP filesystem server (described in Table B). Claude demonstrates\nknowledge of the security risks related to an MCE; in Figure 1(a), an MCE attack hidden\nin octal-encoded variables triggers Claude ’s guardrails, and Claude does not complete the\nrequest while noting the security concerns associated with it. While examples like this\nmay provide confidence in Claude ’s safety guardrails, Figure 1(b) shows a less subversive\nversion of the attack, where the MCE is written in plaintext. In this case, we can see that\nwhile Claude ’s guardrails are partially triggered (noting potential security risks with this\nrequest), Claude completes the request. The backdoor is triggered the next time the MCP\nuser opens a new terminal (Figure B demonstrates the attack in full).\n\nWhile this specific demonstration has a low threat level in a general setting–as the attacker\nrequires direct access to the MCP user’s system to directly prompt Claude Desktop –the threat\nlevel increases drastically when considering shared-office or communal settings (Willison\n& Warkentin, 2013). Furthermore, similar results may be found when directly prompting\nfor an RAC attack; Figure B displays both refused and successful RAC attacks. While the\nformer (in Figure 8(a) triggers Claude ’s guardrails, the latter (in Figure 8(b)) is completed\nwithout any mention of security. Due to the extreme potential for malicious actions enabled\nby MCE and RAC attacks–which are thus enabled through MCP tools–we note the need\nfor reliable guardrails for potentially malicious attacks. However, Figure B shows another\nexample where Claude refuses an MCE attack after its guardrails are triggered (and despite\nthe user escalating the severity of the request), whereas Figure 9 shows an intimidating\nRAC attack request which partially triggers Claude’s guardrails but is completed.\n\n**Llama-3.3-70B-Instruct** **.** Figure 2 shows an MCE attack attempt successfully carried out\nby Llama-3.3-70B-Instruct . While Llama-3.3-70B-Instruct displays knowledge of potential malicious use cases related to the request–suggesting guardrails were partially\ntriggered–the request is nonetheless completed. This is also true for alternative MCE attempts (Figure 16(b)). However, both RAC (Figure C) and multiple CT (Figure C) requests\nare completed without any returned security concerns, suggesting these requests did not\npartially trigger Llama-3.3-70B-Instruct’s guardrails (unlike Claude).\n\nUnlike Claude, Llama-3.3-70B-Instruct refusals for MCE, RAC, and CT attack-requests\nrequired explicit harmful/unsafe language, as displayed in Figure D (for RAC attacks) and\n\n3\n\n\n-----\n\nHowever, Claude executes the less deceptive request, where the command to establish a\nremote execution attack is passed in plaintext and added to the user’s run configuration\nfile.\n\n4\n\n\n-----\n\nFigure 2: **Llama-3.3-70B-Instruct** **completes an MCE attack request.**\nLlama-3.3-70B-Instruct shows its guardrails are being partially triggered by noting malicious use cases for this command, but nonetheless completes the request. The\nrequest is highlighted in purple, while the salient portions of Llama-3.3-70B-Instruct ’s\nresponse are highlighted in orange. The original unhighlighted image, as well as another\ncompleted MCE attack request, may be found in Figure C.\n\nFigure D (for MCE and CT attacks). The only difference between the refused and completed prompts is a phrase in the latter explicitly stating one of the following words/phrases:\n“hack,” “steal,” “backdoor,” and “break into.” We note that, in practice, attackers are unlikely\n\nto overtly state their intentions, but rather much more likely to manipulate their targets with\nwhatever language is required to achieve their goal (Valeriano et al., 2018; Workman, 2008).\nFurthermore, we note that Llama-3.3-70B-Instruct underwent extensive safety alignment\nand cybersecurity evaluation (Grattafiori et al., 2024). Thus, despite rigorous testing on\nprevious (non-MCP related) safety benchmarks, Llama-3.3-70B-Instruct (and likely other\nLLMs) require re-evaluation given the immediate safety-and-security implications of enabling LLMs with MCP tools. In particular, if an MCP-enabled application was solely using\nLlama-3.3-70B-Instruct and was equipped with the MCP filesystem server, the system\nmight allow MCE, RAC, and CT attacks so long as a bad actor does not use harmful or\nunsafe language.",
  "We have shown that both Claude and Llama-3.3-70B-Instruct enabled with MCP servers\nare susceptible to MCE, RAC, and CT attacks when directly prompted. We introduce a new\nattack for MCP-enabled agentic workflows, wherein the LLM is not directly prompted with\nthe MCP-leveraging attack. Rather, the attacker corrupts publicly available data, which end\nup on the MCP-enabled user’s system and which the user adds to a vector database. The\ndata has been corrupted with MCP-leveraging attack commands centered around a specific\ntheme so that, when the MCP user asks to query this database for info related to this theme,\nthe attacker’s commands are loaded and run. We thus call this a *R* etrieval- *A* gent *DE* ception\n(RADE) attack, which is illustrated in Figure 3. As the attacker no longer needs direct access\nto the victim’s system, RADE has a significantly higher threat level than the direct prompt\nattacks (DPA) demonstrated in Section 3.\n\nGiven a system enabled with the MCP servers listed in Table 2, we demonstrate an endto-end RADE attack for CT using Claude Desktop . In it, an attacker has corrupted a\nfile–centered around the theme “MCP” with commands to search for any environment\nvariables containing “OpenAI” or “HuggingFace” and export these over Slack–which ends\nup on the MCP-user’s system. The user forms a vector database out of their files using the\nChroma MCP server and tells Claude to query the database for “MCP” and run the results.\nClaude subsequently uses the Chroma MCP server to run the query, then the everything\n\n5\n\n\n-----",
  "Figure 3: **Threat model for a RADE attack.** An attacker compromises publicly available\ndata with targeted commands centered around a specific theme (“X” in the figure), which\nends up on an MCP user’s system. Compromised data is then automatically added by\na retrieval agent to a vector database so that, when a user requests for content related to\nthese themes, the malicious commands are retrieved and potentially executed automatically.\n\nMCP server to search for “OpenAI” and “HuggingFace” environment variables, finds API\nkeys for both, and finally posts a company-wide Slack notification exposing both the victim’s\nOpenAI and HuggingFace API keys. The successful attack is displayed in Figure 4.\n\nA second successful RADE attack–for RAC using Claude Desktop –is displayed in Figure B\nand B. Similar to the previous RADE attack, an attacker has compromised a file centered\naround the theme “MCP,” but this time stating commands to add an ssh key to the victim’s\nlocal authorized keys file. The attack proceeds as before, where after the user tells Claude\nto query the database for “MCP” and run the results, Claude uses the Chroma MCP server\nto run the query and the filesystem MCP server to create the authorized keys file with the\nattacker’s ssh keys, thus granting immediate access to the victim’s system.",
  "We have thus seen several malicious system attacks made possible by querying Claude or\nLlama-3.3-70B-Instruct connected to MCP servers. Furthermore, we have seen that, while\nthe LLM’s guardrails may be triggered by MCE, RAC, or CT attacks, refusal of the related\nrequests are not guaranteed (especially for Llama-3.3-70B-Instruct ). Thus, to add security\nbeyond just an LLM’s guardrails to MCP-enabled systems, we introduce McpSafetyScanner .\n\nGiven an arbitrary MCP server, McpSafetyScanner uses agents to automatically probe the\nsystem environment and actions enabled by the server for vulnerabilities and subsequent\nremediations. Depicted in Figure 5, this entire process is carried out in three key stages.\nThe first stage consists of **automated vulnerability detection**, wherein a *hacker* agent automatically pulls down an MCP server’s features (i.e., tools, resources, and prompts), then\ndetermines system vulnerabilities using these features. The second stage consists of an\n**expanded vulnerability search and remediation**, wherein, for each (tool, resource, prompt,\nvulnerability) tuple, a *security auditor* agent searches several knowledge bases (i.e., the\nWorld Wide Web, arXiv, and Hacker News) for similar vulnerabilities. For each determined\nvulnerability, the auditor thus determines remediation steps and best practices for an MCP\ndeveloper to mitigate these exploits. The final stage consists of the **security report genera-**\n**tion**, wherein a *supervisor* agent consolidates all vulnerabilities and remediations to produce\na detailed report.\n\nThus, an MCP developer or user inputs the configuration file defining their MCP\nservers and args in json format (e.g., claude desktop config.json for Claude Desktop ),\nand McpSafetyScanner returns a report of its security findings. We present two\nMcpSafetyScanner reports produced by scanning the configuration of MCP servers considered herein (Table 2). McpSafetyScanner is fast (runtime of less than one minute to scan\n\n6\n\n\n-----\n\n(a) RADE attack file for CT centered around the theme “MCP.”\n\nFigure 4: **Successful RADE attack for CT:** From a vector database including CT directions themed around “MCP,” Claude is instructed to search for entries about the MCP and\nperform related actions. Claude complies, completing a RAC attack and providing attackers access to the victim’s system. Conversation is condensed for brevity, full conversation\nis displayed in Figure B and B.\n\n\n|Role = “Hacker” Role = “Auditor” Role = “Supervisor” Tools = { MCP Server tools } Tools = { Web, ArXiv, H a c k e r } Tools = { Hacker, Auditor} News Goals = {“P toro ob lse a s ny ds t se em rv u ers ing Goals ={ “ rfo er s oe ua rc ch e (,t vo uo ll n, ep rr ao bm ilp itt y, ) : Goals = {“M keo en pi t to hr e o mth oe nr a tag se kn ”t,s, resources”, -Find related vulnerabilities”, “Produce a detailed “Find/determine “Return discovered security report”, remediations for all vulnerabilities”, } learned vulnerabilities”, } }|Col2|Col3|\n|---|---|---|\n||News {“for each (tool, prompt, Goals = resource, vulnerability): -Find related vulnerabilities”, “Find/determine remediations for all learned vulnerabilities”, }|{“Monitor other agents, Goals = keep them on task”, “Produce a detailed security report”, }|\n\n\nStage 1 – MCP server vulnerability\n\ndetection\n\n\nStage 2 – Detect related vulnerabilities,\n\ndetermine remediations\n\n\n\n\n\n\n\nStage 3 – Generate a detailed security\nreport of vulnerabilities and remediations\n\n\nFigure 5: **Steps and agents used by the** **McpSafetyScanner** to detect MCP server vulnerabilities and determine remediations.\n\n7\n\n\n-----\n\nand generate each report on an M2 Max MacBook Pro) and, most importantly, accurate; **the**\n**first report** (displayed in Figure E) **catches exploits used in the demonstrated MCE, RAC,**\n**and CT attacks.** Furthermore, the remediation steps enable the developer to strengthen the\nguardrails of the underlying MCP server, as well as the end-user to strengthen the defenses\nof the system (while hosting the MCP server).\n\nFor example, for RAC, McpSafetyScanner correctly notes the abuse possibility of ssh keys\nbeing added to a user’s authorized keys file (while also noting other possible system paths\nfor this attack). The provided remediation–“implement strict file access permissions”–\nprovides a path for a downstream MCP user to protect their system from this exploit, while\nthe second remediation–“Monitor file access and modifications”–provides a means with\nwhich MCP developers can place guardrails on their deployed server (i.e., monitor the files\naccessed during LLM-MCP server exchanges and prevent access to sensitive system files).\nFurthermore, the report includes commandline examples of each attack. A summary of the\nreport is available in Table 5, where the exploits used to achieve the attacks demonstrated\nherein are described and accompanied by remediations.\n\n\n\n\n\n\n|Table 1: Summar Table 2 Attack|ry of McpSafetyScanner findings fro Description|om jointly scanning the MCP servers in Remediation|\n|---|---|---|\n|Attack|Description|Remediation|\n|MCE|An attacker could use the edit file and write file func- tions to inject malicious code or backdoors into critical files, lead- ing to unauthorized access or priv- ilege escalation.|Implement strict access controls and monitoring for file modifications. Re- strict directories where these functions can operate. Monitor changes to critical files using file integrity tools (Linux Tripwire)|\n|RAC|Attackers can add their own public SSH keys to →/.ssh/authorized keys, gain- ing unauthorized access.|Use strict permissions on authorized keys|\n|CT|Attackers can print and capture environment variables to access sensitive data, such as API keys, internal URLs, and credentials.|Avoid storing sensitive information in environment variables. Enforce least privilege principles.|\n|CT|Exploit the Slack API to exfiltrate data or cause unauthorized posts.|Audit and restrict API access, and reg- ularly review channel permissions. Use Slack’s advanced security features (Slack Security Best Practices).|",
  "Although only a handful of months old, the MCP shows significant promise in lowering the\nconnectivity barriers between agentic components. Furthermore, the rapid adaptation over a\nshort period of time point to the protocol’s potential to truly become the “USB-C port for AI\napplications” (Anthropic, 2025f). However, with rapid adaptation also comes the increased\npotential for the abuse of existing safety vulnerabilities. In order to initiate the understanding\nof such MCP vulnerabilities, herein, we’ve studied three serious attacks, ranging in their\nimpact from exfiltration of sensitive information to remote access control of the server’s\nhost. We’ve shown that both Claude and Llama-3.3-70B-Instruct are susceptible to all\nthree attacks. We’ve also introduced a new multi-MCP server attack with a high threat\nlevel, RADE, and shown that Claude may enable CT or RAC under this attack. Furthermore,\nwe’ve shown that the guardrails of both models may be triggered during these attacks, but\nthe reliability of these guardrails to prevent these attacks (via a refusal on the part of the\nLLM) varies drastically based on the model as well as the prompt used to deliver the attack.\n\nTo aid in strengthening the guardrails of the MCP server and the hosting system (thus\nrelieving the LLM of the sole burden of refusal), we’ve introduced an agentic tool,\nMcpSafetyScanner, to automatically scan MCP servers for vulnerabilities and provide remediations. We’ve shown that McpSafetyScanner is capable of catching the exploits which\n\n8\n\n\n-----\n\nhave enabled the attacks considered herein, and provides quick actionable remediations to\nclose MCP-enabled exploits on either the developer’s or MCP-user’s side. We are actively\nworking to release this tool so that MCP server vulnerabilities may be scanned and patched\nprior to deployment, thus decreasing zero-day exploits and any abuse unintentionally\nenabled by MCP servers.\n\nThere is significant room for future work. We plan to continue auditing existing MCP\nservers in order to patch existing vulnerabilities. Furthermore, we plan to work towards\npartnering closely with the active MCP community, in order to automate safety scanning\nprior to deployment using McpSafetyScanner.",
  "Claude Desktop was run using Claude for Mac v0.8.1 on macOS Sequoia v15.3.2 .\nMcpSafetyScanner was written in Agno ( v1.2.6 ), with each agent powered by\ngpt-4o-2024-08-0 for all results presented. All Llama-3.3-70B-Instruct results were run\nusing mcp v1.1.2, huggingface-hub v0.29.3, and 1.68.2, where inference calls were made\nusing HuggingFace’s inference API. GNU netcat v0.7.1 was used for all related results. All\nconsidered MCP servers are listed in Table 2 with their associated tools listed in Table 3. The\nClaude Desktop config file of all MCP servers used for all attacks is available in Section A.\n\nTable 2: MCP servers considered herein. Filesystem, Slack, and everything servers are\nnatively packged with current versions of Claude Desktop . All considered servers are\n\n|atively packged with current versi osted in the official MCP github rep MCP Server|ions of. All considered servers are pository (Anthropic, 2025e). Description|\n|---|---|\n|MCP Server|Description|\n|Filesystem (Anthropic, 2025c)|MCP for filesystem operations (.e.g, read, write, make directory, etc.)|\n|Slack (Anthropic, 2025g)|MCP for the Slack API, enabling Claude to interact with Slack workspaces|\n|Everything (Anthropic, 2025b)|Test server for builders of Client servers|\n|Chroma (Chroma, 2025)|Server enabling data retrieval ca- pabilities powered by Chroma|",
  "We thank Leidos for funding this research through the Office of Technology. Approved for\npublic release **25-LEIDOS-0318-29149** .",
  "Anthropic. *Installing Claude for Desktop* . ” https://support.anthropic.com/en/articles/\n\n10065433-installing-claude-for-desktop”, 2025a. ”Accessed: 2025-03-20”.\n\n”\nAnthropic. *Everything MCP Server - a test server for builders of MCP clients* . https:\n//github.com/modelcontextprotocol/servers/tree/main/src/everything ”, 2025b. ”Accessed: 2025-03-26”.\n\nAnthropic. *Filesystem MCP Server - Node.js server implementing Model Context Protocol*\n*(MCP) for filesystem operations.* ” https://github.com/modelcontextprotocol/servers/\n\ntree/main/src/filesystem”, 2025c. ”Accessed: 2025-03-13”.\n\n”\nAnthropic. *Introducing the Model Context Protocol* . https://www.anthropic.com/news/\nmodel-context-protocol”, 2025d. ”Accessed: 2025-02-12”.\n\nAnthropic. *Model Context Protocol: A protocol for seamless integration between LLM applications*\n\n*and external data sources* . ” https://github.com/modelcontextprotocol/servers ”, 2025e.\n”Accessed: 2025-03-22”.\n\n9\n\n\n-----\n\nAnthropic. *Agents and tools - Model Context Protocol (MCP)* . ” https://docs.anthropic.com/\n\nen/docs/agents-and-tools/mcp”, 2025f. ”Accessed: 2025-03-28”.\n\nAnthropic. *Slack MCP Server* . ” https://github.com/modelcontextprotocol/servers/tree/\n\nmain/src/slack”, 2025g. ”Accessed: 2025-03-20”.\n\nChroma. *Chroma MCP Server - server providing data retrieval capabilities powered by Chroma* .\n\n”https://github.com/chroma-core/chroma-mcp”, 2025. ”Accessed: 2025-03-26”.\n\nForbes. *Why Anthropic’s Model Context Protocol Is A Big Step In The Evolu-*\n*tion* *Of* *AI* *Agents* . ” https://www.forbes.com/sites/janakirammsv/2024/11/30/\nwhy-anthropics-model-context-protocol-is-a-big-step-in-the-evolution-of-ai-agents/ ”,\n2025. ”Accessed: 2025-02-12”.\n\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,\n\nAhmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The\nllama 3 herd of models. *arXiv preprint arXiv:2407.21783*, 2024.\n\nIBM. *Using watsonx.ai Flows Engine with Model Context Protocol (MCP)* . ” https://github.\n\ncom/IBM/wxflows/tree/main/examples/mcp/javascript”, 2025. ”Accessed: 2025-03-20”.\n\nHakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao,\n\nMichael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et al. Llama guard: Llmbased input-output safeguard for human-ai conversations. *arXiv preprint arXiv:2312.06674*,\n2023.\n\nMicrosoft. *Introducing Model Context Protocol (MCP) in Copilot Studio* . ” https://tinyurl.\n\ncom/CopilotMCP”, 2025. ”Accessed: 2025-03-20”.\n\n”\nOpenAI. *OpenAI Agents SDK - Model context protocol* . https://openai.github.io/\nopenai-agents-python/mcp/\", note = ”Accessed: 2025-03-26”, year = 2025.\n\nStripe. *Stripe Agent Toolkit* . ” https://github.com/stripe/agent-toolkit ”, 2025. ”Accessed:\n\n2025-03-20”.\n\nBrandon Valeriano, Benjamin M Jensen, and Ryan C Maness. *Cyber strategy: The evolving*\n\n*character of power and coercion* . Oxford University Press, 2018.\n\nRobert Willison and Merrill Warkentin. Beyond deterrence: An expanded view of employee\n\ncomputer abuse. *MIS quarterly*, pp. 1–20, 2013.\n\nMichael Workman. Wisecrackers: A theory-grounded investigation of phishing and pretext\n\nsocial engineering threats to information security. *Journal of the American society for*\n*information science and technology*, 59(4):662–674, 2008.\n\n10\n\n\n-----\n\nTable 3: All MCP servers considered, and their associated tools\n\n\n|MCP Server|Tools|\n|---|---|\n|Filesystem (Anthropic, 2025c)|read file read multiple files write file edit file create directory list directory move file search files get file info list allowed directories|\n|Slack (Anthropic, 2025g)|slack list channels slack post message slack reply to thread slack add reaction slack get channel history slack get thread replies slack get users slack get user profile|\n|Everything (Anthropic, 2025b)|echo add longRunningOperation sampleLLM getTinyImage printEnv annotatedMessage|\n|Chroma(Chroma, 2025)|create collection peek collection list collections get collection info get collection count modify collection delete collection add documents query documents get documents|\n\n\n11\n\n\n\n-----",
  "{\n\n\"mcpServers\": {\n\n\"chroma\": {\n\n\"command\": \"uvx\",\n\"args\": [\n\n\"chroma-mcp\",\n\"--client-type\",\n\"persistent\",\n\"--data-dir\",\n\"/Users/yourusername/work/mcp/files\"\n]\n},\n\"filesystem\": {\n\n\"command\": \"npx\",\n\"args\": [\n\n\"-y\",\n\"@modelcontextprotocol/server-filesystem\",\n\"/Users/yourusername/\"\n]\n},\n\"everything\": {\n\n\"command\": \"npx\",\n\"args\": [\n\n\"-y\",\n\"@modelcontextprotocol/server-everything\"\n],\n\"env\": {\n\n\"AWS_ACCESS_KEY_ID\": \"YOUR_ACCESS_KEY_HERE\",\n\"AWS_SECRET_ACCESS_KEY\": \"YOUR_SECRET_ACCESS_KEY_HERE\",\n\"AWS_REGION\": \"YOUR_AWS_REGION_HERE\",\n\"OPENAI_API_KEY\": \"sk-randomstring\",\n\"HF_TOKEN\": \"hf_randomstring\"\n}\n},\n\"slack\": {\n\n\"command\": \"npx\",\n\"args\": [\n\n\"-y\",\n\"@modelcontextprotocol/server-slack\"\n],\n\"env\": {\n\n\"SLACK_BOT_TOKEN\": \"YOUR_BOT_TOKEN\",\n\"SLACK_TEAM_ID\": \"YOUR_TEAM_ID\"\n}\n}\n\n}\n}\n\n12\n\n\n-----",
  "(a) Claude delivers an MCE attack, enabling RAC on\nthe victim’s system.\n\n\n(b) Attacker listens on the specified port.\n\n\n(c) Victim runs the command when starting a new terminal, granting system access to the attacker.\n\nFigure 6: **End-to-end MCE attack.** In Figure 6(a), Claude adds a netcat command to the\nvictim’s bash run configuration file, setting a listener on port 4444 which runs every\ntime the victim opens a new terminal. In Figure 6(b), the attacker listens on port 4444 .\nWhen there is a connection, i.e., the victim opens a new terminal (Figure 6(c)), the attacker\nsuccessfully gains system access.\n\n13\n\n\n-----\n\nFigure 7: **A request involving commands for a MCE attack triggers** **Claude** **’s guardrails.**\nClaude initially notes the security implications of commands executing a MCE attack,\nand requests clarification before proceeding. Despite the user conveying several high\nstakes with the request (i.e., retaliation from their boss and extreme monetary losses if the\nrequest isn’t executed), Claude refuses.\n\n14\n\n\n-----\n\nFigure 8: **Refused and completed RAC attacks.** Claude ’s guardrails are triggered by a\nRAC attack (Figure 8(a)), where Claude notes the security risks of this request. Claude\ncarries out the steps for a RAC attack (Figure 8(b).\n\n15\n\n\n-----\n\nFigure 9: **Completed RAC attack with partially triggered guardrails.** Claude ’s guardrails\nare partially triggered by a RAC attack, where Claude completes the steps for a RAC\nattack, then notes the security risks of this request.\n\n16\n\n\n-----\n\nFigure 10: **RADE attack for RAC (part i):** A file containing malicious MCP commands\nexists on an MCP user’s system. A retrieval agent ( Claude using the Chroma MCP server)\nis instructed to create a vector database including this file.\n\n17\n\n\n-----\n\nFigure 11: **RADE attack for RAC (part ii):** From the vector database containing malicious\nMCP commands, Claude is instructed to search for entries about the MCP and perform\nrelated actions. Claude complies, completing a RAC attack and providing attackers access\nto the victim’s system.\n\n18\n\n\n-----\n\n(a) Publicly available file with seemingly helpful MCP content. In actuality, the\ncontent exports the user’s OpenAI and Huggingface API keys to external slack\nchannels\n\n\n-----\n\nFigure 13: **RADE attack for CT (part ii):** A file containing malicious MCP commands\nexists on an MCP user’s system. A retrieval agent ( Claude using the Chroma MCP server)\nis instructed to create a vector database including this file.\n\n20\n\n\n-----\n\n(a) Claude is successfully coerced to perform a RADE attack using available MCP servers, exporting\nthe user’s OpenAI and Huggingface to Slack. **RadBlog** is a Slack app which notifies all Slack users\nin the organization after posting.\n\nFigure 14: **RADE attack for CT (part iii):** A file containing malicious MCP commands\nexists on an MCP user’s system. A retrieval agent ( Claude using the Chroma MCP server)\nis instructed to create a vector database including this file.",
  "Figure 15: **Llama-3.3-70B-Instruct** **completes RAC attack:** Llama-3.3-70B-Instruct\nfulfills RAC attack request\n\n21\n\n\n-----\n\n(a) Llama-3.3-70B-Instruct completes MCE\n\n(b) Llama-3.3-70B-Instruct completes MCE\n\nFigure 16: **Llama-3.3-70B-Instruct** **completes MCE attacks:** Llama-3.3-70B-Instruct\nfulfills MCE attack requests.\n\n22\n\n\n-----\n\n(a) Llama-3.3-70B-Instruct completes CT\n\n(b) Llama-3.3-70B-Instruct completes CT\n\nFigure 17: **Llama-3.3-70B-Instruct** **completes CT attacks:** Llama-3.3-70B-Instruct fulfills CT attack requests.\n\n23\n\n\n-----",
  "(a) Llama-3.3-70B-Instruct refuses an MCE attack with the word “hack.”\n\n(b) Llama-3.3-70B-Instruct refuses an MCE attack with the word “hack.”\n\n(c) Llama-3.3-70B-Instruct refuses a CT attack with the word “steal.”\n\nFigure 18: **Explicit keywords trigger** **Llama-3.3-70B-Instruct** **MCE and CT attack re-**\n**fusals:** MCE and CT attack requests with explicit harmful/unsafe words “hack” and\n“steal” trigger Llama-3.3-70B-Instruct refusals.\n\n24\n\n\n-----\n\n(a) Llama-3.3-70B-Instruct refuses an RAC attack with the word “backdoor.”\n\n(b) Llama-3.3-70B-Instruct refuses an RAC attack with the phrase “break into.”\n\nFigure 19: **Explicit keywords trigger** **Llama-3.3-70B-Instruct** **RAC attack refusals:** RAC\nattack requests with explicit harmful/unsafe words/phrases “backdoor” and “break into”\ntrigger Llama-3.3-70B-Instruct refusals.",
  "25\n\n\n-----\n\nFigure 20: **McpSafetyScanner** **report:** Result of McpSafetyScanner scanning the MCP\nservers listed in Table 2 with tools listed in Table 3\n\n26\n\n\n-----\n\nFigure 21: **McpSafetyScanner** **second report:** Result of McpSafetyScanner scanning the\nMCP servers listed in Table 2 with tools listed in Table 3. Due to the stochasticity of the\nagents involved, more scans may catch more vulnerabilities (and remediations).\n\n27\n\n\n-----",
  "**Abstract**\n\nThis paper proposes a highly robust autonomous agent framework based on the ReAct paradigm, designed to solve\ncomplex tasks through adaptive decision making and multi-agent collaboration. Unlike traditional frameworks that rely\non fixed workflows generated by LLM-based planners, this framework dynamically generates next actions during agent\nexecution based on prior trajectories, thereby enhancing its robustness. To address potential termination issues caused\nby adaptive execution paths, I propose a timely abandonment strategy incorporating a probabilistic penalty mechanism.\nFor multi-agent collaboration, I introduce a memory transfer mechanism that enables shared and dynamically updated\nmemory among agents. The framework’s innovative timely abandonment strategy dynamically adjusts the probability\nof task abandonment via probabilistic penalties, allowing developers to balance conservative and exploratory tendencies\nin agent execution strategies by tuning hyperparameters. This significantly improves adaptability and task execution\nefficiency in complex environments. Additionally, agents can be extended through external tool integration, supported\nby modular design and MCP protocol compatibility, which enables flexible action space expansion. Through explicit\ndivision of labor, the multi-agent collaboration mechanism enables agents to focus on specific task components, thereby\nsignificantly improving execution efficiency and quality.\n**Keywords:** ReAct, Robustness, Autonomous Agent, Multi-Agent",
  "In recent years, the rapid advancement of artificial intelligence technology has revolutionized the field of natural language processing (NLP), enabling the development of increasingly sophisticated autonomous agent frameworks. These\nframeworks aim to solve complex tasks by leveraging the capabilities of large language models (LLMs) and multi-agent\ncollaboration mechanisms. Starting with the introduction of the Transformer model[1], which laid the foundation for\nmodern deep learning architectures, the field has witnessed groundbreaking innovations such as the pre-trained BERT\nmodel[2], chain-of-thought (CoT) prompting[3], and the ReAct paradigm[4]. Each of these advancements has contributed\nsignificantly to enhancing the reasoning, adaptability, and efficiency of AI systems in handling complex tasks.\nDespite these achievements, existing autonomous agent frameworks still face several limitations in flexibility, robustness, and multi-agent collaboration[5]. Traditional frameworks, such as AutoGen and LangChain, often rely on predefined\nworkflows generated by LLM-based planners[6]. While these approaches work well in structured environments, they\nstruggle to adapt to uncertainties and dynamic changes in complex and real-world scenarios. This rigidity can lead to suboptimal performance, resource inefficiencies, and even task failures when faced with unexpected challenges. Furthermore,\nmulti-agent collaboration mechanisms in existing frameworks often lack effective mechanisms for information sharing and\ndynamic coordination, limiting their ability to handle complex tasks that require seamless interaction among multiple\nagents.\nTo address these limitations, this paper proposes a highly robust autonomous agent framework based on the ReAct\nparadigm. The ReAct paradigm[4], which integrates reasoning and action in an iterative process, has demonstrated\nsignificant advantages in reducing hallucinations[7] and improving task accuracy by allowing agents to interact with their\nenvironment and refine their reasoning in real time. By dynamically generating next actions during agent execution based\non prior trajectories, my framework enhances flexibility and adaptability, enabling agents to respond effectively to dynamic\ntask requirements and environmental feedback.\nA key innovation of this framework is the introduction of a timely abandonment strategy with a probabilistic penalty\nmechanism. This strategy dynamically adjusts the probability of task abandonment based on task progress and resource\nconsumption, ensuring timely termination of unproductive or overly resource-intensive tasks. By tuning hyperparameters\n\n∗ Repo: `https://github.com/vortezwohl/Autono`\n\n  - Email: vortez.wohl@gmail.com\n\n1\n\n\n-----\n\nsuch as abandonment probability and penalty coefficient, developers can balance conservative and exploratory tendencies\nin agent execution strategies, thereby improving adaptability and task efficiency in complex environments.\nAdditionally, the framework implements a memory transfer mechanism that enables agents to share and dynamically\nupdate information during multi-agent collaboration. This mechanism allows agents to focus on specific task components\nwhile maintaining a shared understanding of the overall task context, significantly improving execution efficiency and\nquality. Furthermore, the framework adopts a modular design approach, enabling users to customize and optimize agent\nbehaviors according to specific requirements. This design enhances the framework’s applicability and practicality while\nreducing development and deployment complexity.\nThe proposed framework also supports the Model Context Protocol (MCP)[8], an open specification designed to standardize how context is passed between LLMs and applications. By providing a structured framework for communication,\nMCP enhances reliability, security, and functionality, enabling seamless integration with various tools and data sources.\nThis compatibility further extends the framework’s versatility and applicability in diverse scenarios.\nIn summary, this paper aims to address the limitations of existing agent frameworks by proposing a novel and highly\nrobust autonomous agent framework. By leveraging the ReAct paradigm, timely abandonment strategy, memory transfer\nmechanism, and MCP compatibility, the framework is designed to enhance adaptability, robustness, and efficiency in\nsolving complex tasks. The contributions of this paper are expected to advance the state of the art in autonomous agent\nframeworks.",
  "In the flourishing field of artificial intelligence, language models have achieved remarkable advances in natural language\nprocessing. Starting with the groundbreaking Transformer model, the field has seen a series of innovations, including\nthe pre-trained BERT model, chain-of-thought (CoT) prompting, the ReAct paradigm, and multi-agent collaboration\nmechanisms.\nChain-of-Thought (CoT) prompting[3] is a technique that enables reasoning capabilities through intermediate steps.\nIt helps large language models (LLMs) improve their performance in tasks requiring logical progression, such as mathematical and logical problems[3]. By explicitly describing intermediate reasoning steps, CoT enhances the transparency\nand precision of AI’s thought process and can be combined with few-shot prompting[9] for better results on complex tasks\nwith limited examples.\nReAct (Reasoning + Acting)[4] is a prompt engineering method that integrates reasoning and action in an iterative\nprocess. It allows AI to interact with an environment, learn from new data, and refine its reasoning in real time. The\nmethod works by generating reasoning about a problem, interacting with an environment (e.g., querying APIs, searching\nthe web), and using the results to inform the next reasoning step, creating a feedback loop. This combination of CoT with\na connection to external information sources significantly reduces hallucinations, making ReAct agents more accurate and\ntrustworthy.\nMulti-Agent Collaboration Mechanisms[5] have also evolved to address complex tasks that require coordination among\nmultiple agents. These mechanisms enable agents to work together, share information, and achieve common goals.\nNotably, the AutoGen framework[10], designed as an advanced multi-agent dialogue system, supports the creation and\ncollaboration of multiple agents. However, its task termination depends primarily on preset maximum steps or human\nfeedback, lacking a flexible, adaptive task termination mechanism.\nSimilarly, the CrewAI framework[11] emphasizes role-playing and collaboration among agents but falls short in dynamic\ntask execution, struggling to respond effectively to sudden changes during task execution. The LangChain framework[12]\noffers fine-grained control but is also limited in dynamically adjusting execution paths based on real-time task status.\nThe Swarm framework[13], primarily used for educational purposes, is lightweight but lacks production-grade features and\nflexibility, making it insufficient for real-world complex tasks. The Magnetic-One framework[14] explores a novel multiagent collaboration architecture to overcome the limitations of single large models in handling complex tasks but faces\nchallenges in practical applications, such as the need for efficient communication and coordination mechanisms among\nagents, and potential computational and communication bottlenecks in large-scale multi-agent systems.\nMoreover, none of the above frameworks supports the Model Context Protocol (MCP). MCP is an open specification\ndesigned to standardize how context is passed between LLMs and applications[8]. MCP provides a structured framework that improves reliability, security, and functionality by addressing challenges such as standardizing communication,\nenhancing context management, improving security, enabling complex workflows, and promoting interoperability. MCP\ndefines a structured message format[8] that separates different types of information being sent to and from language models, including system messages, user messages, assistant messages, tool messages, and metadata. This structured approach\nallows for clearer delineation of roles and responsibilities within the communication flow, reducing ambiguity and potential\nconfusion for the model.\n\nOne of the key advantages of MCP is its plug-and-play characteristic, platform independence, and technology stack\nindependence. MCP acts as a universal interface, enabling developers to avoid the fragmentation of maintaining separate\n\n2\n\n\n-----\n\nintegrations. Once an application supports MCP, it can connect to any number of services through a single mechanism.\nThis dramatically reduces the manual setup required each time you want your AI to use a new API. Teams can focus on\nhigher-level logic rather than reinventing connection code for the 10th time. MCP replaces fragmented integrations with\na simpler, more reliable single protocol for data access[8], allowing AI applications to seamlessly connect to various tools\nand data sources without the need for custom coding each from scratch.\nTo address these limitations, this paper proposes a highly robust autonomous agent framework based on the ReAct\nparadigm. The framework introduces several innovative mechanisms to enhance adaptability and robustness in complex\nenvironments. One of the key innovations of this framework is its compatibility with the MCP protocol, which enables\nseamless integration with various tools and data sources, enhancing the framework’s versatility and applicability in diverse\nscenarios.",
  "This framework is built around two key concepts: **Agent** and **Tool** .\nAn agent is defined as an autonomous artificial intelligence entity with the following properties:\n\n1. **Autonomy:** Agents can execute tasks independently without human intervention, capable of self-action and selfcorrection.\n\n2. **Goal-Driven:** All actions of the agent are ultimately aimed at achieving specific goals.\n\n3. **Extensible:** The agents’ capabilities can be directly extended without modifying its underlying structure.\n\n4. **Limited Action Space:** An agent only executes actions within its defined action space.\n\n5. **Infinite State Space:** The agents’ states are derived from action feedback, which is determined by the environment.\n\n6. **Observable:** The agents’ operations are observable and modifiable, allowing users to intervene in its action trajectory.\n\n7. **Memory Capability:** The agents can remember its trajectories, and different agent instances can share and transfer\nmemories, enabling multi-agent collaboration.\n\n8. **Controllable Behavior:** An agent’s behavior tendencies can be controlled, which allowing users to adjust the\nagent’s inclination toward cautiousness or exploration.\n\nA tool is an encapsulated action logic that the agent can invoke, with the following properties:\n\n1. **Descriptive:** Tools have descriptive metadata, such as name, description, parameter requirements, and return\ndescription.\n\n2. **Parameterized:** Tools can receive input parameters that define their execution manner and conditions.\n\n3. **Semantic Feedback:** The response of a tool is semantically rich, typically in the form of text or a string, indicating\nthe execution result and environmental feedback.\n\nTools are divided into general tools and handoff tools. General tools are common tools an agent can use, while handoff\ntools enable task handover and memory transfer between agents. When an agent encounters a subtask it cannot solve, it\nchecks its handoff tools and passes the task to another agent with the appropriate capabilities.\n\n3\n\n\n-----",
  "Figure 1: Components of an Agent\n\nFigure 1 shows the architecture design of the agent. An agent consists of the following components:\n\n1. **Thought Engine:** The brain of the agent, typically a fine-tuned general language model.\n\n2. **Tools:** Encapsulated action logic that defines the agent’s action space.\n\n3. **Step Estimator:** Estimates the number of steps required to complete a task based on the agent’s action space.\n\n4. **Penalty:** Implements the probabilistic penalty mechanism for the timely abandonment strategy.\n\n5. **Memory:** Stores the agent’s execution trajectories and state changes, enabling short-term memory and memory\ntransfer between agents.\n\n6. **Request Resolver:** Understands tasks, decomposes subtasks, and extracts goals.\n\n7. **Next Move Scheduler:** Estimates the next action based on the agent’s action trajectories and state changes,\nselecting the appropriate ability and generating corresponding parameters.\n\n8. **Executor:** Executes the actions, analyzes the results and environmental feedback, and updates the agent’s trajectory.\n\n9. **Introspection:** Summarizes the agent’s actions and extracts task results after the agent reaches its final state or\nexits due to the timely abandonment strategy.\n\nFigure 2: Task Execution Workflow of an Agent\n\n4\n\n\n-----\n\nFigure 2 illustrates the process of a task execution workflow. It involves three entities: Human, Agent, and Tool. The\nprocess begins with the Human assigning tasks to the Agent. Upon receiving the task, the Agent uses a Tool to perform\nthe task. After the Tool completes the task, it sends feedback to the Agent. Finally, the Agent responds to the Human\nwith the results of the task execution. This sequence of actions demonstrates how an agent interacts with humans and\ntools to accomplish tasks.\n\nFigure 3: Task Execution Workflow Involving Multiple Agents\n\nFigure 3 illustrates the process of a task execution workflow involving multiple agents. It involves four entities: Human,\nAgent1, Tool1, HandoffTool, Agent2, and Tool2. The process begins with the Human assigning a task to Agent1. Upon\nreceiving the task, Agent1 uses Tool1 to perform the task. After Tool1 completes its part, it sends feedback to Agent1.\nHowever, when Agent1 encounters a subtask that it cannot solve (e.g., Tool2 is inaccessible to Agent1), it uses the\nHandoffTool to pass the task and its memories to Agent2. Agent2 receives the task and memories, uses Tool2 to complete\nthe task, and sends feedback to Agent1. Finally, Agent1 responds to the Human with the overall results of the task\nexecution. This sequence demonstrates how multiple agents interact with humans, tools, and handoff mechanisms to\ncollaboratively accomplish complex tasks.",
  "To ensure compatibility with the MCP protocol, I design and implement an MCP tool adapter and session management\nmechanism. The MCP protocol, a standardized communication protocol, aims to address the interaction challenges\nbetween large language models and external tools, providing flexible tool invocation capabilities for agents.\n\n**Tool Adapter** The core idea of the MCP tool adapter is to encapsulate MCP-defined tool interfaces into tools that agents\ncan directly invoke. The adapter parses the tool description information defined by the MCP protocol to dynamically\ngenerate metadata for tools, including tool names, parameter types, and descriptions. This encapsulation allows agents\nto invoke tools from different sources in a unified manner without concerning themselves with the specific implementation\ndetails of the tools.\n\n**Clients** The MCP client is responsible for establishing connections with MCP servers and managing the communication\nprocess. I implement multiple client adapters, including those based on standard input/output, Server-Sent Events (SSE),\n\n5\n\n\n-----\n\nand WebSocket, to support different types of MCP servers. The client design adopts a modular approach, dynamically\nselecting the appropriate communication method based on the specific MCP server type.\n\n**Session Management** To ensure efficient and reliable communication between agents and MCP servers, I implement\nthe session persistence mechanism, which maintains session states, allowing agents to seamlessly switch tool invocations\nacross different task executions without re-establishing connections.",
  "Algorithm 1 is designed to dynamically determine the next move of an agent based on its prior trajectory and available\ntool set.\n\n**Al** **g** **orithm 1** ReAct Based Action Strate gy\n**Input:** User request *r*, Trajectory *j*, Current state *s*, Tools *t*\n**Output:** Next move (t”, a’)\n\n1: Extract events related to *r* from *j* and *s* in chronological order, denoted as *e*\n2: **if** *r* is completed according to *e* **then**\n3: return success *▷* Goal achieved, reaching the final state\n\n4: **else**\n\n5: Analyze and extract the remaining subtasks, denoted as *u*\n6: **if** *t* contains tools capable of completing *u* **then**\n7: Select a subset of relevant tools *t* *[′]*\n\n8: **else**\n\n9: return failure *▷* Goal determined to be unachievable, reaching the final state\n\n10: **end if**\n\n11: Plan the next action *m* based on *e*\n12: Select the tool *t* *[′′]* from *t* *[′]* for executing *m*\n13: Generate arguments *a* *[′]* required for *t* *[′′]* according to *m*\n14: return *t* *[′′]*, *a* *[′]*\n\n15: **end if**\n\nThis approach enables the agent to adaptively adjust its actions in response to changing task requirements and environmental feedback, ensuring efficient progress toward the ultimate goal. By leveraging chain-of-thought (CoT) reasoning and\ntool matching analysis, the algorithm enhances the agent’s decision-making capabilities and ensures robust task execution.\nThe algorithm consists of several key steps, including relevant event extraction, task completion status determination,\ntool matching analysis, next move estimation, and action execution with state updates. Each step is carefully designed to\noptimize the agent’s performance in complex and dynamic environments. The main steps of the algorithm are as follows:\n\n**Relevant Event Extraction** Extract events related to the task goal from the trajectory. This step uses a language\nmodel to identify and extract relevant events, which are then sorted chronologically.\n\n**Task Completion Status Determination** Analyze the extracted events to determine whether the task goal has been\nachieved. This step uses chain-of-thought (CoT) to optimize the accuracy of the judgment.\n\n**Tool Matching Analysis** Analyze the agent’s tool set to determine if there are suitable tools to advance the task goal.\nThis step also uses CoT to ensure the judgment is strictly based on the extracted events.\n\n**Next Move Estimation and Tool Selection** Estimate the next move based on the prior trajectory and task goal,\nselect the appropriate tool, and generate the corresponding arguments.\n\n**Action Execution and State Update** Execute the next move just estimated, obtain the result and environmental\nfeedback, then update the agent’s trajectory.\n\n6\n\n\n-----",
  "Algorithm 2 is designed to dynamically control the agent’s task execution steps, allowing it to abandon tasks when they\nbecome unproductive or excessively resource-consuming.\n\n**Al** **g** **orithm 2** Timel y Abandonment Strate gy\n**Input:**\nUser request *r* ;\nAbandonment probability *p* ;\nPenalty coefficient *β* ;\nStep estimator *E* ;\nNext move estimator *N* ;\nExecutor *X* ;\n**Output:** Task result\n\n1: *s ←* *E* ( *r* ) *▷* Estimate required steps using the step estimator\n2: *c ←* 0 *▷* Initialize step count\n3: **while** True **do**\n4: *m ←* *N* ( *r, ...* ) *▷* Estimate the next action\n5: *X* ( *m* ) *▷* Execute the next action\n\n6: **if** *r* is completed **then**\n7: **return** success *▷* Task successfully completed\n8: **else if** *r* is confirmed to be unattainable **then**\n9: **return** failure *▷* Task failed\n\n10: **end if**\n\n11: **if** *c > s* **then** *▷* Current steps exceed estimated steps\n\n12: rand *∼* Uniform(0 *,* 1)\n13: **if** rand *> p* **then** *▷* Continue the task with probability 1 *−* *p*\n14: *p ←* ( *β × p* ) mod 1 *▷* Apply penalty\n15: **else** *▷* Abandon the task with probability *p*\n16: **return** failure *▷* Abandoned, task failed\n17: **end if**\n\n18: **end if**\n\n19: *c* + + *▷* Increment step count\n20: **end while**\n\nThis algorithm introduces two hyperparameters: **abandonment probability** *p* and **penalty coefficient** *β*, which\ntogether regulate the agent’s tendency to abandon tasks.\n\n**Abandonment Probability** *p* *p* represents the probability that the agent will abandon the task when the current steps\nexceed the estimated steps. A higher *p* makes the agent more likely to abandon tasks early, reflecting a more cautious\nbehavior. A lower *p* encourages the agent to persist longer, reflecting a more exploratory behavior.\n\n**Penalty Coefficient** *β* *β* is used to amplify the abandonment probability *p* when the agent continues executing the\ntask beyond estimated steps. A higher *β* increases the penalty more rapidly, forcing the agent to abandon tasks sooner.\nA lower *β* allows the agent to continue exploring for longer before abandoning the task.\n\n**How** *p* **and** *β* **Work Together** When an agent’s current steps exceed the estimated steps, it enters a probabilistic\ndecision-making phase. If the agent continues (with probability 1 - *p* ), the abandonment probability *p* is penalized with\nequation 1, which increases the likelihood of abandonment in subsequent steps.\n\n*p* = ( *β × p* ) mod 1 (1)\n\nThe algorithm consists of several key steps, including step estimation, action execution, task completion status determination, and probabilistic abandonment. This ensures that the agent dynamically balances exploration and caution,\nadapting its behavior based on task complexity and environmental feedback.\n\n7\n\n\n-----",
  "In multi-agent collaboration scenarios, memory storage and sharing mechanisms are crucial for achieving information\nsynchronization and effective collaboration among agents. The following sections detail the multi-agent memory sharing\nmechanism implemented in this paper from three aspects: memory storage, memory update, and memory transfer.\n\n**Memory Storage** In this mechanism, each agent is designed with its own memory storage structure to save the experience and knowledge accumulated during task execution. Memories are stored in the form of an ordered dictionary, a data\nstructure that preserves the insertion order of memories, facilitating subsequent retrieval and processing in chronological\norder. The content of each memory includes timestamps, agent identifiers, action executions, and action summaries.\nTimestamps indicate when the memory was created, agent identifiers specify which agent generated the memory, action\nsummaries concisely describe the core content of the action and its feedback, and action executions detail the actions taken\nby the agent and their associated parameters. This structured storage approach ensures that memories are well-organized\nand readable, providing a foundation for subsequent memory transfer and fusion.\n\n**Memory Update** After each action, the agent records the result as a new memory and adds it to its memory storage.\nWhen estimating the next move, the agent retrieves its action trajectory and current state from memory to guide its\nsubsequent actions. Once a task is completed or a termination condition is met, the agent resets its memory to prepare\nfor the next task.\n\n**Memory Transfer** Memory transfer is a critical step in multi-agent collaboration. When an agent needs assistance\nfrom other agents or needs to delegate a task, it passes its memories to the target agent. This allows the target agent to\nunderstand the current task status and existing experience of the source agent. Upon receiving the memories, the target\nagent merges them with its existing memories. This mechanism enables rapid information sharing among agents, avoiding\nredundant exploration and enhancing system efficiency. For example, if Agent A encounters a complex task and lacks\nspecific knowledge or capabilities, it can transfer its task context and accumulated experience to Agent B via the memory\ntransfer mechanism. With this information, Agent B can better understand the task requirements and effectively assist\nAgent A in completing the task.",
  "The primary objective of this experiment is to evaluate the performance of three frameworks: autono (the proposed\nframework), autogen, and langchain, in handling tasks of varying complexity. Specifically, the experiment aims to assess\ntheir adaptability and robustness as task complexity increases. Based on task complexity, the experimental tasks are\ncategorized into three types:\n\n1. **Single-Step Tasks:** Simple tasks that can be completed with a single action.\n\n2. **Multi-Step Tasks:** Tasks that require a sequence of actions to complete.\n\n3. **Multi-Step Tasks with Possible Failures:** The most complex category, these tasks involve actions that may fail\nduring execution, necessitating error correction and retry mechanisms. This category simulates real-world production\nenvironments with uncertainties and failures, posing higher requirements for algorithm robustness and adaptability.\n\nTo comprehensively and objectively evaluate the performance of each framework, success rate is selected as the evaluation metric. Success rate is calculated as the percentage of tasks executed successfully as requested relative to the total\nnumber of tasks. The success rate of each framework is computed for each task type to facilitate an intuitive comparison\nof their performance differences. To ensure the reproducibility of the experiment, the experimental environment has been\npushed to a public GitHub repository at `https://github.com/vortezwohl/experiment-03-22-2025` . Anyone can use\nthis repository to reproduce the experiment.",
  "**Single-Step Tasks** The results indicate that autono performs exceptionally well in single-step tasks. With GPT-4omini as the thought engine, it achieves a success rate of 96.7%. When using Qwen-plus and DeepSeek-v3, the success\nrate reaches 100%. This demonstrates that autono is highly stable and effective in handling simple tasks. In contrast,\nautogen shows slightly inferior performance in single-step tasks, achieving a 90% success rate with both GPT-4o-mini and\nQwen-plus. DeepSeek-v3 is unsupported in autogen, making evaluation impossible (marked as N/A). Langchain achieves\nsuccess rates of 73.3% (GPT-4o-mini), 73.3% (Qwen-plus), and 76.7% (DeepSeek-v3) in single-step tasks. While langchain\ncan complete most single-step tasks, its success rate is significantly lower than that of autono and autogen.\n\n8\n\n\n-----\n\n|Framework|Version|Model|Single-Step Tasks|Multi-Step Tasks|Multi-Step Tasks with Possible Failures|\n|---|---|---|---|---|---|\n|autono|1.0.0|GPT-4o-mini|96.7%|100%|76.7%|\n|autono|1.0.0|Qwen-plus|100%|96.7%|93.3%|\n|autono|1.0.0|DeepSeek-v3|100%|100%|93.3%|\n|autogen|0.4.9.2|GPT-4o-mini|90%|53.3%|3.3%|\n|autogen|0.4.9.2|Qwen-plus|90%|0%|3.3%|\n|autogen|0.4.9.2|DeepSeek-v3|N/A|N/A|N/A|\n|langchain|0.3.21|GPT-4o-mini|73.3%|13.3%|10%|\n|langchain|0.3.21|Qwen-plus|73.3%|13.3%|13.3%|\n|langchain|0.3.21|DeepSeek-v3|76.7%|13.3%|6.7%|\n\n\nTable 1: Experimental Results\n\n**Multi-Step Tasks** In multi-step tasks, autono continues to lead, achieving success rates of 100% (GPT-4o-mini), 96.7%\n(Qwen-plus), and 100% (DeepSeek-v3). This highlights its robust capability in handling complex tasks. Autogen’s\nperformance in multi-step tasks is notably inconsistent. With GPT-4o-mini, the success rate is 53.3%, while with Qwenplus, it drops to 0%. DeepSeek-v3 is unsupported. This indicates that autogen’s compatibility with third-party models is\nsuboptimal, and it struggles to effectively execute complex tasks. Langchain’s success rate in multi-step tasks is uniformly\nlow at 13.3% across all three models, reflecting significant limitations in handling complex task execution scenarios.\n\n**Multi-Step Tasks with Possible Failures** For multi-step tasks with possible failures, autono remains highly effective.\nWith GPT-4o-mini, it achieves a 76.7% success rate, and 93.3% with both Qwen-plus and DeepSeek-v3. This underscores\nits strong adaptability and robustness, as it can automatically correct errors and leverage past experiences to ensure task\nsuccess. Autogen continues to underperform in these tasks, achieving only a 3.3% success rate with both GPT-4o-mini and\nQwen-plus. DeepSeek-v3 is unsupported. This further highlights autogen’s limitations in handling complex and uncertain\ntasks, lacking effective error-handling mechanisms and retry logic. Langchain achieves success rates of 10% (GPT-4omini), 13.3% (Qwen-plus), and 6.7% (DeepSeek-v3) in multi-step tasks with failures, indicating insufficient robustness in\nhandling complex and uncertain tasks.",
  "Overall, autono demonstrates superior performance across all task types, particularly in multi-step and multi-step tasks\nwith possible failures. This demonstrates its superior adaptability and robustness in handling complex tasks, underscoring\nits advantages in complex task processing and validating the effectiveness of its ReAct-based action strategy and timely\nabandonment strategy, and meeting diverse scenario requirements and providing reliable solutions for developers. Autogen\nperforms adequately in single-step tasks but sees a significant drop in success rate in multi-step and multi-step tasks with\npossible failures, especially when using non-OpenAI third-party models. Langchain consistently shows lower success\nrates across all task types, particularly struggling with multi-step and multi-step with possible failures, revealing notable\nlimitations in multi-step execution, adaptability, and robustness. Different models exhibit high performance within autono,\nindicating excellent compatibility with mainstream models.",
  "The paper proposes a highly robust autonomous agent framework based on the ReAct paradigm, designed to solve complex\ntasks through ReAct-based action strategies and timely abandonment strategies. The framework’s key innovations include:\nReAct-based action strategies, timely abandonment strategies, and compatibility with the MCP protocol. Experimental\nresults demonstrate that the framework achieves high success rates in single-step tasks, multi-step tasks, and multi-step\ntasks with potential failures, showcasing strong adaptability and robustness in complex and uncertain environments.\nThe limitations of the framework are primarily reflected in the following aspects: First, the robustness of the framework\nrelies heavily on ReAct-based action strategies and timely abandonment strategies, which may still be insufficient to\nhandle extreme complexity or dynamically changing environments. Second, while the multi-agent collaboration mechanism\nsupports memory sharing, it does not deeply explore how to optimize communication efficiency and coordination among\nagents, which could become a bottleneck in large-scale multi-agent systems. Additionally, the experimental section focuses\nmainly on task success rates, but does not sufficiently address the optimization of task execution efficiency and resource\nconsumption.\nFuture work could explore several directions: First, further optimizing the timely abandonment strategy to enable\nsmarter judgments of task feasibility and reduce unnecessary resource waste. Second, investigating more efficient communication and coordination mechanisms for multi-agent systems to support large-scale complex task collaboration. Third,\nexpanding the framework’s applicability, particularly in real-time task scenarios, to validate its performance. Fourth,\nintegrating reinforcement learning techniques to enhance the agents’ adaptability and decision-making efficiency. Overall,\n\n9\n\n\n-----\n\nthe framework proposed in this paper provides a new perspective for designing intelligent agents in complex tasks, but\nthere is still room for improvement, and future research can build on this foundation to further refine the framework.",
  "[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. *arXiv preprint*, 2017.\n\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. *arXiv preprint*, 2018.\n\n[3] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and\nDenny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In *Conference on Neural*\n*Information Processing Systems (NeurIPS)*, 2022.\n\n[4] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing\nreasoning and acting in language models. *arXiv preprint*, 2022.\n\n[5] Yashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of intelligent llm agents.\n*arXiv preprint*, 2023.\n\n[6] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and\nEnhong Chen. Understanding the planning of llm agents: A survey. *arXiv preprint*, 2024.\n\n[7] Lei Huang et al. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open\nquestions. *arXiv preprint*, 2023.\n\n[8] Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. Model context protocol (mcp): Landscape, security threats,\nand future research directions. *arXiv preprint*, 2025.\n\n[9] Morteza Bahrami, Muharram Mansoorizadeh, and Hassan Khotanlou. Few-shot learning with prompting methods.\nIn *2023 6th International Conference on Pattern Recognition and Image Analysis (IPRIA)*, pages 1–5, 2023.\n\n[10] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun\nZhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversation. *arXiv preprint*,\n2023.\n\n[11] CrewAI Team. CrewAI: Framework for orchestrating role-playing, autonomous AI agents. `https://github.com/`\n`crewAIInc/crewAI`, March 2023.\n\n[12] Harrison Chase. LangChain: Build context-aware reasoning applications. `https://github.com/langchain-ai/`\n`langchain`, October 2022.\n\n[13] OpenAI Solution Team. Swarm: Educational framework exploring ergonomic, lightweight multi-agent orchestration.\n`https://github.com/openai/swarm`, October 2024.\n\n[14] Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Erkang Zhu, Friederike Niedtner,\nGrace Proebsting, Griffin Bassman, Jack Gerrits, et al. Magentic-one: A generalist multi-agent system for solving\ncomplex tasks. *arXiv preprint*, 2024.\n\n10\n\n\n-----",
  "Fig. 1: Attention-guided Grad-CAM (AG-CAM) visualizations show the ChartGemma-3B [39] model’s ‘reasoning’ over images and text\nquestions from the Mini-VLAT [42]. Attention Saliency maps, highlighting important areas in red, are overlain on each chart image. ✓\nand *×* indicate correct vs incorrect responses, respectively. Each model layer and input token produces a unique result that can be\nused to interrogate *how* a Vision Language Model (VLM) produces its responses.\n\n**Abstract** — Vision Language Models (VLMs) demonstrate promising chart comprehension capabilities. Yet, prior explorations of their\nvisualization literacy have been limited to assessing their response correctness and fail to explore their internal reasoning. To address\nthis gap, we adapted attention-guided class activation maps (AG-CAM) for VLMs, to visualize the influence and importance of input\nfeatures (image and text) on model responses. Using this approach, we conducted an examination of four open-source (ChartGemma,\nJanus 1B and 7B, and LLaVA) and two closed-source (GPT-4o, Gemini) models comparing their performance and, for the open-source\nmodels, their AG-CAM results. Overall, we found that ChartGemma, a 3B parameter VLM fine-tuned for chart question-answering (QA),\noutperformed other open-source models and exhibited performance on par with significantly larger closed-source VLMs. We also found\nthat VLMs exhibit spatial reasoning by accurately localizing key chart features, and semantic reasoning by associating visual elements\nwith corresponding data values and query tokens. Our approach is the first to demonstrate the use of AG-CAM on early fusion VLM\narchitectures, which are widely used, and for chart QA. We also show preliminary evidence that these results can align with human\nreasoning. Our promising open-source VLMs results pave the way for transparent and reproducible research in AI visualization literacy.\n**Code and Supplemental Materials:** `https://osf.io/fp3rg/?view_only=9b11aaec4ddf4656b205ebc53f4ef9db`\n\n**Index Terms** —Vision Language Models, Visualization Literacy, Explainability, Chart Question and Answering\n\n\n**1** **I** **NTRODUCTION**\n\nThe increasing sophistication of large language models (LLMs) has\nexpanded their use to diverse data analysis and visualization applications [58]. These include generating visualization code [38], developing\nvisual analytic chatbots [56], providing support for visual design [28],\ndata storytelling [49], and to some extent, visualization education [14].\nHowever, LLMs alone cannot process, integrate, and generate insights\nfrom multiple data types, such as images and text. Consequently, their\nanalytical capabilities to examine existing visualizations, for example,\nto answer questions or produce textual summaries, are constrained;\nusing LLMs alone requires access to the underlying chart data, which\nis not always available. To overcome this limitation, new classes of\n\n*•* *Lianghan Dong and Anamaria Crisan [a7dong, ana.crisan]@uwaterloo.ca*\n*are with the University of Waterloo.*\n\n*Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication*\n*xx xxx. 201x; date of current version xx xxx. 201x. For information on*\n*obtaining reprints of this article, please send e-mail to: reprints@ieee.org.*\n*Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx*\n\n\nmultimodal models, notably vision language models (VLMs) have been\ndeveloped [18, 59]. VLMs offer a distinct advantage by accepting and\nintegrating image and text inputs, enabling them to generate textual\nanalyses of visual content. This capacity allows for the interpretation\nof visualizations within media, such as magazine graphics, newspaper charts, or government reports, even in the absence of underlying\ndatasets. However, the effectiveness of VLMs hinges on their ability to\naccurately synthesize and interpret both visual and textual information.\n\nIn this research, we interrogate the visualization literacy of VLMs.\nVisualization literacy is broadly defined as the ability to “read and\nunderstand” visual representations of data so as to draw “meaning from\npatterns, trends, and correlations” that they may contain [22, 23, 32].\nWhether VLMs possess such abilities, or analogous reasoning skills,\nis an open question [26]. Prior research from the Natural Language\nProcessing (NLP) and ML/AI research communities has prioritized exploring VLM’s reasoning abilities with naturalistic images and scenes\n(e.g., containing cats and dogs, or people) [18, 59]. However, data visualizations present distinct challenges compared to naturalistic images,\nrequiring the precise translation of visual encodings into quantitative\ndata . The alignment between the chart content and a resultant response also requires understanding chart-specific and data-specific\n\n\n-----\n\nsemantics (e.g., “the upward trend on the scatter chart reflects growth in\nsales” [9]). While recent research has explored the visualization literacy\nof VLMs [5, 34, 37, 43], their use of proprietary (closed-source) models (e.g., GPT-4o, Gemini) limited their analysis to response accuracy\nresults on standard tests (e.g., mini-VLAT [42], VLAT [31]).\n***We probe the visualization literacy of VLMs by interrogating their***\n***internal reasoning when responding to questions about chart im-***\n***ages.*** Like prior work, we used questions from mini-VLAT [42] and\nVLAT [32] to examine the visualization literacy of VLMs. However,\nunlike prior work, which focused on evaluating the model’s outputs,\nwe used open-source models to explore what the models “see” when\nattempting to respond to a question. To do so, we developed a variation\nof Attention-Guided Gradient-weighted Class Activation Mapping (AGCAM) [33] for Chart QA tasks to explore relationships and important\nfeatures between image and text tokens when generating a response to\na question. We use our approach to interrogate what different models\n“look at” when responding to mini-VLAT questions, what this reveals\nabout their spatial and semantic reasoning capabilities, and how models\ncompare to people’s focus and reasoning. We summarize our findings\nalong the following considerations:\n\n  - **The Good:** We show that small VLMs fine-tuned on chart QA\ntasks are nearly as performant as much larger closed-source models and with the added benefit of greater transparency. Using\nChartGemma [39], we demonstrate the spatial and semantic abilities of VLMs and areas where they still struggle. We also show\nthe AG-CAM can help us contrast VLM reasoning with that of\npeople. Overall, we see a lot of potential for open-source models.\n\n  - **The Bad:** Even beyond overall performance, visualization literacy is not uniform across VLMs, like people they vary in their\nabilities. This issue may be more consequential for smaller models than larger ones and makes the choice of VLM important\nwhen embedding them into downstream applications.\n\n  - **The Ugly:** VLMs build their reasoning over layers and by learning\nrelationships between text and image tokens. There is not always\none single, simple, representative image that captures their entire\nreasoning processes. Moreover, interpreting the AG-CAM results\ncan be more of an art than a science. However, this is not a\nfatal flaw, but rather opens exciting possibilities toward nuanced\nexaminations, which we initiate here, into VLM capabilities.\n\nCollectively our research makes the following contributions:\n\n  - We describe and release an adaptation of Attention Guided GradCAM (AG-CAM) for ChartQA on early-fusion VLMs (§4), including a companion application to explore its use.\n\n  - We conduct an exploration of visualization literacy of four opensource VLMs, using AG-CAM to examine their spatial and semantic reasoning abilities. We summarize VLM strengths and\nlimitations and conduct a preliminary comparison to people.\n\n  - Discussions and future directions toward open-source VLM use\nand challenges and opportunities for exploring their ‘reasoning’.\n\nThe rapid adoption of VLMs by researchers, practitioners, and corporations warrants closer scrutiny. Our research provides a mechanism\nand preliminary results for probing the capabilities of these models,\nsetting up fruitful avenues for future work and experimentation.\n\n**2** **R** **ELATED** **W** **ORK**\n\nWe summarize pertinent prior research for assessing visualization literacy (in humans and for VLMs), chart question-answering, and finally\nAI model explainability.\n\n**2.1** **Assessments of Visualization Literacy**\n\nPioneering research into visualization literacy proposed various direct\nand indirect methods of assessments. Boy *et. al.* [8] proposed using\nitem response theory to directly assess visualization literacy on a small\nrange of charts – this work inspired VLAT and follow-on research.\nWhile Börner *et. al.* [10] explored the general public visualization\nliteracy indirectly through a museum setting. More recent research has\n\n\nfocused more on direct testing. A widely used approach for studying\nvisualization literacy is the VLAT [31], a 53-item questionnaire that\nposes multiple questions using 12 chart types and relating to eight visualization tasks. A recent derivative, the mini-VLAT [42], demonstrates\nsimilar performance from a shorter assessment. Both assessments are\nwidely for assessing visualization literacy in people. Nobre *et. al.* [41]\nuses VLAT to identify barriers to visualization literacy. Hedayati and\nKay [22] used a modified 36-item version of VLAT to explore what\nuniversity students learn in visualization classes. The VLAT test has\nalso been shown to correlate with other cognitive characteristics (e.g.,\nnumeracy), that are generally essential for understanding and working with data. There also exist variations and extensions to VLAT.\nCritical Thinking Assessment for Literacy in Visualizations (CALVI)\naddresses the potential for readers to be misinformed by visualizations\nand draws inspiration from the VLAT. Saske *et. al.* [47] develop a\nMultidimensional Assessment of Visual Data Literacy (MAVIL) to\nassess visualization and data literacy along six dimensions, ranging\nfrom cognitive capabilities to aesthetics elements, including avenues for\nself-assessment of literacy skills. Taken together, all these tests roughly\narrive at the same conclusion, which is that visualization literacy is a\ntestable skill, varies in the human population, and should be considered\nwhen assessing data visualizations.\nThese same tests are now applied to assess vision language models\n(VLMs). Early work by Haehn *et. al* [20] explored CNN graphical\nperception, a complementary, but not identical to visualization literacy.\nMore recent studies focus on VLMs. Hong *et. al.* [23] found that\nGemini [19] and GPT-4o [1], using a modified VLAT, underperform\nhuman baselines, relying on internal knowledge. Conversely, Li *et.*\n*al.* [34] showed comparable or superior VLM performance (GPT-4o,\nClaude 3 Opus, Gemini) to humans baselines. Benedeck and Statsko [5]\nexamining only GPT-4o, further confirms that VLMs excel at certain\nvisualization tasks and struggle with others. Pandey and Ottley [43]\nused a unique prompting strategy to benchmark model performance\n(GPT-40, Claude, Gemini, and Llama) on both VLAT and CALVI to delineate the strengths and limitations of VLMs across visualization tasks.\nFinally, Lo and Qu [37], exploring explore that closed-source (GPT4o, Gemini, Co-pilot) and open-source models (LLaVA-1.5 7B [35]),\nfound evidence that VLMs possess sufficient visualization literacy to\ninterpret misleading charts. However, because these studies prioritize\nclosed-source models, they limit insight into VLM internal reasoning.\n*Unlike prior work, our research attempts to probe what VLMs attend*\n*to and prioritize when responding to a prompt. In doing so, we provide*\n*another critical lens for examining the visualization literacy of VLMs.*\n\n**2.2** **Chart Question and Answering**\n\nVLMs can be used for a variety of tasks, from generating captions, textto-image search, retrieval, and question answering [24, 25]. We focus\non Chart Question-Answering (QA) tasks; the VLAT is essentially a\nspecific version of Chart QA. Islam *et. al.* [26] conducted a evaluation of three closed-source (GPT-4o, Gemini, and Claude) and several\nopen-source models (Phi-3 [2]) against seven benchmark datasets of\nchart-specific tasks (e.g. QA, captioning, fact-checking, etc.). Their\nresults demonstrate the VLMs do possess impressive capabilities for a\nvariety of chart tasks, but also encounter common problems (e.g. hallucinations, sensitivity to prompts) and make errors. They also show that\nPhi-3 can be more performant than the closed-sourced models on zeroshot chart QA tasks. For some reason, prior research into visualization\nliteracy has omitted open-source models. However, several exist that\nare fine-tuned specifically for chart QA and understanding, for example,\nChartGemma [39], ChartLLama [21], and ChartAssistant [40]; of these\nChartGemma is the most performant and we investigate it here. We\nthink this is an oversight as performant open-source models are more\ntransparent than their closed-source counterparts and enable the reproduction of experimental results because the weights are available. This\nis important given the rapid evolution of closed-source VLMs, which\nproduced different results even in closely timed studies ( [5, 34, 37, 43]).\n*Here, we continue to explore the capabilities of open and closed*\n*source models and ChartQA tasks. However, unlike prior which focuses*\n*just on the outputs, we also examine the model’s internal reasoning.*\n\n\n-----\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n\n\n\n\n\n\n\n|“What|Col2|distance|Col4|s|\n|---|---|---|---|---|\n|have|customer||||\n|traveled in the most?”|||the taxi||\n\n\n\ncursively multiplying attention matrices, highlighting input regions\ninfluencing output, and is represented as an attention map [3]. Conversely, Layer-wise Relevance Propagation (LRP) propagates relevance\nscores backward; the idea was initially developed for CNNs and it was\nlater adapted for ViTs [11, 52]. However, while self-attention indicates\nwhat a model attends to, it does not reveal the influence of individual\ninput elements on the final response. Combining attention with gradients (e.g., [33,61]) attempts to address this limitation, bringing together\nthe best of both worlds. However, this combined approach has been\nprimarily developed for and demonstrated in classification tasks and its\nuse for more complex QA tasks requires further development.\n*We adapted an attention-guided GradCAM approach to visualize*\n*the reasoning process of VLMs when responding to chart QA tasks. We*\n*show this reasoning process through attention-saliency maps overlain*\n*on chart images. To our knowledge, this is the first application of*\n*attention and gradient-based methods to probe visualization literacy.*\n\n**3** **B** **ACKGROUND** **: V** **ISION** **L** **ANGUAGE** **M** **ODELS**\n\nAhead of discussing our methodology, we first present a brief overview\nof VLMs and the specific architectures that are the focus of our work.\n\n**3.1** **VLM classes and Architectures**\n\nVision language models are a type of multi-modal model that can\nlearn simultaneously from images and text [18]. There are different\ncategories of such models. The first is multi-modal vision language\n\n\ndifferent transformer block layers of the language model component.\nEarly fusion models do not use cross-attention, but can still learn relationships between image and query tokens through self-attention\nmechanisms (we will describe the significance of this shortly). If the\nreader is unfamiliar with attention mechanisms, a pithy explanation is\nthat it is a dynamic weighting system that allows the model to focus on\nthe most informative features in a sequence of tokens. Self-attention\noperates on a single sequence, while cross-attention can operate on\nmultiple sequences. Between the two approaches, EF-type models are\nthe most widely used because of their architectural simplicity, computational efficiency, and the ease of using different pre-trained vision and\nlanguage models without significant resource overhead. As they are the\nmost common class of VLMs, we explore them here; comparing both\nthe EF and DF models would be intractable in the scope of this work as\nboth operate in different ways and would require different treatments.\n\n3.1.2 Early Fusion VLM Pipeline\n\nAs a final piece, we summarize the early fusion VLM pipeline shown\nin Figure 2. To begin, a person can provide some query (e.g., “What\ndistance have customers traveled in the taxi the most”) and some input\n(e.g., a histogram image), and the VLM produces a textual response\n(e.g., “40 km”). In our research, these queries are drawn from the\nmini-VLAT and VLAT assessments. Both the query text and image\npixels are processed in a manner according to their data types.\n\n\n\n-----\n\nFor images, each is re-sized to a common dimension and then split\ninto non-overlapping patches. These patches have a standard size (e.g.,\n\n[16 pixels x 16 pixels x 3 RGB layers]) that are individually flattened\ninto a vector and then concatenated into a sequence of size [ *S* *I* *, f* ] [1]\nwhere *S* *I* is the total number of patches and *f* is the dimensions of\nthe flattened patch vector. For images, the patches are processed by\na pre-trained Vision Transformer (ViT) to produce an initial `image`\n`embedding` . The ViT used in VLMs comes from a simpler model,\nfor example, CLIP [44] or some of its more recent variations (e.g.,\nSigLIP [60]), which use a contrastive pre-training method that already\nlearns some associated between text and images. This is important only\nfor how embeddings are constructed; these ViTs, while suitable for\nclassification, cannot generate text. The `image embedding` is the raw\nencoded representation of visual information of the image; it represents\nthe image data in the ViT image latent space only and has not yet been\nconnected with the query. The `image embedding` is further processed\nby an adaptor, whose role is to transform the embedding vector to\nmatch the language model’s input space. We refer to this output as the\n`adapted image embedding`, it has size [ *S* *I*, *f* *[′]* ].\nText is simply tokenized resulting in a [ *S* *Q*, *v* ] vector, where *S* *Q* is the\ntotal sequence length and *v* is the vocabulary size of the tokenizer. The\n`adapted image embedding` is eventually concatenated with the text\ntokens of the query [2] . The concatenated image and text tokens produce\nthe `fused input embedding`, which is processed by the language\nmodel. The self-attention layers of each transformer block can learn\nrelationships between the text and image tokens, which we can directly\nobserve and use to interrogate the model’s internal reasoning.\n\n**4** **M** **ETHODOLOGY**\n\nIn this section, we describe how we adapted Attention-Guided Gradientweighted Class Activation Mapping (Grad-CAM) from Leem and\nSeo [33] to visualize the reasoning processes of early fusion VLMs.\nThe key differences between our approach and theirs are as follows.\nFirst, Leem and Seo [33] developed their method for classification\ntasks. We implement an extension, summarized in Algorithm 1, that\nmakes it applicable to QA, which requires dealing with sequences of\ntext inputs *and* outputs. Second, their method does not return results for\neach token *q ∈* *Q*, whereas our approach does. Finally, our approach\ncan return results for a single layer or any range of layers in the model,\nenabling greater flexibility to probe its internal reasoning.\n\n**4.1** **Attention-Guided CAM (AG-CAM) for Chart QA**\n\nAG-CAM is an integration of two ideas, Grad-CAM, which quantifies the influence of individual elements of *I* influenced the model’s\nresponse, and self-attention, which tells us about the relationships between tokens in a sequence ( *S* ). The idea can be simply expressed as:\n\n*A* *ij* *·* *grad* *ij* (1)\n\nEach element *A* *i j* of the attention matrix A represents the amount of\ninformation flowing from a token *j* in the preceding layer ( *k* ) to token\n*i* in the subsequent layer ( *k* + 1 ), and where *i* and *j* are indices within\nthe sequence of tokens *S* = *S* *I* *∥* *S* *Q* . Weighting attention by gradients\ngives higher priority to relationships between tokens that influence the\nmodel’s output. This is visualized in a *attention-saliency (AS) map*\n(Figure 3); prior work [33] just refers to this as a heatmap, but, we use\n*AS map* to be more precise in the uniqueness of this output.\n\n4.1.1 Computing Attention and Gradients\nWe conduct a forward pass to compute the attention for each of the\nmodel’s layers the model’s layers (transformer blocks, *K* ; where 1 *≤*\n*k < K* ) and attention heads ( *H* ; where 1 *≤* *h < H* ):\n\n*F* *h* *[k]* *,q* [=] *[ softmax]* [(] *[A]* *[k]* *h,q* [)] (2)\n\nLeem and Seo [33] refer to normalized attention scores (aka attention\nweights) as the feature map ( *F* ), however, their approach *only computes*\n*the feature map for the [CLS] token*, a special token that is intended\n\n1 For the moment, we are ignoring the batch dimension\n2 Concatenation is more nuanced than we show here, see [53] for details.\n\n\n**Al** **g** **orithm 1** Attention Guided CAM for QA\n\n1: *out ←* model(**inputs)\n2: *logits ←* *out.logits*\n3: *y ←* ∑ *[S]* *s* =1 *[max]* *v* *logits* *[v]* *s*\n4: *y* .backward()\n\n5: Get *F*, *grad* in layers [ *start,* *end* ], where *grad* refers to *∂* *[∂]* *F* *[y]*\n\n6: **for** *k* = *start,...end* **do**\n7: *L* *h* *[k]* *[←]* *[F]* *h* *[k]* *[⊙]* *[ReLU]* [(] *[grad]* *h* *[k]* [)]\n\n8: *L* *[k]* *←* ∑ *[H]* *h* =1 *[L]* *h* *[k]*\n9: Get *L* *q* *[k]* at *q* th prompt of *L* *[k]*\n\n10: **end for**\n11: *L* *q* *←* ∑ *[K]* *k* *[L]* *q* *[k]*\n12: Select the image part of *L* *q*\n13: Normalize, reshape, overlay on original image\n\nto serve as the ViTs final classification response. *We extend this idea*\n*to compute the feature map for each token* *q ∈* *Q* *allowing for more*\n*flexible and fine-grained resolution* .\nThe backward pass computes the model’s gradients. Once again\nLeem and Seo [33] propagate gradients from a final classification result\n( *y* *[c]* ), however, in chart QA there is no classification result. Note that\nwhile the VLAT questions are multiple-choice and could technically\nhave a classification, we do not limit ourselves to this. To address\nthis limitation, we can treat the generation of the response as multiple\nclassification tasks for predicting each word. So, for each element of *S*,\nit can be classified into anyone of the tokenizer’s vocabulary ( *v* ). We\ncan sum the output probabilities as follows:\n\n*S*",
  "Independent of our efforts, Zhang *et. al.* [61] proposed a similar\napproach; we discovered this only in the final stages of preparing our\nwork. However, like Leem and Seo [33] their approach does not return\na result for each *q ∈* *Q* . We also note that our results show better\nlocalization overall and even across layers (Figure 6) relative to [61].\n\n4.1.2 Generating Attention Saliency Maps\n\nFinally, we can compute the attention saliency (AS) map for a single\nlayer ( *L* ) or a slice of layers, for each token *q* as follows:\n\n\nwhere [ *start,* *end* ] denotes the layers we select, with 1 *≤* *start ≤*\n*end ≤* *K* . We normalize and reshape the results of *L* *q* and overlay it on\nthe input image to explore the model’s reasoning. We use a rainbow\ncolor map scheme to visualize *L* *q*, where dark blue spots are not areas\nof interest and brighter red areas indicate higher importance. While\nrainbow color maps have some controversy [55], they are also the\ncurrent norms for saliency maps that we opt to retain. An additional\nconsideration from Leem and Seo [33] is to optionally normalize *F* *h* *[k]* *,q*\nby sigmoid( *G* ( *·* )) because it highlights more relevant pixels compared\nto softmax, which can hyperfixate. We explored both approaches (Figure 3) but opted to use primarily softmax when reporting our findings,\nwhich is more common and provided better localization for charts (i.e.,\na more focused response). We also experimented with different aggregation methods for *L* *q* ; namely using multiplication as is done in\nattention rollout [3], but found that summing across layers captured\nmore interesting artifacts. As we indicate in §4.2, we make all of these\noptions available in our companion application.\n*Overall, our approach generalizes both prior methods [33,* *61] to*\n*show more complex relationships between image and text tokens across*\n*layer slices of the model’s reasoning* . We also demonstrate that our\napproach can be applied across several models, of different sizes, and\nwith varying vision and language model components.\n\n\n*end*",
  "*k* = *start*\n\n\n*H*",
  "-----\n\nTable 1: Performance of VLMs on Mini-VLAT [42]. VLM responses vary across runs, in keeping with prior work [43] we show the average performance\n(% correct) across 10 runs. We use the performance baseline to add context to our exploration of the model reasoning.\n\n|Model #Params Language Model Vision Model|M.ini-VLAT Questions|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||Q1|Q2|Q3|Q4|Q5|Q6|Q7|Q8|Q9|Q10|Q11|Q12|",
  "Fig. 3: Sigmoid ( *G* ( *·* ) ) smoothing finds more correlations between pixels\nthan softmax ( *S* ( *·* ) ). While this may work well for natural scenes (e.g., the\ntiger), it introduces more noise for chart images. We implemented both,\nbut primarily report on the softmax results. In this attention-saliency (AS)\nmap, brighter colors up to red signify areas of more importance.\n\n**4.2** **Implementation and Availability**\n\nWe implemented our method in Python and released our code so that\nothers could build our work. To view our results we also provide\na lightweight Gradio application, a simple front-end framework that\nintegrates natively with Python. Our application provides configurable\ncontrols to select different models, to set AG-CAM parameters, such as\nwhether to apply *G* ( *·* ), to select layers to visualize, among other options.\nIt has the mini-VLAT [42], VLAT [33], and an additional modified\nversion of VLAT [41] available for use and it is also possible to upload\nimages. Our application is available on Hugging Face [3] ; to see a video\nof it in action please visit the supplemental materials. We do not claim\nthis application as a contribution, it is just a companion to our work\nintended to increase its transparency.\n\n**5** **P** **ROBING THE** **V** **ISUALIZATION** **L** **ITERACY OF** **VLM** **S**\n\nWe apply AG-CAM to compare what four different open-source VLMs\n“see” when attempting to answer mini-VLAT questions. We are interested not just in the variation of overall model responses, but also\ntheir internal processing, which we refer to as its “reasoning”. We are\nmotivated by two primary research questions:\n\n**RQ1:** How do VLMs vary in their responses and reasoning?\n\n**RQ2:** What kinds of reasoning capabilities do VLMs\ndemonstrate and how does this compare to people?\n\nWe explore these questions along multiple dimensions. The first is\nthe relationships between individual tokens in the query ( *q ∈* *Q* ) and\nassociated pixels in the image ( *p ∈* *I* ). We did so by generating attention saliency (AS) maps and comparing them between models (§5.1.2).\nWe focused more on tokens from *Q* and *I*, over the response, because\nthe model uses these to reason and generate an answer. The second\ndimension is between different types of charts, leveraging AS maps to\nexamine the errors models make (§5.1.3). Finally, we look across the\nmodel’s layers and how the AS maps compare to human annotations to\nmake judgments of models’ reasoning capabilities (§5.2). Throughout\nour investigations, we also made interesting and unexpected observations that we describe.\n\n3 `https://huggingface.co/spaces/uw-insight-lab/Multimodal_`\n`Understanding/`, running this space requires access to an Nvidia 1xL40S\ninstance (or larger). Approximate costs are $1.80 USD/hour to operate (roughly\n$1,300 USD/month). Due to costs, we do not run this space continuously.\nHowever, it is possible to clone it and run it yourself.\n\n\n**5.1** **RQ1: VLMs Fine-tuned on ChartQA Perform Best**\n\nOur first research question explores how different language models\nmight reason about a chart image ( *I* ) and a user-provided query ( *Q* )\nto produce a response. While we do present performance baselines\n(§5.1.1), we do not index our analysis on it. Instead, we use performance as a guide to explore *why* the model might have responded the\nway it did when probing its internal reasoning.\n\n5.1.1 Models and Baseline Performance\n\nWe primarily investigate four open-source VLMs. ChartGemma [39] is\ncurrently the most performant on images of data visualizations, having\nbeen specifically fine-tuned on Chart QA. As a basis of comparison, we\nalso explore other models of similar size: Janus-Pro-1B [13], Janus-Pro7B [13], and LLaVA-1.5-7B [35]. In Table 1 we show each model, its\nspecific vision and language model components, and its results on the\nmini-VLAT questions. ***One difference from prior work is we require***\n***all models to generate the response, not choose among multiple-***\n***choice options.*** A numeric response is marked correct if it is close ( *±* 2 )\nto the actual answer (e.g., a response of 41 or 42 Mps, instead of 40).\nAs VLM responses can vary, we report the average correctness from\n10 model runs. We contrast the open-source model performance to the\nGPT-4o [1] and Gemini-2-Flash [19] closed-source models. While the\nspecific details of closed-source architectures are not known at this time,\nwe can be certain they are significantly larger than the open-source\nmodels we investigate. Of the smaller models, ChartGemma is the most\nperformant, capable of rivaling even the closed-source models.\n**Overfitting and Generalization.** The mini-VLAT [42] and VLAT [32],\nalong with their responses, are publicly available. As such, it is reasonable to assume they may be part of VLM training datasets; if a\nvalid assumption, the models can be overfit to mini-VLAT questions\n(e.g., it has memorized the answer). Prior research [23], attempted to\nmitigate these issues by creating new questions, while still using chart\nand task types of the original VLAT. However, we do not take the same\napproach here.By visualizing AS maps it would be evident if a model\nmemorizes the answer but does not identify the appropriate elements\nof the chart to produce the results; the caveat being that this only works\nfor open-source models (in our opinion, a clear benefit of their use).\nIn fact, even if models are overfitting, our approach would allow us\nto interrogate if they are learning valid relationships between pixels\nand text tokens as part of that training process. As such, we are not as\nvulnerable to overfitting issues.\n\n5.1.2 Interpreting the Relationship between Text and Pixels\n\nIn Figure 4 we compare how the four open-sources models ‘reason’ and\nrespond to the question “What is the average internet speed in Japan?”\nusing the bar chart. The final column is a token that precedes the answer\ngeneration, which often, but not always, visualizes the answer. As we\nshow in this figure, and later sections of this manuscript, reasoning\nis built over layers and tokens and one single image does not always\ncapture the full result. However, this bar chart example is an exception.\nUsing it as a reference, we summarize key observations on VLM\nperformance, which can be explored via our companion application.\n\n**All models can recognize text.** All models can make associations\nbetween a query token and text that appears in the image. For example,\nall models can identify ’speed’ and ‘Japan’ in the chart axis and titles.\n\n\n-----\n\nFig. 4: Comparison of how different models “see” the chart image ( *I* ) in response to a common Query ( *Q* ) token. We show the attention saliency maps\nacross four open source models (ChartGemma, Janus 1-B and 7-B, an LLaVA) across each word of the query *‘What is the average internet speed in*\n*Japan?’* . The final column is an empty separator token, added by the models [6], which anchors the text generation of the output. Each model’s final\nresponse is shown below, only ChartGemma is correct. While these models are of comparable size, and share common vision components, they\nbehave differently; ChartGemma is the only model fine-tuned for ChartQA. *Note these are high-res images, zoom in to see details.*\n\n\nInterestingly, all models also show stronger associations with ‘Internet’\nin the chart axes over the title. Text can also be correctly read across the\nchart areas. For example, in Choropleth map example (Figures1 and 5),\nChartGemma makes associations between abbreviated and full state\nnames (e.g., WA is Washington) and in the bubble, pie, and treemap\ncharts it can identify labels (Figures 1, 6, and 8). VLMs are known to\npossess optical character recognition abilities [36], and so this result is\nnot surprising, but, verifies the AS maps show expected associations.\n\n**ChartGemma focuses more on visual encodings.** The Janus and\nLLaVA models do not show strong associations between *q ∈* *Q* and the\nvisual encoding elements, whereas ChartGemma does. One reason for\nthis is that ChartGemma is fine-tuned for ChartQA tasks, which not\nonly has better overall performance (Table 1) but creates stronger associations between pixel and text tokens. Looking closely at the Janus\nand LLaVA models, there is some importance placed on elements of the\nencoding, particularly the horizontal tops of the bars which show a faint\nblue hue. However, compared to ChartGemma they do not ascribe importance to one bar (i.e., the one associated with Japan) over the others.\nThis lack of focus on the encoding is reflected in the model’s response\nas well. LLaVA’s response appears drawn from its ’prior knowledge,’\nwhich refers to the information the model learned during pre-training\nfrom vast amounts of text and image data, and ignores the chart e ~~ntirely~~ .\nBoth Janus models return the incorrect response; Janus-1B’s invalid\nresult is a non-sense computation. A final observation is that AS maps\nfor ChartGemma appear more focused (e.g., show more specific areas\nof importance) compared to other models. One possible interpretation\nof these results is that when there are weaker associations between\ntext and image tokens, not only does the model produce an invalid\nresponse but, the response may ignore the image. This means that the\nmodel is relying on its pre-existing understanding of general concepts\nand patterns, rather than accurately processing the specific visual data\npresented in the chart. Zhang *et. al.* [61] demonstrated something\nanalogous to this when they conducted an experiment truncating image\ntokens, inducing the model to eventually indicate it could not respond.\nFurthering their approach could proactively detect when models rely\nmore on their prior knowledge.\n\n\nFig. 5: The <BOS> (Beginning of Sentence) token is often added by the\nmodels as part of processing the inputs. It behaves as a starting point\nfor a model to establish its context. In ChartGemma, the <BOS> token\nappears to focus on axes, text, and (faintly) aspects of the encodings\n(e.g., the shape of the line, points in the chart)\n\nFinally, an interesting observation was the behavior of ChartGemma\nwith the <BOS> (Beginning of Sentence) token, which is added by\nVLMs [6]. <BOS> signals the start of the sequence and can act as\na contextual grounding for where the model will begin to build its\nunderstanding. Interestingly, in Figure 5 this appears to be the chart axis,\nwhich is a logical place to begin reasoning about a data visualization.\nWe can observe the <BOS> token prioritizes different elements in\ndifferent chart types, such as labels of all relevant text or encoding\naspects of elements, such as the shapes of a line or preliminary trends.\n\n**Finding:** VLMs vary in how they balance textual and visual encoding information in charts. VLMs that are fined-tuned on chart QA\ntasks appear to balance these vision and text modalities better.\n\n5.1.3 Performance Across Chart Types & Error Analysis\n\nWhile we consider all open-source models, our analysis prioritizes\nChartGemma as it makes the clearest associations with visual encodings.\nTo conduct this analysis, we reviewed the mini-VLAT results for all\nmodels and made qualitative observations of their errors; future work\ncan explore robust quantitative evaluations (§6). For ChartGemma we\nalso explored the full 53-item VLAT from Nobre *et. al.* [41]. Across\nall models, we observed three primary types of errors relating to **Data**,\n**Encoding**, and **Reasoning** . These are not mutually exclusive, for\nexample, data errors are related to issues with model reasoning.\n\n\n-----\n\n**[A]: Data Errors** refer to inaccuracies in associating *Q* with *I*, hindering the extraction of correct values for a valid response.\n\n[A.1]: Look-up Errors concern instances where the model struggles\nto pinpoint the correct chart region. This was most apparent for Janus\nand LLaVA, which were capable of identifying text but often failed to\nmake appropriate associations with visual encodings. By comparison,\nChartGemma, even when incorrect, did identify the correct regions of\nthe image; for example, in the stacked bar chart (Figure 1); Q: “ *What is*\n*the cost of peanuts in Seoul?* ”), ChartGemma seems to correctly identify\nthe bar relating to Seoul and the part of the stack concerning peanuts.\nIn the example of the bubble chart (Q:“ *Which city’s metro system has*\n*the largest number of stations?* ”), it places the most importance on\nShanghai (the correct answer), but, returns Beijing instead.\n\n[A.2]: Extraction Errors pertains to problems retrieving accurate data\n~~va~~ l ~~u~~ e ~~s~~ ~~f~~ r ~~om~~ ~~the~~ ~~c~~ h ~~art~~ ~~images~~ . On ~~ce~~ ~~again~~, ~~in~~ ~~t~~ h ~~e~~ ~~s~~ t ~~a~~ c ~~ked~~ ~~*bar*~~ ~~chart~~\nexample, the model appears to prioritize the correct parts of the image\nbut ultimately does not produce the correct response. Another example\nis the stacked *area* chart, where the model accurately associates pixels\nrepresenting the total number of girls with the names Amelia, Isla, and\nOlivia in 2012. It also gave higher importance to the values associated\nwith Isla and Olivia, along with their corresponding positions on the\nx-axis. Still, the model generated an incorrect response, potentially due\nto difficulties in performing the ratio calculation.\n\n**[B]: Encoding Errors** pertains to difficulties interpreting the shapes,\ntrends, or patterns of visual encodings and relationships between marks.\n\n[B.1]: Encoding Interpretation involves understanding individual data\nmarks, such as associating a circle’s size in a bubble chart or a wedge’s\nshape in a pie chart to its data value (related also to Extraction Error\n\n[A.2]). This also includes interpreting pattern trends (e.g., decreasing line chart segments) or relationships between data marks (e.g.,\ncorrelations in a scatter plot). We observed that ChartGemma (and\noccasionally Janus) performed effectively with simpler, more common\nchart types that are regularly used in media and public reports (e.g.,\nline, bar, pie). They encountered greater difficulty with area encodings,\ncomplex spatial polygons (e.g., choropleth shape outlines), and charts\ninvolving relative baselines (e.g., stacked charts).\n\n[B.2]: Hierarchical Relationships concern specific issues involving\nmarks that are nested within each other. For example, in the tree map\n(Q:“True/False: eBay is nested in the Software category”), the model\nfocuses on category names (e.g., Software, Retail, and Computer) but\ncannot resolve the hierarchical relationship with ‘eBay’. While closedsource models performed better than ChartGemma here, it is uncertain\nwhether this reflects a deeper understanding of these relationships or\nis the result of memorization (§5.1.1). However, neither mini-VLAT\nnor VLAT extensively assesses hierarchical relationships, such as those\nalso found in phylogenetic trees or more complex composite encodings.\nThis is a potent area for future work.\n\n**[C]: Reasoning Errors** involve flaws in the model’s ability to draw\nconclusions or make deductions based on the chart’s elements.\n\n[C.1]: Multi-step Reasoning concerns the number of analytic steps\nrequired to answer a question. For example, a bar chart retrieval task\n(Q:‘ *‘What is the average internet speed in Japan?”* ) can require just\ntwo steps: identifying the relevant part of the chart and extracting the\nresult. Whereas, comparison or range tasks (e.g., *“About how much did*\n*the price of a barrel of oil rise from April to August in 2020?”* in the\nline chart) require multiple retrievals and arithmetic operations (e.g.,\nsubtraction). Multi-step operations remain a challenge for VLMs, even\nwith Chain-of-Thought prompting [29]. In Figure 9 we show that AS\nmaps can help us interrogate this challenge and make progress.\n\n[C.2]: Prompt Sensitivity concerns how the model’s responses and reasoning may change because of how the model is prompted. To explore\nthese types of errors we made modifications to the mini-VLAT questions in an *ad hoc* manner. We show in §5.2.3, Figure 7 and Figure 9,\nthat the model’s reasoning changes and impacts its responses.\n\n**Finding:** We observed three primary types of errors, **Data**, **Encod-**\n**ing**, and **Reasoning**, that can be interrogated with AS maps.\n\n\nFig. 6: The evaluation of reasoning over model layers. We use sigmoid\nnormalization ( *G* ( *·* ), §4.1.2) to show a fuller range of the model’s focus.\n\n**5.2** **RQ2: VLMs Reasoning Exhibit Higher-Order Reasoning**\n\nWe now explore how VLMs construct reasoning over multiple layers\n(§5.2.1), which is a precursor for examining their spatial (§5.2.2) and\nsemantic reasoning (§5.2.3). Here, we focus exclusively on ChartGemma and consider the 53-item version of VLAT reported by Nobre *et. al.* [41], which also includes annotations made by people that\nwe compare to with AS maps (§5.2.4).\n\n5.2.1 Building Reasoning Over Layers\n\nUp to this point, we have shown results from one or more tokens, but\nonly from a single layer. In Figure 6, we show how VLMs construct\ntheir reasoning over multiple layers (specifically, 9, 11, 13, and 15)\nfrom the query “What is the a pp roximate global smartphone market\nshare of Samsun g ? ”; we do not show more layers simply in the interest\nof space, but these can be explored in our companion application (§4.2).\nEach layer in Figure 6 is visualized individually without the contributions of prior layers (i.e., *start* = *end*, §4.1.2). Like prior work [61], we\nfind that earlier layers in the model demonstrate more activity toward\nspatial understanding of visual encodings, while later layers contribute\nless. Individual layers also prioritize different information and do not\njust extend prior layers. For example, Layer 9 in the token ‘approximate’ highlights the importance of the pie’s diameter (the other tokens\ndo as well, but less so), while in Layer 11 this focus is absent. Instead,\nthe model appears to prioritize text more. By Layer 13, the model\nseems to have localized the Samsung wedge. We generally observed\nthat, across all chart types, Layer 13 in ChartGemma shows the most\nconcrete examples of spatial reasoning that also relates to the model’s\nfinal answer. Another relevant observation is the relationships between\ntext and image tokens. For example, why does ‘approximate’ appear to\nemphasize the pie’s diameter and ‘?’ emphasize the diameter *and* the\nSamsung wedge? There isn’t a clear answer. For terms like ‘approximate’ or ‘average’, which imply some mathematical operation, we\nhave made a few observations that models, even Janus and LLaVA, attempt to ‘look’ at the broad spatial structure of the encoding. However,\nthese observations were too few and qualitative to make more definitive\nconclusions. Lastly, interpreting precise token contributions can be\ncomplex because attention mechanisms dynamically assign weights\nthat allow the model to focus on the most relevant information. The\n‘?’ token, which appears toward the end of the query, potentially receives more aggregated information from other tokens, leading to a\ncontextually richer visualization of the response within a layer.\n\n\n-----\n\n5.2.2 VLMs exhibit Spatial Reasoning\n\n\n\n**Rising** **Falling**\n\n\n\n5.2.3 VLMs Integrate Spatial and Semantic Reasoning\n\n***We define semantic reasoning with data visualizations as the ability to***\n***understand and infer relationships between elements of*** *Q* ***and*** *I* ***.*** We\nhave already shown (Figure 4 and 6), that models, but ChartGemma\nespecially, can identify pixels in the image that correspond to words\nin the query. In Figure 1 there are more complex demonstrations\nof semantic and spatial understanding as well. In the scatter plot,\n~~t~~ h ~~er~~ e d ~~oe~~ s ~~no~~ t ~~exi~~ st a ~~ny~~ ~~explic~~ i ~~t~~ ~~l~~ i ~~n~~ e ~~that~~ su ~~mmarize~~ s ~~the~~ ~~r~~ e ~~lationship~~\nbetween the two axes (height and weight). It was fascinating to observe\nChartGemma appear to draw such a line and then correctly interpret its\ndirection (upward) and that this represented a positive correlation. Here,\nwe explore the relationship between semantics and spatial reasoning\ncapabilities a little further. In Figure 9 we show examples of queries,\ntaken from the mini-VLAT, that ask whether some trend is increasing\nor decreasing. We can observe that across three chart types (Line, Area,\nand Stacked area) the model can make associations between ‘increasing’\nand the parts of the encoding that show an increasing trend and viceversa for ‘decreasing’. As we have discussed previously (§5.1.3) this\ncan vary by encoding type, with area charts being some of the more\nchallenging, but the model nonetheless finds increasing/decreasing\nsegments across all charts. Retaining all other words in the query, but\nreplacing increasing/decreasing with rising/falling, respectively, we\ncan see a change in the model’s reasoning. For ‘rising’ the model\ncontinues to find the upward trending regions of the charts (the area\nchart excepted), while for ‘falling’ the model appears to no longer\n~~make~~ ~~an~~ ~~ass~~ o ~~ciatio~~ n ~~with~~ ~~the~~ ~~downward~~ ~~tren~~ d ~~ing~~ regions. Bromely and\nSetlur [9] showed people ascribe diverse semantics to line chart trends,\ntheir results are pertinent in light of the VLM’s sensitivity to wording.\n\n**Finding:** VLMs fine-tuned on chart QA tasks demonstrate semantic\n~~rea~~ s ~~o~~ n ~~ing~~ ~~by~~ ~~i~~ d ~~e~~ n ~~ti~~ f ~~y~~ i ~~ng~~ ~~rela~~ t ~~ion~~ s ~~h~~ i ~~ps~~ ~~b~~ et ~~w~~ e ~~en~~ ~~key~~ ~~ele~~ m ~~ents~~ ~~of~~ *Q*\nand *I* and, with some limitations, making appropriate inferences by\n\nFinally, we explore the question of whether there is any alignment\nbetween what AS maps visualize and the reasoning of people. To do so\nwe gathered examples from a previously published study by Nobre *et.*\n*al.* [41]. In their study, they collected free-form sketches and openended text responses that captured the participants’ thought (reasoning)\nprocess when responding to a modified set of VLAT questions. We\ncompare the AS maps we generate to participants’ sketch annotations,\nonce again summarizing key observations.\n\n\n**Original Prompt** **Modifified Prompt**\n\nFig. 7: VLMs are sensitive to the language of prompts, not just in\ntheir responses, but internal reasoning. Here, we change the increasing/decreasing terms to rising/falling, which have similar meanings. Notably, the VLM exhibits more difficulty associating ‘falling’ specific areas\nof the chart, but shows a stronger association with ‘decreasing’.\n\nThere are some challenges in directly comparing AS maps to these\nannotations. First, as we have already discussed, model reasoning\nevolves over layers and tokens, yet, to compare to human annotations,\nwe had to select a representative AS map to compare against. Second,\nas we show in Figure 8 humans also have different annotation strategies\nand can also focus on the wrong things. It is worth noting the reference human, like the reference VLM, matters as the accuracy range\nreported in [41] on tasks varies from 0.47 to 0.94. As such, it is not\nstraightforward to treat these as ground-truth annotations to identify the\nmutual interest of the model and people [7]; our entire paper could be\ndevoted to this topic alone. From our observations, the AS maps from\nChartGemma do have reasonable alignment with regions of the chart\nthat people also focus on. When people responded correctly (regardless\nof how the model responded) we noted two divergent strategies. First,\npeople use the legend more consistently, while this is variable among\nVLMs. Sometimes, the AS maps show some evidence that the VLM\nattends to the legend (e.g., Figure 8-BUCQ3), but in other examples\n(e.g., the stacked area chart) it does not. *People appeared to attend to*\n*the legend more consistently than VLMs* . Second, for area encodings,\nwhich VLMs continue to struggle with, *people are better at tracing the*\n*shape of a mark* (e.g., tracing the outline of an area in the tree map).\n\n\n**ACQ4**\n\n\nOver the fifirst six months of 2018, the price of a pound of\ncoffee beans was roughly decreasing or increasing?\n\n\n\n**BUCQ3**\n\n\nTrue or False: Tokyo has a bigger ridership than Guangzhow.\n\n\nFig. 8: Comparing our results with human annotations from [41]\n\n\n-----\n\nHumans look for\n\nthe differences\n\nbetween two points\n\nThe model can\n\nstruggle with multistep reasoning and\nfail to extract\n\nrelevant data points\n\nDescribing each\nstep to the model\nimproves its results\nand its internal\n\nreasoning to align\nmore with people\n\nFig. 9: Adding step-by-step instructions impacts reasoning & responses\n\nWe can explore the synergy between people and models further.\nIn Figure 9 we show that humans distinctly annotate the two points necessary to answer the question. We could translate their interactions into\nsteps as instructions to the model. Experimenting with this approach,\nwe modified the original question prompt to provide step-by-step instructions (“ *About how much did the price of a barrel of oil rise from*\n*April to August in 2020? Steps: First, extract the price in April. Then,*\n*extract the value of August. Finally, subtract and get results.* ”). As a\nresult, not only does the model put more importance on the April *and*\nAugust data points, aligning more with people, but it returns the correct\nresponse. Extending this experiment further can lead to novel avenues\nfor model tuning with human guidance.\n\n**Finding:** Our preliminary findings suggest VLMs and people can\nidentify common regions of importance when reasoning with charts.\n\n**6** **D** **ISCUSSION**\n\nVisualization literacy requires reasoning over visual encodings to identify key elements (e.g., points, trends, patterns) and draw meaningful\nconclusions when answering questions. Prior research that explores\nthese capabilities in VLMs only interrogates the model’s behaviors\nand overall performance, limiting our understanding of their internal\nreasoning. We extended AG-CAM for ChartQA tasks to probe the\nvisualization literacy of VLMs more deeply. We demonstrate that this\napproach reveals that VLMs possess spatial and semantic reasoning\nabilities that even appear to align with aspects of human reasoning.\nHowever, VLMs are not perfect and can struggle with complex encoding shapes (namely areas) and questions requiring multi-step reasoning.\nThese limitations can be improved with further model tuning, including\nleveraging guidance from people. We now reflect on our findings.\n\n**6.1** **The Good, the Bad, and the Ugly**\n\nDespite generally optimistic results concerning VLMs’ visualization\nliteracy, there are still areas for improvement, as reflected in our title.\n\n**The Good: the Power of Small Open-Source Models.** A key difference between our work and prior research into visualization literacy is\nthat we prioritized open-source models. This was not difficult as ChartGemma was nearly as performant as closed source counterparts, with\nthe added benefit of being more transparent and, given the availability\nof weights, capable of generating reproducible results. ChartGemma is\nalso small enough to be run on a performant gaming laptop. Overall,\nthe architecture of early fusion models, which include ChartGemma,\nreduces their computational overhead relative to their deep fusion counterparts, making it easier for visualization researchers to experiment\nand extend its capabilities or propose alternative models. Moreover, we\nmay not need to prioritize large general-purpose LLMs or VLMs, but\ncould create specialized small models for visualization tasks. Given\ninequities in AI research [16], it is welcome news that it is not necessary\nto have a fistful of dollars to make progress. Beyond this, if we have\nmore transparency toward the model’s training data, we do not need to\nconduct overfitting arms races to define new VLAT questions [23].\n\n\n**The Bad: Visualization Literacy is not Uniform Across VLMs.**\nIn our research, and in keeping with prior work [5, 23, 34, 43], we\n\nresearch, we found VLMs specifically finetuned on ChartQA to be\nmore performant than those that are not and are of equivalent size. It\n\nThe good news is that our results suggest that producing specialized\nmodels for visualization tasks is possible and viable.\n\nmodels create their reasoning across layers and tokens, it can be difficult\nto point to one singular representation of their reasoning. However,\nwe believe this opens up exciting and potent avenues for future work.\nAS maps are another lens that can be used to probe the capabilities of\nthese models, along with performance and other new approaches. For\nexample, we can ask models to also explain themselves as Nobre *et.*\n*al.* [41] prompted people to do. Taking multiple lenses sourced from\nthe models’ behavior (e.g., explaining itself, accuracy) with its internal\nreasoning (e.g., AS maps) will help us create a more complete picture.\nComplicating matters further is that, even in AI research writ large,\ndefining and especially quantifying ‘reasoning’ remains an open problem. We provide specific examples of spatial and semantic reasoning,\nbut our results are subject to the limitations of current understanding in\nthe field of AI more broadly. Thus, future work examining the reasoning of both people and AI models requires a measured consideration\nthat attends to the idiosyncrasies of both. Even when AIs and people\nprioritize common information, it should not be necessary for VLMs to\nentirely replicate human processes, but, rather be guided by them.\n\n**6.2** **Limitations and Future Work**\n\nWhile this study offers valuable insights into VLM visualization literacy\nthrough the application of AG-CAM, it is important to acknowledge\nseveral avenues for future exploration. First, while our interpretation of\nattention saliency maps revealed compelling alignments between model\nresponses and internal reasoning, particularly in fine-tuned models, we\nreport primarily qualitative observations. As our goal was to probe and\nexplore VLMs, we felt this was appropriate. Moreover, the challenge\nof quantifying reasoning is too broad to resolve in a single manuscript.\nHowever, throughout our work, we provided suggestions for future\ndirections stemming from our observations, including experiments\nthat could yield more quantitative insights. Second, the VLAT test\nsuite, while useful for assessing baseline visualization literacy, may not\nfully capture the nuances of real-world chart understanding. Future\nresearch could incorporate more complex, real-world chart scenarios\nand evaluation metrics, leading to more ecologically valid results.\n\n**7** **C** **ONCLUSION**\n\nWe examine the visualization literacy of VLMs using an adaptation of\nAG-CAM that we developed. Our results show the variations among\nVLM internal reasoning, with models fine-tuned on chart questionanswering exhibiting the strongest overall performance. We also show\nthat VLM reasoning can align with human reasoning, opening avenues\nfor leveraging human guidance. Collectively, our findings demonstrate\nthe value and viability of interrogating model reasoning, particularly by\nusing small, open-source models. As VLMs continue to be adopted for\nreal-world applications, our approach provides a valuable tool for the\ntransparent and effective investigation of their visualization literacy.\n\n\n-----\n\n**A** **CKNOWLEDGMENTS**\n\nWe wish to thank the members of UW Insight Lab, F. Feng, X. Yu,\nand V. Bector for their feedback. We also wish to thank F. Shi for the\ndiscussion and suggestions. L. Dong is supported by a Cheriton School\nof Computer Science Undergraduate Research Fellowship.\n\n**R** **EFERENCES**\n\n[1] Openai: Gpt-4 technical report, 2024.\n\n[2] M. Abdin, J. Aneja, H. Awadalla, A. Awadallah, A. A. Awan, N. Bach,\nA. Bahree, A. Bakhtiari, and *et al.* Phi-3 technical report: A highly capable\nlanguage model locally on your phone, 2024.\n\n[3] S. Abnar and W. Zuidema. Quantifying attention flow in transformers. In\nD. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, eds., *Proc. ACL’20*, pp.\n[4190–4197, 2020. doi: 10.18653/v1/2020.acl-main.385](https://doi.org/10.18653/v1/2020.acl-main.385)\n\n[4] J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt, and B. Kim.\nSanity checks for saliency maps. In *Proc. NeurIPs’18*, 12 pages, p.\n9525–9536, 2018.\n\n[5] A. Bendeck and J. Stasko. An empirical evaluation of the gpt-4 multimodal\nlanguage model on visualization literacy tasks. *IEEE TVCG*, 31(1):1105–\n[1115, 2025. doi: 10.1109/TVCG.2024.3456155](https://doi.org/10.1109/TVCG.2024.3456155)\n\n[6] L. Beyer, A. Steiner, A. S. Pinto, A. Kolesnikov, X. Wang, D. Salz,\nM. Neumann, I. Alabdulmohsin, M. Tschannen, E. Bugliarello, and *et. al.*\nPaligemma: A versatile 3b vlm for transfer. *arXiv*, 2024.\n\n[7] A. Boggust, B. Hoover, A. Satyanarayan, and H. Strobelt. Shared interest:\nMeasuring human-ai alignment to identify recurring patterns in model\nbehavior. In *Proc. CHI’22* [, article no. 10, 17 pages, 2022. doi: 10.1145/](https://doi.org/10.1145/3491102.3501965)\n[3491102.3501965](https://doi.org/10.1145/3491102.3501965)\n\n[8] J. Boy, R. A. Rensink, E. Bertini, and J.-D. Fekete. A principled way of\nassessing visualization literacy. *IEEE TVCG*, 20(12):1963–1972, 2014.\n[doi: 10.1109/TVCG.2014.2346984](https://doi.org/10.1109/TVCG.2014.2346984)\n\n[9] D. Bromley and V. Setlur. What is the difference between a mountain and\na molehill? quantifying semantic labeling of visual features in line charts.\nIn *Proc IEEE VIS’23* [, pp. 161–165, 2023. doi: 10.1109/VIS54172.2023.](https://doi.org/10.1109/VIS54172.2023.00041)\n[00041](https://doi.org/10.1109/VIS54172.2023.00041)\n\n[10] K. Börner, A. Maltese, R. N. Balliet, and J. Heimlich. Investigating aspects\nof data visualization literacy using 20 information visualizations and 273\nscience museum visitors. *Information Visualization*, 15(3):198–213, 2016.\n[doi: 10.1177/1473871615594652](https://doi.org/10.1177/1473871615594652)\n\n[11] H. Chefer, S. Gur, and L. Wolf. Transformer interpretability beyond\nattention visualization. In *Proc. IEEE CVPR’21*, pp. 782–791, 2021.\n\n[12] L. Chen and G. Varoquaux. What is the role of small models in the llm\nera: A survey. *arXiv*, 2024.\n\n[13] X. Chen, Z. Wu, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, and C. Ruan.\nJanus-pro: Unified multimodal understanding and generation with data\nand model scaling. *arXiv preprint arXiv:2501.17811*, 2025.\n\n[14] Z. Chen, C. Zhang, Q. Wang, J. Troidl, S. Warchol, J. Beyer, N. Gehlenborg, and H. Pfister. Beyond generating code: Evaluating gpt on a data\nvisualization course. In *Proc. IEEE EduVis)* [, pp. 16–21, 2023. doi: 10.](https://doi.org/10.1109/EduVis60792.2023.00009)\n[1109/EduVis60792.2023.00009](https://doi.org/10.1109/EduVis60792.2023.00009)\n\n[15] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng,\nS. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023.\n\n[16] A. Crisan. We don’t know how to assess llm contributions in vis/hci. In\n*Proc. IEEE BELIV’24’* [, pp. 115–118, 2024. doi: 10.1109/BELIV64461.](https://doi.org/10.1109/BELIV64461.2024.00018)\n[2024.00018](https://doi.org/10.1109/BELIV64461.2024.00018)\n\n[17] DeepSeek-AI. Deepseek-v3 technical report. *arXiv*, 2024.\n\n[18] Y. Du, Z. Liu, J. Li, and W. X. Zhao. A survey of vision-language\npre-trained models. In *Proc. IJCAI’22* [, pp. 5436–5443, 2022. doi: 10.](https://doi.org/10.24963/ijcai.2022/762)\n[24963/ijcai.2022/762](https://doi.org/10.24963/ijcai.2022/762)\n\n[19] Google. Gemini 2.0, 2024.\n\n[20] D. Haehn, J. Tompkin, and H. Pfister. Evaluating ‘graphical perception’\nwith cnns. *IEEE TVCG* [, 25(1):641–650, 2019. doi: 10.1109/TVCG.2018.](https://doi.org/10.1109/TVCG.2018.2865138)\n[2865138](https://doi.org/10.1109/TVCG.2018.2865138)\n\n[21] Y. Han, C. Zhang, X. Chen, X. Yang, Z. Wang, G. Yu, B. Fu, and H. Zhang.\nChartllama: A multimodal llm for chart understanding and generation.\n*arXiv*, 2023.\n\n[22] M. Hedayati and M. Kay. What university students learn in visualization\nclasses. *IEEETVCG* [, 31(1):1072–1082, 2025. doi: 10.1109/TVCG.2024.](https://doi.org/10.1109/TVCG.2024.3456291)\n[3456291](https://doi.org/10.1109/TVCG.2024.3456291)\n\n[23] J. Hong, C. Seto, A. Fan, and R. Maciejewski. Do llms have visualization\nliteracy? an evaluation on modified visualizations to test generalization\n\n\nin data interpretation. *IEEE TVCG* [, pp. 1–13, 2025. doi: 10.1109/TVCG.](https://doi.org/10.1109/TVCG.2025.3536358)\n[2025.3536358](https://doi.org/10.1109/TVCG.2025.3536358)\n\n[24] E. Hoque, P. Kavehzadeh, and A. Masry. Chart question answering: State\nof the art and future directions. *Computer Graphics Forum*, 41(3):555–572,\n[2022. doi: 10.1111/cgf.14573](https://doi.org/10.1111/cgf.14573)\n\n[25] K.-H. Huang, H. P. Chan, Y. R. Fung, H. Qiu, M. Zhou, S. Joty, S.-F.\nChang, and H. Ji. From pixels to insights: A survey on automatic chart\nunderstanding in the era of large foundation models. *IEEE TKDE*, 2024.\n\n[26] M. S. Islam, R. Rahman, A. Masry, M. T. R. Laskar, M. T. Nayeem,\nand E. Hoque. Are large vision language models up to the challenge of\nchart comprehension and reasoning. In *Proc. EMNLP(Findings)’24*, pp.\n3334–3368, 2024.\n\n[27] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,\nS. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural\nlanguage models. *arXiv*, 2020.\n\n[28] N. W. Kim, G. Myers, and B. Bach. How good is chatgpt in giving advice\non your visualization design? *arXiv*, 2024.\n\n[29] T. Lanham, A. Chen, A. Radhakrishnan, B. Steiner, C. Denison, D. Hernandez, and *et. al.* Measuring faithfulness in chain-of-thought reasoning.\n*arXiv*, 2023.\n\n[30] H. Laurençon, A. Marafioti, V. Sanh, and L. Tronchon. Building and better\nunderstanding vision-language models: insights and future directions.\n*arXiv*, 2024.\n\n[31] S. Lee, S.-H. Kim, and B. C. Kwon. Vlat: Development of a visualization\nliteracy assessment test. *IEEE TVCG* [, 23(1):551–560, 2017. doi: 10.](https://doi.org/10.1109/TVCG.2016.2598920)\n[1109/TVCG.2016.2598920](https://doi.org/10.1109/TVCG.2016.2598920)\n\n[32] S. Lee, B. C. Kwon, J. Yang, B. C. Lee, and S.-H. Kim. The correlation\nbetween users’ cognitive characteristics and visualization literacy. *Applied*\n*Sciences* [, 9(3), 2019. doi: 10.3390/app9030488](https://doi.org/10.3390/app9030488)\n\n[33] S. Leem and H. Seo. Attention guided cam: Visual explanations of vision\ntransformer guided by self-attention. In *Proc. AAAI’24*, vol. 38, pp. 2956–\n2964, 2024.\n\n[34] Z. Li, H. Miao, V. Pascucci, and S. Liu. Visualization literacy of multimodal large language models: A comparative study. *arXiv*, 2024.\n\n[35] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. In A. Oh,\nT. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, eds.,\n*Proc. NeurIP’23*, vol. 36, pp. 34892–34916, 2023.\n\n[36] Y. Liu, Z. Li, M. Huang, B. Yang, W. Yu, C. Li, X.-C. Yin, C.-L. Liu,\nL. Jin, and X. Bai. Ocrbench: on the hidden mystery of ocr in large\nmultimodal models. *Science China Information Sciences*, 67(12), 2024.\n[doi: 10.1007/s11432-024-4235-6](https://doi.org/10.1007/s11432-024-4235-6)\n\n[37] L. Y.-H. Lo and H. Qu. How good (or bad) are llms at detecting misleading visualizations? *IEEE Transactions on Visualization and Computer*\n*Graphics* [, 31(1):1116–1125, 2025. doi: 10.1109/TVCG.2024.3456333](https://doi.org/10.1109/TVCG.2024.3456333)\n\n[38] P. Maddigan and T. Susnjak. Chat2vis: Generating data visualizations\nvia natural language using chatgpt, codex and gpt-3 large language models. *IEEE Access* [, 11:45181–45193, 2023. doi: 10.1109/ACCESS.2023.](https://doi.org/10.1109/ACCESS.2023.3274199)\n\n[3274199](https://doi.org/10.1109/ACCESS.2023.3274199)\n\n[39] A. Masry, M. Thakkar, A. Bajaj, A. Kartha, E. Hoque, and S. Joty. Chartgemma: Visual instruction-tuning for chart reasoning in the wild. *arXiv*\n*preprint arXiv:2407.04172*, 2024.\n\n[40] F. Meng, W. Shao, Q. Lu, P. Gao, K. Zhang, Y. Qiao, and P. Luo. Chartassisstant: A universal chart multimodal language model via chart-to-table\npre-training and multitask instruction tuning, 2024.\n\n[41] C. Nobre, K. Zhu, E. Mörth, H. Pfister, and J. Beyer. Reading between the\npixels: Investigating the barriers to visualization literacy. In *Proc. CHI’24*,\n[article no. 197, 17 pages, 2024. doi: 10.1145/3613904.3642760](https://doi.org/10.1145/3613904.3642760)\n\n[42] S. Pandey and A. Ottley. Mini-vlat: A short and effective measure of\nvisualization literacy. *Computer Graphics Forum* [, 42(3):1–11, 2023. doi:](https://doi.org/10.1111/cgf.14809)\n[10.1111/cgf.14809](https://doi.org/10.1111/cgf.14809)\n\n[43] S. Pandey and A. Ottley. Benchmarking visual language models on standardized visualization literacy tests. *arXiv*, 2025.\n\n[44] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever.\nLearning transferable visual models from natural language supervision. In\n*Proc. ICML’21*, vol. 139, pp. 8748–8763, 2021.\n\n[45] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical\ntext-conditional image generation with clip latents, 2022.\n\n[46] C. Saharia, W. Chan, S. Saxena, L. Lit, J. Whang, E. Denton, S. K. S.\nGhasemipour, B. K. Ayan, S. S. Mahdavi, R. Gontijo-Lopes, T. Salimans,\nJ. Ho, D. J. Fleet, and M. Norouzi. Photorealistic text-to-image diffusion\nmodels with deep language understanding. In *Proc. NeurIPs’22*, article\nno. 2643, 16 pages, 2022.\n\n\n-----\n\n[47] A. Saske, T. Möller, L. Koesten, J. Staudner, and S. Kritzinger. MAVIL:\nDesign of a multidimensional assessment of visual data literacy and its\napplication in a representative survey. *arXiv*, 2024.\n\n[48] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and\nD. Batra. Grad-cam: Visual explanations from deep networks via gradientbased localization. In *Proc. IEEE ICCV’17* [, pp. 618–626, 2017. doi: 10.](https://doi.org/10.1109/ICCV.2017.74)\n[1109/ICCV.2017.74](https://doi.org/10.1109/ICCV.2017.74)\n\n[49] N. Sultanum and A. Srinivasan. Datatales: Investigating the use of large\nlanguage models for authoring data-driven articles. In *Proc. IEEE VIS’23’*,\n[pp. 231–235, 2023. doi: 10.1109/VIS54172.2023.00055](https://doi.org/10.1109/VIS54172.2023.00055)\n\n[50] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin. Attention is all you need. In *Proc.*\n*NeurIPS’17*, vol. 30, 2017.\n\n[51] J. Vig. A multiscale visualization of attention in the transformer model.\nIn *Proc. ACL’19* [, pp. 37–42, 2019. doi: 10.18653/v1/P19-3007](https://doi.org/10.18653/v1/P19-3007)\n\n[52] E. Voita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov. Analyzing\nmulti-head self-attention: Specialized heads do the heavy lifting, the rest\ncan be pruned. In A. Korhonen, D. Traum, and L. Màrquez, eds., *Proc.*\n*ACL’19* [, pp. 5797–5808, 2019. doi: 10.18653/v1/P19-1580](https://doi.org/10.18653/v1/P19-1580)\n\n[53] S. N. Wadekar, A. Chaurasia, A. Chadha, and E. Culurciello. The evolution\nof multimodal model architectures. *arXiv*, 2024.\n\n[54] Z. J. Wang, R. Turko, and D. H. Chau. Dodrio: Exploring transformer\nmodels with interactive visualization. In H. Ji, J. C. Park, and R. Xia, eds.,\n*Proc. ACL (Demos)* [, pp. 132–141, 2021. doi: 10.18653/v1/2021.acl-demo.](https://doi.org/10.18653/v1/2021.acl-demo.16)\n[16](https://doi.org/10.18653/v1/2021.acl-demo.16)\n\n[55] C. Ware, M. Stone, and D. A. Szafir. Rainbow Colormaps Are Not All Bad\n. *IEEE CG&A* [, 43(03):88–93, 2023. doi: 10.1109/MCG.2023.3246111](https://doi.org/10.1109/MCG.2023.3246111)\n\n[56] L. Weng, X. Wang, J. Lu, Y. Feng, Y. Liu, H. Feng, D. Huang, and W. Chen.\nInsightlens: Augmenting llm-powered data analysis with interactive insight management and navigation. *arXiv*, 2024.\n\n[57] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang, B. Cui,\nand M.-H. Yang. Diffusion models: A comprehensive survey of methods\nand applications. *ACM Comput. Surv.*, 56(4), article no. 105, 39 pages,\n[2023. doi: 10.1145/3626235](https://doi.org/10.1145/3626235)\n\n[58] Y. Ye, J. Hao, Y. Hou, Z. Wang, S. Xiao, Y. Luo, and W. Zeng. Generative ai for visualization: State of the art and future directions. *Visual*\n\n*Informatics* [, 8(2):43–66, 2024. doi: j.visinf.2024.04.003](https://doi.org/j.visinf.2024.04.003)\n\n[59] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen. A survey on\nmultimodal large language models. *National Science Review*, 11(12), 11\n[2024. doi: 10.1093/nsr/nwae403](https://doi.org/10.1093/nsr/nwae403)\n\n[60] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid Loss for\nLanguage Image Pre-Training . In *Proc. IEEE ICCV’23’*, pp. 11941–\n[11952, 2023. doi: 10.1109/ICCV51070.2023.01100](https://doi.org/10.1109/ICCV51070.2023.01100)\n\n[61] X. Zhang, Y. Quan, C. Shen, X. Yuan, S. Yan, L. Xie, W. Wang, C. Gu,\nH. Tang, and J. Ye. From redundancy to relevance: Enhancing explainability in multimodal large language models. *Proc. NAACL’25*, 2025.\n\n\n-----",
  "1 Biometrics and Data Pattern Analytics Lab, Universidad Autonoma de Madrid, Madrid, Spain\n2 IMDEA Food, CEI UAM+CSIC, Madrid, Spain",
  "*Automatic dietary assessment based on food images re-*\n*mains a challenge, requiring precise food detection, seg-*\n*mentation, and classification.* *Vision-Language Models*\n*(VLMs) offer new possibilities by integrating visual and*\n*textual reasoning.* *In this study, we evaluate six state-*\n*of-the-art VLMs (ChatGPT, Gemini, Claude, Moondream,*\n*DeepSeek, and LLaVA), analyzing their capabilities in*\n*food recognition at different levels.* *For the experimen-*\n*tal framework, we introduce the FoodNExTDB, a unique*\n*food image database that contains 9,263 expert-labeled*\n*images across 10 categories (e.g., “protein source”), 62*\n*subcategories (e.g., “poultry”), and 9 cooking styles (e.g.,*\n*“grilled”).* *In total, FoodNExTDB includes 50k nutri-*\n*tional labels generated by seven experts who manually an-*\n*notated all images in the database.* *Also, we propose a*\n*novel evaluation metric, Expert-Weighted Recall (EWR),*\n*that accounts for the inter-annotator variability. Results*\n*show that closed-source models outperform open-source*\n*ones, achieving over 90% EWR in recognizing food prod-*\n*ucts in images containing a single product. Despite their*\n*potential, current VLMs face challenges in fine-grained*\n*food recognition, particularly in distinguishing subtle dif-*\n*ferences in cooking styles and visually similar food items,*\n*which limits their reliability for automatic dietary assess-*\n*ment. The FoodNExTDB database is publicly available at*\n*[https://github.com/AI4Food/FoodNExtDB.](https://github.com/AI4Food/FoodNExtDB)*",
  "Food is a fundamental source of energy for human life and\nplays a critical role in preventing chronic diseases. Maintaining a healthy diet has become increasingly challenging\ndue to multiple factors, including food quality, dietary diversity, cooking styles, and nutrient absorption. An imbalance in these aspects can lead to suboptimal nutrition,\n\n\nnegatively impacting health. In recent decades, diets have\nshifted toward increased consumption of processed, highcalorie foods and reduced intake of fruits and vegetables,\ncontributing to the rise of diet-related diseases [7, 13].\n\nTo counteract these trends, dietary guidelines such as the\nMediterranean and Japanese diets have been widely promoted for their health benefits and balanced nutritional profiles [45, 52]. However, these guidelines provide general\nrecommendations and require individualized monitoring to\nassess effectiveness. Traditional dietary assessment methods, such as 24-hour recall and food frequency questionnaires, often fail due to reliance on self-reporting, which\ncan be tedious and prone to inaccuracies. Advancements in\ndietary assessment suggest that automatic and personalized\nnutrition may offer more effective solutions [20, 49].\n\nFood computing has emerged as a rapidly growing field,\napplying computational techniques to various food-related\ndomains [29]. The increasing digitalization of daily life has\nled to the availability of vast amounts of online food data,\nenabling the development of large-scale food datasets [37].\nMoreover, advancements in artificial intelligence (AI) have\nfacilitated the evaluation of nutritional content and the un\nderstanding of individual dietary habits, contributing to both\nhealth maintenance and disease prevention [40, 44]. machine learning (ML), particularly deep learning (DL), has\ndemonstrated remarkable capabilities in food image recognition and classification tasks [23, 41]. However, these\nmethods primarily focus on visual classification and struggle to extract contextual information, such as ingredient\ncomposition, preparation techniques, etc.\n\nTo address these limitations, foundation models have\ndemonstrated impressive performance across a wide range\nof multimodal tasks [1, 3, 11]. Models such as ChatGPT\nand Gemini have achieved state-of-the-art results in applications such as food tracking, dietary recommendation, and\nfood science research [21, 24, 34, 35, 49, 51]. More recently, Vision-Language Models (VLMs) have emerged as\n\n\n-----\n\n**A) Food Nutrition Expert-Tagged Database (FoodNExTDB)** **B) Food Product Detection & Recognition**\n\n\n\n\n\n\n\n\n\n\n\n**Models (VLMs)**\n\n\n\n\n\nFigure 1. Overview of the proposed framework. (A) The FoodNExTDB consists of 9,263 food images labeled by nutrition experts across\n10 food categories, 62 subcategories, and 9 cooking styles, with approximately 50,000 assigned labels. (B) The experimental setup, where\nsix Vision-Language Models (VLMs) process food images using a structured prompt and generate predictions. These predictions are then\ncompared against expert annotations to assess model performance in food product detection and recognition.\n\n\nthe next generation of explainable computer vision systems,\nintegrating textual and visual data for a more comprehensive understanding of food-related information [18, 19, 43].\nDespite their potential, several challenges still remain such\nas the reliability, interpretability, and accuracy of these\nmodels for food-related tasks [5]. This raises fundamental questions about their practical application in real-world\nnutritional assessments: *Are VLMs ready to assist nutrition*\n*experts in critical tasks such as supervised dietary qual-*\n*ity assessment? Can they accurately recognize food prod-*\n*ucts based on subtle factors like cooking styles, which can*\n*significantly impact nutritional values?* *Can current AI-*\n*based tools reliably evaluate an individual’s dietary behav-*\n*ior solely through image uploads?*\n\nAdvancing knowledge around these questions, this paper has the following contributions: *i)* we introduce the\nFood Nutrition Expert-Tagged Database (FoodNExTDB),\na unique food image database that contains 9,263 expertlabeled images, containing real dietary records from individuals who participated in a weight loss intervention [39],\n*ii)* we propose an experimental framework to assess stateof-the-art VLMs, analyzing their capabilities in food recognition at different levels and prompt comprehension, and\n*iii)* we design a novel evaluation metric, Expert-Weighted\nRecall (EWR), that accounts for inter-annotator variability. Differences in nutritional paradigms, dietary guidelines,\nand individual professional experiences can lead to inconsistencies in labeling the same food item. Also, cultural\nand regional differences further contribute to variations in\nannotation. The FoodNExTDB includes annotations from\n\nseven different nutrition experts (50,000 labels), representing a valuable contribution for the research community. Figure 1 illustrates the overall framework of the present study.\n\nThe remainder of this paper is structured as follows: Sec.\n\n\n2 reviews key food image databases and recognition systems. Sec. 3 details the proposed FoodNExTDB. Sec. 4\npresents the VLM models used in this study, while Sec.\n5 outlines the experimental protocol and proposed metric.\nSec. 6 analyzes the experimental results, followed by discussions and conclusions in Sec. 7 and 8, respectively.",
  "Food image databases are essential in training and evaluating automatic recognition models. These databases\nare typically acquired through three main methods: *i)*\n*self-collected*, where food images are manually captured\nin controlled or semi-controlled environments, *ii) web-*\n*scraped*, where large-scale databases are gathered from online platforms, and *iii) combination*, where multiple existing databases are merged to enhance diversity and coverage.\nFirst, self-collected databases provide high-quality images but are limited in size due to manual acquisition. For\ninstance, the UNIMIB2016 database includes around 20K\nimages of various food products [10], while Nutrition5k offers 5K unique dishes with videos and nutritional data [47].\nIn contrast, web scraping allows for the collection of\nlarge-scale food image databases from social media and online sources. Examples include Food-101 [6] with 101,000\nimages spanning 101 food categories, VireFood-172, with\n172 categories [8], and ISIA Food-500 [30], which covers\n500 different food types.\nCombination databases merge multiple sources to create\nmore diverse and comprehensive databases. For example,\nFood-11 [46] integrates data from Food-101, UECFOOD100 [27], and UECFOOD-256 [16], grouping food items\ninto 11 main categories. MAFood-121 [2] focuses on global\n\n\n-----\n\ncuisines, featuring 121 food products across 21,000 images. AI4FoodNutritionDB introduces a structured nutritional taxonomy, consisting of 553,000 images categorized\ninto different nutritional levels, main categories, subcategories, and final food products [41]. Finally, Food-500\nCap enhances traditional food databases by incorporating\n24,700 images with detailed captions that describe finegrained visual attributes [26].",
  "Food image recognition has evolved significantly, transitioning from traditional classification techniques to DL approaches. Early methods struggled with complex and diverse food datasets, leading to the adoption of DL architectures such as convolutional neural networks (CNNs).\nThese models integrated feature extraction and classification, achieving high accuracy rates exceeding 80% [28, 31].\nOutstanding models include Squeeze-and-Excitation Networks (SENet) [14], Stacked Global-Local Attention Networks (SGLANet), and Progressive Region Enhancement\nNetworks (PRENet), which demonstrated strong generalization across multiple food databases [30, 31]. Transfer\nlearning and ensemble techniques further enhanced food\nclassification performance, while object detection frameworks such as LOng-tailed FIne-Grained Network (LOFI)\nand YOLO improved food recognition [7, 23, 38].\nThe introduction of vision transformers (ViTs) marked\na shift in food recognition by capturing global dependencies within images, enhancing classification accuracy [44].\nHowever, ViTs alone required significant computational resources and struggled with fine-grained classification due to\nhigh intra-class variations in food appearance. As a result,\nhybrid architectures combining CNNs and ViTs emerged,\nleveraging the spatial awareness of CNNs and the contextual understanding of transformers [33].\nIn the present study, we explore the application of recent VLMs. Models such as CLIP align images and text\nin a shared multimodal space, enabling zero-shot classification and improved fine-grained food differentiation\n\n[25, 36]. Large multimodal models like FoodLLM and\nLarge Language and Vision Assistants (LLaVA), specifically LLaVA-Chef, extend this capability by incorporating\ndomain-specific knowledge about food ingredients, preparation methods, and cultural contexts. These models leverage multimodal prompting, combining textual and visual inputs to enhance classification accuracy, making them effective at extracting contextual information [17, 32, 50].",
  "The FoodNExTDB is a food image database derived\nfrom from the AI4FoodDB, a comprehensive multimodal\ndatabase acquired from a one-month randomized controlled\ntrial (RCT) with 100 overweight and obese participants un\n\ndergoing a nutritional intervention [39]. Figure 1A) summarizes its key features, including food images collected over\n14 days per participant. With many food products reflecting\ncharacteristics of Spanish cuisine, this database provides a\nvaluable resource for studying food intake within a dietary\npattern. The database is publicly available on GitHub [1] .",
  "Participants were instructed to capture images of all consumed foods and beverages using their smartphones. A\ntotal of 10,739 images were collected, of which approximately 14% were discarded during post-processing (e.g.,\ndue to non-food images, blurred images, etc.), resulting in a\nfinal database of 9,263 food images. Notably, around 88%\nof the images contain a valid timestamp, a key parameter\nfor analyzing participants’ eating behaviors.\nFood images were annotated during the post-processing\nstage by a team of seven nutrition experts, ensuring that\neach image was reviewed by at least three annotators. This\napproach was implemented to enhance labeling reliability,\ngiven the varying complexity of food product recognition.\nWhile some food products are straightforward to classify,\nothers require expert judgment due to their ambiguity.\nThe annotation process involved identifying the food\nitems present in each image and categorizing them according to a predefined nutritional taxonomy. Each food product\nwas assigned a *category*, *subcategory*, and *cooking style* .\nThe taxonomy comprises 10 main food categories (e.g.,\n*“cereals and legumes”, “protein sources”*, etc.), 62 subcategories (e.g., *“alcoholic beverages”, “fruits”*, etc.), and 9\ncooking styles (e.g., *“fried”, “boiled or stewed”*, etc.). Additionally, free-text fields were provided to accommodate\nfood products that did not fit into the predefined classifications. The complete list of *category*, *subcategory*, and *cook-*\n*ing style* classes is available in the supplementary material.\nA custom Windows GUI was developed to streamline\nlabeling while ensuring consistency. The database uniquely\ncombines food categorization with culinary techniques, offering technical annotations and nutritional insights.",
  "On average, each participant captured approximately 96\nfood images, with a standard deviation of 58 images. Notably, *∼* 20% of participants took fewer than 50 images,\nwhile *∼* 15% captured more than 150. Regarding tempo\n*∼*\nral distribution, most images ( 79%) were taken during\n\n*∼*\nSpain’s main daily meals, 1,836 images ( 20%) at break\n*∼* *∼*\nfast, 2,540 ( 27%) at lunch, and 2,988 ( 32%) at dinner.\nIn total, nutrition experts assigned over 50,000 labels.\nThe three most frequently assigned food categories are\n\n*∼*\n*“vegetables and fruits”* ( 28%), *“cereals and legumes”*\n\n*∼* *∼*\n( 17%), and *“beverages”* ( 16%). At the subcategory\n\n1 [https://github.com/AI4Food/FoodNExtDB](https://github.com/AI4Food/FoodNExtDB)\n\n\n-----\n\n*∼* *∼*\nlevel, they are *“vegetables”* ( 13%), *“fruits”* ( 13%),\nand *“bread”* ( *∼* 8%). Finally, for cooking styles they\nare *“none”* ( *∼* 31%), *“fresh”* ( *∼* 28%), and *“boiled or*\n*steamed”* ( *∼* 10%), remarking the prevalence of raw foods.",
  "The analysis of the annotations from different nutrition experts revealed inconsistencies in the classification of certain food products and cooking styles, highlighting a lack\nof consensus in specific cases. This variability can significantly impact dietary assessments, as cooking styles influence the nutritional quality of a meal.\nAdditionally, in many cases, the cooking style could not\nbe reliably determined from the image alone, nor could certain food components like sauces or oils, which are often\nvisually indistinguishable. Moreover, complex dishes with\nmultiple food categories and subcategories posed a significant challenge, making labeling tedious and requiring expert judgment for consistency.",
  "In this study, six state-of-the-art VLMs are explored: ChatGPT (GPT-4o), Gemini 2.0 Flash, Claude 3.5 Sonnet,\nMoondream, DeepSeek Janus-Pro, and LlaVA. The primary\nobjective is to compare VLMs that are accessible through\nAPI on limited-resource environments.\n\nChatGPT, Gemini, and Claude are closed-source models\nwith dedicated APIs, while DeepSeek, LlaVA, and Moondream are open-source models, which in this study are deployed using Replicate [2], a platform offering efficient AI infrastructure to open-source VLMs through an API. To ensure unbiased results, each model instance is initialized independently for each image.\nSeveral parameters can be adjusted by the user in models\nthat support customization, including *temperature*, *top-p*,\n*max tokens*, and *seed* . The *temperature* parameter controls\nthe randomness of model outputs, where lower values result\nin more deterministic responses. The *top-p* parameter, also\nknown as nucleus sampling, defines the probability mass\nfor selecting the next token, balancing diversity and coherence. *Max tokens* determines the maximum length of the\ngenerated output, while *seed* ensures reproducibility across\ndifferent runs. For models that allow parameter tuning, we\nexperimentally set the following values: *temperature* = 0.2,\n*top-p* = 0.95, *max tokens* = 200, and *seed* = 42.\nWe describe next the key details of the selected VLMs:\n\n- **ChatGPT (GPT-4o)** : this is a multimodal model from\nOpenAI, capable of processing both text and image inputs while generating text-based outputs. It has been pretrained on a diverse dataset, including web content, pro\n2 [https://replicate.com/](https://replicate.com/)\n\n\nprietary data, and multimodal sources [15]. For this study,\nGPT-4o is accessed via OpenAI’s API using the openai\nPython library. It supports a 128,000-token context window and a maximum output of 16,384 tokens [3] .\n\n- **Gemini 2.0 Flash** : developed by Google, this is a highspeed multimodal model designed for diverse tasks, including text and image processing. It features nextgeneration capabilities such as native tool use and multimodal generation. The model has been pre-trained on\na diverse dataset, enabling strong vision-language reasoning [3, 12]. For this study, Gemini is accessed via\nGoogle’s API using the google Python library. It supports a context window of around 1M tokens and a maximum output of 8,192 tokens [4] .\n\n- **Claude 3.5 Sonnet** : developed by Anthropic, this is a\nstate-of-the-art model with multilingual, vision, and reasoning capabilities. It features a 200,000-token context window and a maximum output of 8,192 tokens [4].\nTrained on a proprietary dataset combining publicly available web data, third-party sources, and internally generated content, Claude employs a hybrid reasoning approach with strong performance in code generation, computational tasks, and extended context processing. For\nthis study, it is accessed via Anthropic’s API [5] .\n\n- **Moondream** : this is an open-source, lightweight VLM\ndesigned for efficient image analysis, object detection, visual reasoning, and scene comprehension. Moondream2B, in particular, is optimized for visual understanding\ntasks with a minimal computational footprint [6] . For this\nstudy, we use Moondream-2B via Replicate’s API [7] .\n\n- **DeepSeek Janus-Pro** : this is one of the first opensource models incorporating a vision module. It is built\nupon DeepSeek Janus [48], an autoregressive transformer\nframework designed for both multimodal understanding\nand generation. The core innovation lies in decoupling\nvisual encoding, enhancing the model’s ability to process\nand generate text from visual inputs [8] [9]. For this study,\nDeepSeek Janus-Pro is accessed via Replicate’s API [9] .\n\n- **LlaVA** : this is an end-to-end trained multimodal\n\nmodel that employs a fully-connected vision-language\nconnector [10] [22]. For this study, we use the\nLlaVA-v1.6-mistral-7b model via Replicate’s\nAPI [11] . Leveraging Mistral-7B, LlaVA enhances multimodal text generation and image-based reasoning while\nbalancing performance and computational cost.\n\n3 [https://chatgpt.com/](https://chatgpt.com/)\n4 [https://deepmind.google/](https://deepmind.google/)\n5 [https://claude.ai/](https://claude.ai/)\n6 [https://moondream.ai/](https://moondream.ai/)\n7 [https://replicate.com/lucataco/moondream2](https://replicate.com/lucataco/moondream2)\n8 [https://chat.deepseek.com/](https://chat.deepseek.com/)\n9 [https://replicate.com/deepseek-ai/janus-pro-1b](https://replicate.com/deepseek-ai/janus-pro-1b)\n10 [https://LlaVA-vl.github.io/](https://LlaVA-vl.github.io/)\n11 [https://replicate.com/yorickvp/LlaVA-v1.6-mistral-7b](https://replicate.com/yorickvp/LlaVA-v1.6-mistral-7b)\n\n\n-----",
  "To optimize the prompt for obtaining accurate and structured responses, several key aspects were considered. The\nprimary goal was explicitly defined: analyzing the image\nand identifying the foods present. Since many of the food\nproducts align with Mediterranean and Spanish dietary patterns, the final prompt enforces a standardized format, requiring each detected food item to be categorized into a predefined *category*, *subcategory*, and *cooking style* . To maintain consistency, it restricts responses to the given taxonomy, prevents additional explanations or assumptions, and\nensures outputs follow a strict, structured format.\nSome *categories* are *“cereals and legumes”, “vegeta-*\n*bles and fruits”, “protein sources”*, etc. Furthermore, *sub-*\n*categories* such as *“fruits”, “infusions”, and “pizza”*, and\n*cooking styles*, including *“fresh”, “fried”, “boiled” or*\n*“steamed”, and “oven-baked”*, ensure a detailed and standardized representation of food items.\nTo ensure consistency, foods not in the predefined list\nwere labeled as *“Others”*, while partially visible or distant items were excluded. Finally, for models that failed to\ngenerate structured responses (i.e., DeepSeek, LLaVA, and\nMoondream), we implemented a secondary post-processing\nstep using ChatGPT-4o. This step refined outputs by correcting formatting errors, separating merged elements, and\nclassifying food products into predefined *category*, *subcate-*\n*gory*, and *cooking style* . The complete prompts are available\nin the supplementary material.",
  "In order to systematically evaluate the performance of the\nsix selected VLMs with our proposed FoodNExTDB, we\ndesign an experimental protocol consisting of three tasks.\nAs indicated in Sec. 4.2, we insert to each VLM a structured\nprompt to analyze each image, identifying individual food\nproducts, and generating an appropriately formatted output.\nTo ensure consistency, manual post-processing was applied\nto fewer than 1% of the generated labels, addressing cases\nwhere models produced similar but non-exact matches (e.g.,\ngenerating food categories or cooking styles not included in\nthe predefined taxonomy). The total cost of all experiments\nwas $434.04, covering all API executions. We describe next\nthe three tasks analyzed in our experimental study.\n\n**5.1.1. Task 1: Food Image Recognition**\n\nThis task evaluates the ability of VLMs to recognize food\nproducts from images by classifying them into predefined\ncategories. The evaluation is conducted at three consecutive\nlevels, from simpler to more complex classifications:\n\n- *Category* : Recognizing general food groups, such as\n*“vegetables and fruits”* or *“protein sources”* .\n\n\n\n- *Category + Subcategory* : Providing finer-grained classification within each category, for example, distinguishing\nbetween *“vegetables”* and *“fruits”* subcategories.\n\n- *Category + Subcategory + Cooking Style* : Identifying\nboth the food *category* and *subcategory* and its preparation method, such as differentiating between *“grilled”*\nfish and *“fried”* fish.\n\n**5.1.2. Task 2: Fine-Grained Food Recognition**\n\nThe second task focuses on the model’s ability to recognize\nspecific food products correctly. Performance is analyzed\nacross different categorization levels to determine which\nfood products are easier or more challenging to recognize.\nThis task helps to assess whether VLMs can reliably differentiate between foods that look similar and correctly assign\nthem to their respective *categories* and *subcategories* .\n\n**5.1.3. Task 3: Image Complexity Performance**\n\nThe third task examines the impact of image complexity\non model performance. Two scenarios are evaluated, with\nimages containing only one identifiable food item ( *single-*\n*product* images), and images featuring multiple food products in a single image ( *multi-product* images).",
  "Our metric quantifies how well the VLMs match the annotations considering possible differences between annotators.\nIn image *i*, the annotators identify *p* *[i]* *j* [food products, with]\n*j* = 1 *, . . ., M* *[i]*, where *M* *[i]* is the total number of food products detected by the annotators. Let *L* *[i]* be the total number\nof product labels *l* *k* *[i]* [, with] *[ k]* [ = 1] *[, . . ., L]* *[i]* [, assigned by the]\nannotators (note that a single product can originate multiple\nlabels from multiple experts). We assign a different weight\nfor each marked label *l* *k* *[i]* [based on the level of label agree-]\nment between nutrition experts *W* Nutritionists ( *i, l* *k* *[i]* [) =] *[ n]* *[i]* *k* *[/N]* *[ i]* [,]\nwhere *n* *[i]* *k* [is the number of nutritionists who marked the]\ngiven label *l* *k* *[i]* [for the same product] *[ p]* *[i]* *j* [(out of a total of] *[ N]* *[ i]*\n\nannotators). Note that the number of products and annotators can vary between images, and some annotators may not\nlabel a particular product if they are not sure about it.\nFor VLMs predictions we consider the following\nweights: *i)* if a predicted product label appears in the annotations of experts, it is assigned the corresponding weight\n(i.e., *W* VLM ( *i, l* *k* *[i]* [) =] *[ n]* *[i]* *k* *[/N]* *[ i]* [), otherwise] *[ ii)]* [ if a prediction]\nis not in the annotated list, then *W* VLM ( *i, l* *k* *[i]* [) = 0][. The pro-]\npose Expert-Weighted Recall (EWR) metric is as follows:\n\n*L* *[i]*\nEWR *[i]* = � � *Lk* =1 *i* *k* = *[W]* 1 [Nutritionists] *[W]* [VLM] [(] *[i]* *[,]* [(] *[ l][i, l]* *k* *[i]* [)] *k* *[i]* [)] (1)\n\nThe final EWR score is obtained by averaging the individual EWR *[i]* values from all images. The EWR metric\nensures that higher-agreement predictions contribute more\nto the final score, allowing flexibility for partial agreement.\n\n\n-----",
  "Figure 2. Illustration of the proposed Expert-Weighted Recall (EWR) computation for a food image *i* (left). This graphical example\ncompares annotations (e.g. label *l* 1 = *Yogurt* for product *p* 1 ) from three nutrition experts (middle) with the predictions made by a VLM\n(right). The proposed EWR metric reflects how well the VLM aligns with expert consensus while accounting for annotation variability.\n\n\n**+Cooking**\n**Model Name** **Category** **+Subcat.** **Average**\n**St** **y** **le**\n\nChatGPT 80.67 69.87 42.41 64.32\n\nGemini **85.79** **74.69** **50.00** **70.16**\n\nClaude 82.60 69.88 45.09 65.86\n\nMoondream 76.94 63.28 23.91 54.71\n\nDeepSeek 49.39 37.42 15.30 34.04\nLlaVA 63.67 48.84 28.48 47.00\n\nTable 1. Performance comparison in terms of Expert-Weighted\nRecall (EWR) of the selected VLMs in food image recognition.\nResults are reported across three categorization levels, from simpler to more complex ones: *i) Category*, *ii) Category + Subcate-*\n*gory*, and *iii) Category + Subcategory + Cooking Style* .\n\nFig. 2 shows the EWR computation by comparing the annotations (middle) with the VLM predictions (right). In the\nfood image *i* (left), three food products are detected: *p* *[i]* 1 [,] *[ p]* *[i]* 2 [,]\nand *p* *[i]* 3 [(in Fig.][ 2][ we remove the upper] *[ i]* [ notation for sim-]\nplicity). All experts agreed on *p* 1 (label *l* 1 = *“yogurt”* ),\nwhile *p* 2 was labeled differently by two experts. All identified *p* 3 with the label *l* 3 = *“fruits”* . ChatGPT correctly\npredicted *l* 1, matched *l* 2 *B*, and identified *l* 3 . The 77.76%\nEWR achieved reflects how well ChatGPT aligns with the\nexperts while accounting for annotation variability.",
  "This section evaluates the VLM performance in food image\nrecognition. Sec. 6.1 analyzes classification across different\ngranularity levels, Sec. 6.2 explores the most and least chal\n\nlenging food products, and Sec. 6.3 evaluates the impact of\nimage complexity on recognition accuracy.",
  "Table 1 presents the performance of each VLM in terms\nof EWR across three classification levels: *i) category*, *ii)*\n*category + subcategory*, and *iii) category + subcategory*\n*+ cooking style* . Overall, closed-source models (ChatGPT, Gemini, and Claude) consistently outperform opensource ones (Moondream, DeepSeek, and LLaVA) at all\nclassification levels. Gemini achieves the highest average\nEWR across all levels (70.16%), indicating a broader detection range, while ChatGPT (64.32%) and Claude (65.86%)\nmaintain strong performance.\nAs classification complexity increases, all models experience performance drops. For instance, Gemini’s EWR declines from 85.79% ( *category* ) to 74.69% ( *category + sub-*\n*category* ) and 50.00% ( *category + subcategory + cooking*\n*style* ), with a similar trend observed for ChatGPT (80.67%,\n69.87% and 42.41%, respectively). These results highlight\nthe challenge of fine-grained recognition.\nAmong open-source models, Moondream outperforms\nDeepSeek and LLaVA (avg. 54.71% vs. 34.04% and\n47.00%, respectively), excelling in *category* and *category*\n*+ subcategory* recognition. DeepSeek, with the lowest average EWR (34.04%), struggles due to limited exposure to\nfood datasets. Figure 3 presents challenging cases. In Figure 3A), a multi-component dish is inconsistently labeled\nby experts, with VLMs only recognizing individual ingre\n\n-----\n\nFigure 3. Examples of VLMs predictions compared to nutritionist’s annotations. (A) A multi-component dish where some experts identify\nindividual ingredients ( *“legumes”*, *“meat”*, *“vegetables”* ), while others classify it as *“Spanish stew (cocido”* ) (B) A whole grain bread\nmisclassified by Gemini as *“croquettes”* due to shape similarity. (C) An orange juice image where some VLMs such as DeepSeek\nincorrectly identifies a sandwich from a background image on the paper tray liner.\n\n\ndients. Figure 3B) shows whole-grain bread misclassified\nby Gemini as croquettes due to shape similarity. In Figure 3C), DeepSeek mistakenly identifies a sandwich from a\nbackground image on a paper tray liner, highlighting VLM\nsusceptibility to background distractions.",
  "The ability of VLMs to recognize specific food products\nvaries significantly across different categorization levels.\nFigure 4 presents radar charts illustrating the percentage of\ncorrectly recognized food products for each level, similar to\nTask 1. It is important to highlight that these radar charts\ninclude some examples of all possible classes considered in\nour proposed FoodNExTDB, i.e., 10 categories, 62 subcategories, and 9 cooking styles.\nAt the *category* level, VLMs perform best in recognizing food categories such as *“cereals and legumes”, “protein*\n*sources”*, and *“dairy and plant-based drinks”* . In these particular categories, ChatGPT consistently outperforms other\nmodels, achieving the highest accuracy across most categories. Conversely, *“fast food”* remains the most challenging category for all models. At the *subcategory* level, recognition declines, with *“fruits”* being more accurately detected than *“vegetables”* and *“fish”* outperforming *“poul-*\n*try”* within their corresponding main categories. *“Pasta”* is\nalso more frequently recognized than *“rice”* .\nAt the *cooking style* level, all models struggle significantly. *“Fresh”* is the most identifiable style, followed\nby *“grilled”*, while *“fried”* and *“stewed”* are the least\naccurately predicted. Notably, Gemini excels in detecting *“fresh”* foods, Moondream in *“oven-baked”* products,\nChatGPT in *“preserved”* foods, and LLaVA in *“stewed”*\n\n\ndishes, despite overall lower performance in this category.",
  "Finally, we evaluate the impact of image complexity (i.e.,\nthe number of food products present in one image) on\nthe VLMs’ performances. This analysis is carried out for\n*single-product* images (i.e., only one food product appears\nin the image) and *multi-product* images (i.e., multiple food\nproducts can appear in one image). Table 2 presents the\nEWR of each VLM across these scenarios. For all models\n\nexcept DeepSeek, performance is higher for the scenario\nof single-product images, with VLMs such as ChatGPT,\nGemini, Claude, and Moondream are able to achieve EWRs\nabove 90%. In contrast, EWR results decrease considerably for the scenario of multi-product images. Again, Gemini demonstrates the highest overall performance in both\nscenarios (88.35% EWR) whereas DeepSeek is the worst\n(47.66% EWR), showing a difference of over 20% EWR\nwith the rest of the VLMs.\n\nA more detailed analysis of our proposed database reveals that, for the scenario of single-product images, approximately 28% of images contain *“fruits”*, 10% include\n\n\n**Model Name** **Sin** **g** **le-Product Ima** **g** **e** **Multi-Product Ima** **g** **e** **Avera** **g** **e**\n\n\nChatGPT 90.71 76.51 83.61\n\nGemini **94.52** **82.18** **88.35**\n\nClaude 92.48 78.51 85.50\n\nMoondream 90.03 71.52 80.78\n\nDeepSeek 43.48 51.83 47.66\nLlaVA 68.29 61.76 65.03\n\nTable 2. Performance comparison in terms of EWR of the selected\nVLMs in food recognition ( *category* level) based on the image\ncomplexity (i.e., *single-product* and *multi-product* images).\n\n\n-----\n\n**A) Category**\n\n\n\n**Protein**\n\n\n**Beverages**\n\n\n**C) Cooking Style**\n\n**Preserved**\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n**Dishes**\n\n\n\n**Legumes**\n\n\n\nFigure 4. Radar charts illustrating VLM performance in fine-grained food recognition. We include some examples of all available classes\nconsidered in our proposed FoodNExTDB, i.e., 10 categories, 62 subcategories, and 9 cooking styles. (This figure is best viewed in color.)\n\n\n*“beverages”* such as *“infusions or coffee”*, and 7% feature *“yogurt and fresh cheese”* . As observed in previous\nexperiments, these are among the most accurately recognized food *categories* and *subcategories*, which explains the\nhigher performance.",
  "Diet analysis remains a major challenge in nutrition, requiring the consideration of multiple interrelated factors. While\npure image recognition models have improved significantly\nin the task of food recognition, they still struggle with complex, multi-food images and fail to provide sufficient contextual understanding. Although these models bring automated nutritional assessment closer to reality, they remain\ninsufficient for a comprehensive analysis.\nVLMs present a promising alternative by integrating textual and visual reasoning, improving explainability in food\nrecognition and dietary analysis. However, they still face\ndifficulties with fine-grained tasks such as identifying cooking style, which require additional multi-modal data integration. More research is needed to benchmark their performance against transformer-based models in this area.\nA key observation is the performance gap between openand closed-source VLMs. Open-source models consistently\nunderperform, often struggling with structured prompts and\ngenerating accurate responses. As a result, improving\nfine-tuning strategies, dataset diversity, and domain-specific\ntraining are crucial to bridge this gap.\nFinally, integrating VLM with personalized nutrition\nstrategies could improve dietary tracking and chronic disease prevention. Combining AI-driven food recognition\nwith multimodal data from wearables, dietary questionnaires, and expert supervision may improve accuracy and\nadherence to automated dietary assessments [42].",
  "This study presents FoodNExTDB, a food image database\nwith 9,263 expert-labeled images, many reflecting Mediterranean and Spanish diets. A key strength of this database\nis that all images were annotated by seven nutrition experts,\nadding value by providing structured nutritional information, including the main *category*, *subcategory*, and *cooking*\n*style* for each detected food product.\nWe also propose an experimental framework to assess\nsix state-of-the-art VLMs, analyzing their capabilities in\nfood recognition and prompt comprehension. Due to interannotator variability, we designed a novel weighted evaluation metric named Expert-Weighted Recall (EWR). Our\nfindings reveal a clear gap in model performance across\nclassification levels. While in main categories such as *“pro-*\n*tein sources”* and *“vegetables and fruits”* the top models\nreach around 80% EWR, *cooking style* recognition remains\nchallenging, with top VLMs achieving only 50% EWR.\nThis discrepancy highlights VLMs’ limits in capturing finegrained food attributes, suggesting the need for advances in\ncontext-aware learning and multimodal integration.",
  "This study is supported by projects: AI4FOOD-CM\n(Y2020/TCS6654), FACINGLCOVID-CM (PD2022- 004REACT-EU), INTER-ACTION (PID2021-126521OB-I00\nMICINN/FEDER), HumanCAIC (TED2021-131787BI00\nMICINN), PowerAI+ (SI4/PJI/2024-00062), and C´atedra\nENIA UAM-VERIDAS en IA Responsable (NextGenerationEU PRTR TSI-100927-2023-2). We thank the nutrition\nexperts for their valuable image annotations (B. LacruzPleguezuelos, L.J. Marcos-Zambrano, S. Gallego Pozo,\nJ. Haya, M. Izquierdo-Mu˜noz, J.A. Lemke Flores, G.X.\nBaz´an).\n\n\n-----",
  "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGPT-4 Technical Report. *arXiv preprint arXiv:2303.08774*,\n2023. 1\n\n[2] Eduardo Aguilar, Marc Bola˜nos, and Petia Radeva. Food\nRecognition using Fusion of Classifiers Based on CNNs. In\n*Proc. of the International Conference on Image Analysis and*\n*Processing*, 2017. 2\n\n[3] Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac,\net al. Gemini: A Family of Highly Capable Multimodal\nModels. *arXiv preprint arXiv:2312.11805*, 2023. 1, 4\n\n[4] AI Anthropic. The Claude 3 Model Family: Opus, Sonnet,\nHaiku. *Claude-3 Model Card*, 1:1, 2024. 4\n\n[5] Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay,\nAlexander C Li, Adrien Bardes, Suzanne Petryk, Oscar\nMa˜nas, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman,\net al. An Introduction to Vision-Language Modeling. *arXiv*\n*preprint arXiv:2405.17247*, 2024. 2\n\n[6] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\nFood-101 – Mining Discriminative Components with Random Forests. In *Proc. of the European Conference on Com-*\n*puter Vision*, 2014. 2\n\n[7] Le Bu, Caiping Hu, and Xiuliang Zhang. Recognition of\nFood Images Based on Transfer Learning and Ensemble\nLearning. *Plos One*, 19(1):e0296789, 2024. 1, 3\n\n[8] Jingjing Chen and Chong-Wah Ngo. Deep-based Ingredient\nRecognition for Cooking Recipe Retrieval. In *Proc. of the*\n*International Conference on Multimedia*, 2016. 2\n\n[9] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan,\nWen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified Multimodal Understanding and Generation with\nData and Model Scaling. *arXiv preprint arXiv:2501.17811*,\n2025. 4\n\n[10] Gianluigi Ciocca, Paolo Napoletano, and Raimondo Schettini. Food Recognition: A New Dataset, Experiments, and\nResults. *IEEE Journal of Biomedical and Health Informat-*\n*ics*, 21(3):588–598, 2016. 2\n\n[11] Ivan Deandres-Tame, Ruben Tolosana, Ruben VeraRodriguez, Aythami Morales, Julian Fierrez, and Javier\nOrtega-Garcia. How Good is ChatGPT at Face Biometrics?\na First Look into Recognition, Soft Biometrics, and Explainability. *IEEE Access*, 12:34390–34401, 2024. 1\n\n[12] Petko Georgiev, Ving Ian Lei, Ryan Burnell, et al. Gemini\n1.5: Unlocking Multimodal Understanding across Millions\nof Tokens of Context. *arXiv preprint arXiv:2403.05530*,\n2024. 4\n\n[13] Pan He, Zhu Liu, Giovanni Baiocchi, Dabo Guan, Yan Bai,\nand Klaus Hubacek. Health–environment Efficiency of Diets\nShows Nonlinear Trends over 1990–2011. *Nature Food*, 5\n(2):116–124, 2024. 1\n\n[14] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-Excitation Networks. In *Proc. of the Conference on Computer Vision and*\n*Pattern Recognition*, 2018. 3\n\n[15] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Weli\n\nhinda, Alan Hayes, Alec Radford, et al. GPT-4o System\nCard. *arXiv preprint arXiv:2410.21276*, 2024. 4\n\n[16] Y. Kawano and K. Yanai. Automatic Expansion of a Food\nImage Dataset Leveraging Existing Categories with Domain\nAdaptation. In *Proc. of the Workshop on Transferring and*\n*Adapting Source Knowledge in Computer Vision*, 2014. 2\n\n[17] Jun-Hwa Kim, Nam-Ho Kim, Donghyeok Jo, and Chee Sun\nWon. Multimodal Food Image Classification with Large\nLanguage Models. *Electronics*, 13(22), 2024. 3\n\n[18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBLIP-2: Bootstrapping Language-image Pre-training with\nFrozen Image Encoders and Large Language Models. In\n*Proc. of the International Conference on Machine Learning*,\n2023. 2\n\n[19] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. VILA: On Pre-training for Visual Language Models. In *Proc. of the Conference on Com-*\n*puter Vision and Pattern Recognition*, 2024. 2\n\n[20] Jakob Linseisen, Britta Renner, Kurt Gedrich, Jan Wirsam,\nChristina Holzapfel, Stefan Lorkowski, Bernhard Watzl,\nHannelore Daniel, Michael Leitzmann, et al. Perspective: Data in Personalized Nutrition: Bridging Biomedical, Psycho-behavioral, and Food Environment Approaches\nfor Population-wide Impact. *Advances in Nutrition*, page\n100377, 2025. 1\n\n[21] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao\nWu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu\nZhang, Chong Ruan, et al. Deepseek-v3 Technical Report.\n*arXiv preprint arXiv:2412.19437*, 2024. 1\n\n[22] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved Baselines with Visual Instruction Tuning. In *Proc.*\n*of the Conference on Computer Vision and Pattern Recogni-*\n*tion*, 2024. 4\n\n[23] Lihua Luo. Research on Food Image Recognition of Deep\nLearning Algorithms. In *Proc. of the International Confer-*\n*ence on Computers, Information Processing and Advanced*\n*Education*, 2023. 1, 3\n\n[24] Peihua Ma, Shawn Tsai, Yiyang He, Xiaoxue Jia, Dongyang\nZhen, Ning Yu, Qin Wang, Jaspreet K.C. Ahuja, and Cheng-I\nWei. Large Language Models in Food Science: Innovations,\nApplications, and Future. *Trends in Food Science & Tech-*\n*nology*, 148:104488, 2024. 1\n\n[25] Peihua Ma, Yixin Wu, Ning Yu, Xiaoxue Jia, Yiyang He,\nYang Zhang, Michael Backes, Qin Wang, and Cheng-I Wei.\nIntegrating Vision-Language Models for Accelerated HighThroughput Nutrition Screening. *Advanced Science*, 11(34):\n2403578, 2024. 3\n\n[26] Zheng Ma, Mianzhi Pan, Wenhan Wu, Kanzhi Cheng, Jianbing Zhang, Shujian Huang, and Jiajun Chen. Food-500\nCap: A Fine-Grained Food Caption Benchmark for Evaluating Vision-Language Models. In *Proc. of the International*\n*Conference on Multimedia*, 2023. 3\n\n[27] Y. Matsuda, H. Hoashi, and K. Yanai. Recognition of\nMultiple-Food Images by Detecting Candidate Regions. In\n*Proc. of the International Conference on Multimedia and*\n*Expo*, 2012. 2\n\n\n-----\n\n[28] Patrick McAllister, Huiru Zheng, Raymond Bond, and Anne\nMoorhead. Combining Deep Residual Neural Network Features with Supervised Machine Learning Algorithms to Classify Diverse Food Image Datasets. *Computers in Biology and*\n*Medicine*, 95:217–233, 2018. 3\n\n[29] Weiqing Min, Shuqiang Jiang, Linhu Liu, Yong Rui, and\nRamesh Jain. A Survey on Food Computing. *ACM Com-*\n*puting Surveys*, 52(5):1–36, 2019. 1\n\n[30] Weiqing Min, Linhu Liu, Zhiling Wang, Zhengdong Luo,\nXiaoming Wei, Xiaolin Wei, and Shuqiang Jiang. ISIA\nFood-500: A Dataset for Large-Scale Food Recognition via\nStacked Global-Local Attention Network. In *Proc. of the In-*\n*ternational Conference on Multimedia*, 2020. 2, 3\n\n[31] Weiqing Min, Zhiling Wang, Yuxin Liu, Mengjiang Luo,\nLiping Kang, Xiaoming Wei, Xiaolin Wei, and Shuqiang\nJiang. Large Scale Visual Food Recognition. *IEEE Trans-*\n*actions on Pattern Analysis and Machine Intelligence*, 45(8):\n9932–9949, 2023. 3\n\n[32] Fnu Mohbat and Mohammed J Zaki. Llava-chef: A Multimodal Generative Model for Food Recipes. In *Proc. of*\n*the International Conference on Information and Knowledge*\n*Management*, 2024. 3\n\n[33] Kintoh Allen Nfor, Tagne Poupi Theodore Armand, Kenesbaeva Periyzat Ismaylovna, Moon-Il Joo, and Hee-Cheol\nKim. An Explainable CNN and Vision Transformer-Based\nApproach for Real-Time Food Recognition. *Nutrients*, 17\n(2):362, 2025. 3\n\n[34] Vasiliki Pitsilou, George Papadakis, and Dimitrios Skoutas.\nUsing LLMs to Extract Food Entities from Cooking Recipes.\nIn *Proc. of the International Conference on Data Engineer-*\n*ing Workshops*, 2024. 1\n\n[35] Zhixiao Qi, Yijiong Yu, Meiqi Tu, Junyi Tan, and Yongfeng\nHuang. FoodGPT: A Large Language Model in Food Testing Domain with Incremental Pre-training and Knowledge\nGraph Prompt. *arXiv preprint arXiv:2308.10173*, 2023. 1\n\n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\nTransferable Visual Models from Natural Language Supervision. In *Proc. of the International Conference on Machine*\n*Learning*, 2021. 3\n\n[37] Jes´us M Rodr´ıguez-de Vera, Pablo Villacorta, Imanol G Estepa, Marc Bola˜nos, Ignacio Saras´ua, Bhalaji Nagarajan, and\nPetia Radeva. Dining on Details: LLM-Guided Expert Networks for Fine-Grained Food Recognition. In *Proc. of the In-*\n*ternational Workshop on Multimedia Assisted Dietary Man-*\n*agement*, 2023. 1\n\n[38] Jes´us M Rodr´ıguez-De-Vera, Imanol G Estepa, Marc\nBola˜nos, Bhalaji Nagarajan, and Petia Radeva. LOFI: LOngtailed FIne-Grained Network for Food Recognition. In *Proc.*\n*of the Conference on Computer Vision and Pattern Recogni-*\n*tion*, 2024. 3\n\n[39] Sergio Romero-Tapiador, Blanca Lacruz-Pleguezuelos,\nRuben Tolosana, et al. AI4FoodDB: A Database for Personalized e-Health Nutrition and Lifestyle through Wearable Devices and Artificial Intelligence. *Database*, 2023:\nbaad049, 2023. 2, 3\n\n\n\n[40] Sergio Romero-Tapiador, Ruben Tolosana, Aythami\nMorales, et al. AI4Food-NutritionFW: A Novel Framework for the Automatic Synthesis and Analysis of Eating\nBehaviours. *IEEE Access*, 1:112199 – 112211, 2023. 1\n\n[41] Sergio Romero-Tapiador, Ruben Tolosana, Aythami\nMorales, et al. Leveraging Automatic Personalised Nutrition: Food Image Recognition Benchmark and Dataset\nBased on Nutrition Taxonomy. *Multimedia Tools and*\n*Applications*, 84:1945–1966, 2024. 1, 3\n\n[42] Sergio Romero-Tapiador, Ruben Tolosana, Aythami\nMorales, et al. Personalized Weight Loss Management\nthrough Wearable Devices and Artificial Intelligence. *arXiv*\n*preprint arXiv:2409.08700*, 2024. 8\n\n[43] Aditya Sharma, Michael Saxon, and William Yang Wang.\nLosing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts. *arXiv preprint arXiv:2406.16851*, 2024. 2\n\n[44] Guorui Sheng, Weiqing Min, Xiangyi Zhu, Liang Xu, Qingshuo Sun, Yancun Yang, Lili Wang, and Shuqiang Jiang. A\nLightweight Hybrid Model with Location-preserving ViT for\nEfficient Food Recognition. *Nutrients*, 16(2):200, 2024. 1, 3\n\n[45] Ram B Singh, Jan Fedacko, Ghizal Fatima, Aminat\nMagomedova, Shaw Watanabe, and Galal Elkilany. Why and\nHow the Indo-Mediterranean Diet May Be Superior to Other\nDiets: The Role of Antioxidants in the Diet. *Nutrients*, 14\n(4):898, 2022. 1\n\n[46] Ashutosh Singla, Lin Yuan, and Touradj Ebrahimi.\nFood/Non-Food Image Classification and Food Categorization Using Pre-Trained GoogLeNet Model. In *Proc. of*\n*the International Workshop on Multimedia Assisted Dietary*\n*Management*, 2016. 2\n\n[47] Quin Thames, Arjun Karpur, Wade Norris, Fangting Xia,\nLiviu Panait, Tobias Weyand, and Jack Sim. Nutrition5k:\nTowards Automatic Nutritional Understanding of Generic\nFood. In *Proc. of the Conference on Computer Vision and*\n*Pattern Recognition*, 2021. 2\n\n[48] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma,\nXingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai\nYu, Chong Ruan, et al. Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation.\n*arXiv preprint arXiv:2410.13848*, 2024. 4\n\n[49] Zhongqi Yang, Elahe Khatibi, Nitish Nagesh, Mahyar Abbasian, Iman Azimi, Ramesh Jain, and Amir M Rahmani. ChatDiet: Empowering Personalized Nutritionoriented Food Recommender Chatbots through an LLMAugmented Framework. *Smart Health*, 32:100465, 2024.\n1\n\n[50] Yuehao Yin, Huiyan Qi, Bin Zhu, Jingjing Chen, Yu-Gang\nJiang, and Chong-Wah Ngo. FoodLMM: A Versatile Food\nAssistant Using Large Multi-modal Model. *arXiv preprint*\n*arXiv:2312.14991*, 2023. 3\n\n[51] Dongyu Zhang, Ruofan Hu, Dandan Tao, Hao Feng, and\nElke Rundensteiner. LLM-based Hierarchical Label Anno\ntation for Foodborne Illness Detection on Social Media. In\n\n*Proc. of the International Conference on Big Data*, 2024. 1\n\n[52] Ping Zhang. Influence of Foods and Nutrition on the Gut\nMicrobiome and Implications for Intestinal Health. *Interna-*\n*tional Journal of Molecular Sciences*, 23(17):9588, 2022. 1\n\n\n-----",
  "*Corresponding Author: Sonu Kumar, R&D, Sporo Health, USA. Email: sonu@sporohealth.com\n\n**Abstract:** As Agentic AI gain mainstream adoption, the industry invests heavily in model capabilities, achieving rapid leaps in\nreasoning and quality. However, these systems remain largely confined to data silos, and each new integration requires custom\nlogic that is difficult to scale. The Model Context Protocol (MCP) addresses this challenge by defining a universal, open standard\nfor securely connecting AI-based applications (MCP clients) to data sources (MCP servers). However, the flexibility of the MCP\nintroduces new risks, including malicious tool servers and compromised data integrity. We present MCP Guardian, a framework\nthat strengthens MCP-based communication with authentication, rate-limiting, logging, tracing, and Web Application Firewall\n(WAF) scanning. Through real-world scenarios and empirical testing, we demonstrate how MCP Guardian effectively mitigates\nattacks and ensures robust oversight with minimal overheads. Our approach fosters secure, scalable data access for AI assistants,\nunderscoring the importance of a defense-in-depth approach that enables safer and more transparent innovation in AI-driven\nenvironments.\n\n**Keywords:** model context protocol, mcp, agentic ai, artificial intelligence, generative ai",
  "LLMs have witnessed a rapid expansion in both scale and capability, demonstrating unprecedented performance in tasks\nranging from natural language generation to complex programming challenges. While initially confined to relatively passive\nroles—delivering text-based answers or summaries—LLMs are now increasingly being placed in “agentic” positions, where they\nnot only generate content but also initiate and orchestrate actions across various external systems. This paradigm shift underscores\nhow LLMs can serve as decision-making engines, interfacing with diverse tools, such as databases, web services, and file systems.\nBy autonomously chaining multiple tool calls, agentic workflows can solve sophisticated problems that extend beyond the written\nword.\n\nHowever, unlocking these extended capabilities has introduced significant engineering complexity, largely because of the lack\nof standardized interfaces. Historically, developers resorted to custom “plugin” or “adapter” logic for each new external tool,\nleading to fragmented solutions that are difficult to maintain at scale. To address this fragmentation, the Model Context Protocol\n(MCP) was recently proposed as a universal “multiplexer,” enabling LLM-powered clients to discover and invoke tool servers in\na unified manner. By abstracting the underlying implementation details, the MCP simplifies tool integration, thereby lowering the\nbarrier to building AI applications that can incorporate external data and services.\n\nHowever, this newfound flexibility comes with an increased risk. LLMs or more advanced agentic workflows that can\nautonomously access file systems or databases pose non-trivial security challenges: a maliciously crafted prompt or compromised\nserver can result in unauthorized data exfiltration, destructive operations, or other exploitative behaviors. Moreover, this agentic\nparadigm requires enhanced observability. Traditional logging and monitoring methods are insufficient for capturing the complex\nchains of reasoning and actions that an LLM may perform when orchestrating multiple tools in parallel. The absence of thorough\ninstrumentation complicates both real-time auditing and post-hoc forensics, raising concerns about transparency and compliance.\n\nIn light of these challenges, this study introduces MCP Guardian, a comprehensive middleware layer aimed at securing and\nmonitoring the interactions between MCP Clients and MCP-based tool servers. Drawing inspiration from zero-trust security\nframeworks, web application firewalls, and distributed tracing practices, MCP Guardian intercepts every tool call to:\n\n  - Enforce authentication and authorization checks,\n\n  - Apply rate-limiting strategies to protect against abuse or runaway processes,\n\n  - Provide extensive logging and tracing for transparent auditing, and\n\n  - Scan suspicious input patterns via a lightweight Web Application Firewall (WAF).\n\nThis contribution synthesizes insights from LLM alignment, software security, and distributed system observability to propose\na practical solution for the next generation of AI-driven agents. In particular, we highlight the following points:",
  "-----\n\n1. **Problem Analysis:** A thorough examination of security and observability gaps in MCP-based systems, especially where\nLLMs autonomously issue tool calls.\n2. **Framework Design:** A detailed architectural description of MCP Guardian highlighting its core components\n(authentication, access control, request logging, rate limiting, and WAF scanning) and how they interoperate.\n3. **Implementation and Evaluation:** A reference implementation in Python, tested on real-world scenarios, including a\nweather-tool MCP server, to illustrate both security efficacy and performance overhead.\n4. **Empirical and Theoretical Insights:** Scenario-based testing of malicious inputs, latency measurements, and throughput\nanalyses, offering an understanding of how MCP Guardian scales and adapts to various domains.",
  "Recent scholarly interest in AI agents has intensified, driven by the desire to move beyond passive text generation and\nempower Large Language Models (LLMs) to autonomously perform tasks in real-world contexts. Early attempts at “tool use” often\nrelied on bespoke plugins or direct calls to specialized APIs. For instance, OpenAI’s ChatGPT introduced plugin frameworks that\nconnect to external services [3], while other AI-based “copilot” tools were designed to read and write files in code repositories.\nDespite these innovations, the lack of a unified, standardized method for discovering and invoking tools frequently forced\ndevelopers to create patchwork solutions, thereby increasing the risk of security vulnerabilities, inconsistent access controls, and a\nlimited audit trail.",
  "The Model Context Protocol (MCP)—promoted by Anthropic [1] and further explored by others [2] —addresses these\nintegration challenges by offering an open, extensible protocol for LLM-driven interactions with external tools. By allowing AI\nclients to query a server for available functions and associated metadata, MCP significantly reduces the repeated overhead\nencountered in ad-hoc “plugin” models. Instead of requiring specialized integrations for each tool, a single request/response channel\n(e.g., JSON over stdio or HTTP) serves as a universal interface. This design shares similarities with gRPC or JSON-RPC but is\noptimized for LLMs’ iterative reasoning, where multiple tool calls may be chained in a single session.\n\nHowever, MCP’s openness also presents a notable attack surface. Malicious or compromised MCP servers can cloak harmful\ncode under seemingly benign functionality and exfiltrate sensitive data. Consequently, security researchers and practitioners have\nhighlighted the need for robust authentication, authorization, and supply chain protections within the MCP ecosystem. Some\npreliminary guidelines exist for “trusted” AI-tool environments [4], but comprehensive security strategies remain underdeveloped\nfor broader, community-driven MCP deployments.",
  "As LLMs evolve into autonomous agents, the scope of potential security risks widens substantially. These systems can, in\nprinciple, read, write, and execute code, posing serious threats if not rigorously controlled. Zero-trust architectures emphasize\ncontinuous request validation rather than assuming any “trusted” status at the outset, an approach that applies naturally to MCP:\nevery incoming request to a tool server should undergo authentication, parameter sanitization, and logging.\n\nLi and Hsu [2] underscore the importance of policy-driven authorization in agentic AI, where a model’s capacity to decide\nwhich actions to take must align with stringent security and compliance policies. In tandem, DevSecOps best practices mandate\ncode scanning, secret rotation, and frequent audits. Combining zero-trust principles with LLM-based tool usage forms a key frontier\nin ensuring secure operations in AI-intensive infrastructures.",
  "Recent findings detail multiple ways in which malicious actors exploit the flexibility of MCP-based communication:\n\n**1.** **Tool Poisoning Attacks**\n\nAn adversary may embed harmful instructions within otherwise benign tool documentation, guiding the LLM to\nperform actions invisible to end users. Invariant Labs [5] describes a case where a “simple” addition function secretly\ninstructs the AI to read SSH keys ( *~/.ssh/id_rsa* ) and local MCP configuration files ( *~/.cursor/mcp.json* ), then transmit\nthem to an attacker [6]. An abbreviated, paraphrased version of such malicious code is shown below:",
  "-----\n\nAt first glance, it appears to be a tool for arithmetic, but hidden instructions prompt the AI model to perform\nunauthorized file reading and exfiltration.\n\n**2.** **Tool Name Conflicts**\n\nAttackers may register MCP servers under names resembling those of trusted tools (e.g., *tavily-mcp* vs *. mcp-tavily* ),\naiming to dupe an LLM into calling a counterfeit server [6]. This can lead to sensitive data leaks or unintended command\nexecutions if the AI or user confuses the malicious server with a legitimate one.\n\n**3.** **Shadowing Attacks (Overwriting Tool Descriptions)**\n\nMalicious servers can overwrite or override the description of an existing, trusted tool, effectively hijacking its\nbehavior. Invariant Labs demonstrated how a routine “ *send_email* ” tool could be silently re-routed to funnel messages to\nan attacker’s address [5] . A paraphrased example is shown below:\n\nEven though this snippet claims to focus on addition, it includes hidden directives that alter an entirely different\ntool’s functionality.\n\n**4.** **Installer Spoofing**\n\nSome community-driven MCP installers (e.g., *mcp-get, smithery-cli* ) lack robust integrity checks. Attackers can\ndistribute tampered installers that compromise system configurations or introduce backdoors [6] . This risk is exacerbated\nif users skip verification steps.\n\n**5.** **Command Injection Vulnerabilities**\n\nA common threat in software applications, command injection is especially risky in AI-driven systems where usersupplied parameters might be dynamically assembled into shell commands. Equixly’s research found that 43% of MCP\nserver implementations tested were susceptible to injection [7]. A paraphrased vulnerable snippet might appear as:\n\nAn attacker can insert shell metacharacters to execute arbitrary code, such as:\n\n**6.** **MCP Rug Pulls**",
  "-----\n\nA “rug pull” occurs when a tool initially seems safe but later adds malicious logic to exfiltrate sensitive information.\nTools that are not version-pinned or code-signed can be silently updated with harmful features, leading to data theft or\nprivilege escalation [8] .\n\n**7.** **Token Theft and Account Takeover**\n\nWhere MCP servers rely on OAuth tokens or API credentials, these tokens can be stolen if stored insecurely or\nexposed through logs. Attackers may then access user emails, databases, or other resources impersonating legitimate\nclients [9].\n\n**8.** **Sandbox Escape**\n\nEven if an MCP server attempts to sandbox each tool, vulnerabilities in libraries or misconfigurations can grant a\nmalicious script unwarranted access to the host system. Escalation paths include system calls, buffer overflows, or logic\nerrors in third-party dependencies [6] .",
  "Parallel to security, observability—encompassing logging, tracing, and metrics—has become essential in distributed\nmicroservice architectures. Tools like OpenTelemetry provide a standardized way to correlate logs and traces, simplifying rootcause analysis across multiple services [2]. However, LLM-based systems bring unique challenges: the model’s chain of thought\nis often opaque, and the agent may independently chain together multiple tool calls without explicit user direction. Capturing this\ncomplexity requires granular instrumentation that records each request and response in detail. Existing research underscores how\nlimited logging can thwart debugging and forensics in complex AI pipelines [6], prompting calls for deeper integration of standard\nobservability frameworks within agentic AI ecosystems.",
  "Although prior work acknowledges the need for standardizing AI-to-tool communications, relatively little guidance addresses\ncomprehensive security and monitoring at the protocol level. Efforts like ChatGPT plugins [3] and specialized policy engines [2]\npartially address issues of access control, but do not converge into a fully integrated middleware that merges:\n\n     - Authentication & Authorization\n\n     - Rate Limiting\n\n     - WAF Scanning & Intrusion Detection\n\n     - Detailed Logging & Tracing\n\nHence, the literature reveals a notable gap for a defense-in-depth framework that bolsters both security and observability in\nMCP-based agentic workflows. Attack vectors such as tool poisoning, malicious naming, and command injection underscore the\nurgency of robust safeguards that can intercept risky operations at runtime.",
  "MCP Guardian aims to fill this void by providing a unified security and monitoring layer for MCP-based systems. Through\nintercepting each request at a single control point, it enforces authentication, rate limiting, suspicious pattern detection, and\ncomprehensive logging. Drawing on zero-trust principles and best practices from web application firewalls, our approach is\ndeliberately lightweight, allowing seamless integration without major restructuring of MCP servers. At the same time, we address\na broader range of vulnerabilities, from tool poisoning to command injection, by blocking suspicious calls before they reach critical\ninternal APIs.\n\nIn the sections that follow, we detail MCP Guardian’s architecture and evaluate its efficacy against common threats,\nhighlighting its minimal performance overhead and adaptability to diverse MCP use cases. We also discuss potential extensions—\nsuch as code signing, anomaly detection, and distributed tracing—paving the way for enterprise-ready solutions that secure AIdriven workflows without stifling innovation.",
  "In order to secure and monitor interactions between MCP clients and servers, we propose MCP Guardian as an intermediate\n“middleware” layer. Rather than requiring developers to embed security checks directly into each tool server, MCP Guardian\nintercepts all calls via an override of the *invoke_tool* method in MCP. This design choice ensures minimal disruption to existing",
  "-----\n\ncodebases while providing a central point of control for authentication, authorization, rate limiting, request monitoring, and Web\nApplication Firewall (WAF) scanning.",
  "**1.** **Authentication and Authorization**\n\na. Enforces an API-token mechanism, verifying that each request is associated with a valid token.\nb. Optionally restricts specific tokens to certain tools or to read-only versus administrative privileges.\n\n**2.** **Rate Limiting**\n\na. Tracks usage on a per-token basis and denies further requests if a certain threshold is exceeded (e.g., five\nrequests per minute).\nb. Prevents resource exhaustion attacks and unintentional “infinite loop” scenarios triggered by LLMs.\n\n**3.** **Web Application Firewall (WAF)**\n\na. Scans request arguments for known malicious patterns (e.g., SQL injection signatures, destructive file\ncommands).\nb. Blocks or flags requests exhibiting suspicious behavior, thus preventing unsafe inputs from reaching the\nunderlying MCP server.\n\n**4.** **Logging and Observability**\n\na. Logs each request and response, capturing contextual information such as the calling user/agent, request\nparameters, timestamps, and any triggered warnings.\nb. Facilitates optional integration with tracing systems like OpenTelemetry, enabling end-to-end correlation of\nrequests across distributed architectures.",
  "Figure 1 Conceptualized below is an illustration of how MCP Guardian fits into a typical LLM-based workflow:\n\n**Figure 1**\n**MCP Tool Call Sequence**\n\n**1.** **Request Interception:** The LLM client submits a request specifying which MCP tool it intends to call.\n**2.** **Security Checks:** MCP Guardian validates the request token, checks rate limits, and scans for malicious patterns.",
  "-----\n\n**3.** **Invocation:** If the request passes these checks, the Guardian forwards it to the original MCP server.\n**4.** **Response Handling:** The server’s response is logged and then returned to the LLM client, maintaining a complete audit\ntrail.",
  "We developed our MCP Guardian reference implementation in Python, building on a standard MCP server setup. The design\nfollows a middleware approach, intercepting calls between the AI client (MCP client) and underlying tool servers through a single\nclass that applies security and observability controls.",
  "- **MCPGuardian:** A class overriding the default invoke_mcp_tool method. It orchestrates token validation, rate\nlimiting, WAF scanning, logging, and optional administrative alerts.\n\n     - **guarded_invoke_tool():** A custom method that examines each request’s parameters—such as the user token and\ntool arguments—applies security rules, and logs relevant data. Only when all checks pass does it forward the call to\nthe original MCP server function.\n\nIn addition to these core methods, we have integrated best practices inspired by the broader AI security community:\n\n**1.** **Secure Token Storage (Optional)**\n\n     - Tokens can be encrypted before being saved to a datastore.\n\n     - For workflows requiring higher assurance, we also support short-lived tokens with scope limitations and expiration\n(e.g., 5 minutes). This approach limits the damage if a token is inadvertently exposed.\n\n**2.** **Logging and Observability**\n\n     - We rely on Python’s built-in logging to record each request and response, capturing timestamps, tool names, and\nuser identifiers.\n\n     - Suspicious patterns—such as references to SSH files or tokens in tool parameters—can trigger a warning or critical\nlog entry. Advanced users can configure real-time alerts (e.g., emails, Slack messages) by implementing a custom\nnotification function.\n\n**3.** **Suspicious Pattern Detection (WAF Layer)**\n\n     - The Guardian checks parameters against a regex-based WAF. Commonly flagged indicators include SQL injection\nstrings, destructive shell commands, and references to sensitive files or environment variables.\n\n     - Administrators can extend or replace these WAF rules with domain-specific logic—for example, scanning for\nunauthorized file paths in HPC or database commands.\n\n**4.** **Rate Limiting**\n\n     - We maintain a per-token counter to track how many requests are made within a defined interval. If calls exceed the\nconfigured threshold (e.g., 5 requests per minute), a “429 Too Many Requests” error is returned.\n\n     - This measure prevents runaway processes or denial-of-service scenarios triggered by LLM loops.",
  "**1.** **Tokens**\n\n     - Default: A set of valid tokens loaded from a file or environment variable.\n\n     - Advanced: A dynamic authentication backend that generates encrypted or short-lived tokens.\n\n**2.** **Rate Limits**\n\n     - A numerical threshold (e.g., 5 requests per minute per token).\n\n     - Customizable at runtime to accommodate varying workloads or usage policies.\n\n**3.** **WAF Patterns**\n\n     - Default: A small ruleset targeting common attack vectors (SQL injection, <script> tags, destructive commands).",
  "-----\n\n     - Extensible: Users can add domain-specific rules or leverage existing intrusion detection systems.\n\n**4.** **Logging & Tracing**\n\n     - By default, logs are written to a local file (mcp_guardian.log).\n\n     - Users may specify a remote logging endpoint or incorporate a distributed tracing framework (e.g., OpenTelemetry)\nto visualize cross-service request flows.\n\n     - Critical or suspicious events can optionally trigger real-time alerts via email, chat, or webhook integrations.",
  "Below is a simplified code example demonstrating the Guardian’s setup and usage:\n\nWith just a few lines of code, MCPGuardian applies its entire security and monitoring stack to any MCP tools exposed by the\nserver. Developers can choose to add optional modules—for instance, an *MCPSecurityMonitor* that looks for references to secrets\nor tokens in the request parameters, or an encrypted token store that issues and validates short-lived OAuth credentials.\n\nOverall, this simple architecture simplifies the adoption of best practices in authentication, rate limiting, intrusion detection,\nand observability, allowing organizations to deploy AI-driven tools with confidence under the Model Context Protocol.",
  "Although the core middleware layer provides a baseline defense, MCP Guardian can be extended to support enterprise-grade use\n\ncases:",
  "called or the range of allowable arguments.",
  "to security rules without redeploying code.",
  "from a learned norm.",
  "We evaluated MCP Guardian in two primary dimensions: (a) its effectiveness at preventing or mitigating malicious or\nunintended requests, and (b) the computational overhead introduced when deployed within typical MCP-based communication.",
  "We tested scenarios where a user intentionally supplied malicious input, such as rm -rf /, hoping the LLM would call a file\nsystem tool. MCP Guardian’s WAF scanning recognized the substring rm\\s+-rf, triggering an immediate block and returning a\n“Request blocked by WAF scanning” message.",
  "-----\n\n**High-Frequency Abuse:** In a stress test, the client repeatedly invoked get_forecast 100 times in quick succession. By setting\na max_requests_per_token limit of 5, Guardian rejected requests beyond the threshold, responding with a “429 Too Many\nRequests” status. This approach thwarts denial-of-service attempts originating from an overactive or compromised LLM.",
  "**Token Validation:** We submitted requests without a token or with an invalid token. In each case, MCP Guardian denied the\ncall with an “Unauthorized” error, thus preventing unknown or malicious entities from exploiting open endpoints.\n\nOverall, these security tests confirm that even a lightweight ruleset and straightforward token checks can thwart typical attack\nvectors, substantially reducing the risk of both unintentional and malicious misuse.",
  "We conducted load tests on a VM (8-core CPU, Python 3.12) running a simple weather MCP server protected by MCP\nGuardian. The baseline measured calls to *get_forecast* without the Guardian, while the test scenario included the authentication,\nrate-limiting, and WAF scanning modules.",
  "**Table 1**\n**Interpretation of Median latency and 95** **[th]** **percentile for different scenarios**\n\nScenario Median Latency (ms) 95 [th] Percentile (ms)\n\nBaseline (No MCP Guardian) 25.1 32.4\n\nMCP Guardian 28.9 36.7\n\nThe Guardian introduced an absolute increase of about 3–4 ms in median latency which can be observed in the Table 1\nInterpretation of Median latency and 95 [th] percentile for different scenarios. This overhead primarily stems from:\n\n1. Token lookups in a dictionary or database,\n2. Updating counters for rate-limiting,\n3. Executing regex-based WAF checks, and\n4. Logging each request and response.\nThese extra steps added a 10–15% overhead in a controlled local environment. In many real-world scenarios—where each\nrequest may incur additional network hops or LLM processing time—this overhead remains acceptable.",
  "excessive request rates, showcasing its robustness in handling common attack patterns and resource misuse.",
  "approach without compromising responsiveness in typical AI-driven applications.",
  "MCP Guardian illustrates how established security measures—such as authentication, rate limiting, and WAF scanning—can\nbe applied to agentic workflows where Large Language Models (LLMs) autonomously invoke tool APIs. Still, true defense-indepth demands additional safeguards:",
  "-----",
  "request bypasses Guardian’s checks, the operating system’s sandbox would prevent catastrophic damage to the\nunderlying infrastructure.",
  "This mitigates supply-chain risks where an attacker might inject harmful code into public repositories.",
  "a “read-only” role for weather data retrieval ensures that destructive or unauthorized updates are impossible with the\nsame token.",
  "While the current Guardian implementation focuses on core logging, rate limiting, and WAF checks, a more holistic solution\nfor observability and governance could significantly improve transparency and control over agentic AI systems:",
  "across multiple MCP servers, linking each step of the LLM’s decision process in a shared “trace ID.” This is particularly\nvaluable for diagnosing errors that emerge from multi-tool sequences.",
  "based policies, tamper-proof logs, and governance dashboards could enable real-time oversight and post-hoc\ninvestigations.",
  "consistently calls a particular server at a steady rate suddenly spiking to thousands of requests in a short period. By\nidentifying such deviations in real time, organizations can quickly contain potential misuse.",
  "Given the open and extensible nature of the Model Context Protocol, there is a compelling need for official or communitydeveloped standards that codify best practices for security. Potential enhancements include:",
  "friction and encourage uniform adoption of secure communication channels.",
  "could permit fine-grained policy definitions. This approach would streamline how permissions, rate limits, and usage\npatterns are specified and enforced.",
  "base layer of trust, preventing LLMs from connecting to uncertified or rogue endpoints.",
  "Although our results demonstrate the effectiveness of MCP Guardian in curbing malicious requests and limiting resource\noveruse, several limitations merit attention:\n\n1. **Regex-Based WAF** : The proof-of-concept WAF relies on basic pattern matching. More advanced intrusion detection\n(e.g., curated rulesets, ML-based classifiers) would likely yield fewer false positives and a wider range of threat coverage.\n2. **Centralized Logging** : Writing logs to a local file may not scale well in large deployments. Shifting to distributed log\naggregation or cloud-based services can enhance both reliability and query performance.\n3. **Partial Attack Coverage** : MCP Guardian cannot fully protect against a compromised server or malicious code within\nan MCP tool itself. Complementary measures—such as sandboxing and code-signing—are crucial to address deeper\nsupply chain risks.\n4. **Multi-Agent Context** : When multiple LLMs share the same Guardian instance, tracking distinct agent identities and\nusage quotas becomes non-trivial. Future work might explore identity management solutions that maintain robust peragent policies and data segregation.",
  "Another promising avenue for expanding MCP’s usability and security is the **mcpo** project [10] . This proxy tool exposes any MCP\nserver as a RESTful OpenAPI service, eliminating the need for raw stdio or custom connectors. By automatically generating\nOpenAPI documentation and leveraging standard HTTP protocols, mcpo makes it easier to integrate existing security controls (e.g.,\nHTTPS, OAuth) and to scale out deployments using conventional web infrastructure. In addition:",
  "simplifying the creation of AI-driven applications that rely on mainstream HTTP and JSON.",
  "-----",
  "security practices (e.g., TLS, reverse proxies, load balancers) without extensive reconfiguration.",
  "available endpoints, thereby reducing the risk of misconfiguring APIs.\nBy combining MCP Guardian with solutions like mcpo, developers could achieve a layered approach: Guardian handles\nsophisticated security checks (authentication, rate limiting, WAF), while mcpo provides a stable, interoperable interface that aligns\nwith modern web standards. Future research may focus on tightly integrating these tools to offer a robust, end-to-end solution for\nsecuring, monitoring, and scaling MCP-based AI workflows with minimal developer friction.",
  "Agentic AI promises to transform how LLMs interact with data and software tools. The Model Context Protocol (MCP)\nprovides a flexible framework for this interaction, yet greater autonomy raises substantial security and observability concerns. We\nintroduced MCP Guardian to address these risks through authentication, rate limiting, WAF scanning, and logging—all without\ndisrupting the simple MCP workflow. The empirical results show that Guardian effectively blocks common threats and maintains\nits performance at scale. Looking ahead, we envision advanced policy engines, vetted tool registries, real-time anomaly detection,\nand open telemetry standards as the key steps toward fostering safe and accountable agentic AI. By integrating proven security\npractices into MCP-based agentic workflows, we can unlock new possibilities for productivity and creativity, without\ncompromising safety or transparency.",
  "Organizations integrating Large Language Models (LLMs) with the Model Context Protocol (MCP) should prioritize security\nawareness and training for developers, data scientists, and system administrators. Emphasizing zero-trust networking, token\nprotection, sandboxing, and safe coding practices is key to preventing tool poisoning, token theft, and command injection. Adopting\nmiddleware frameworks like MCP Guardian can help establish consistent authentication, rate limiting, WAF scanning, and detailed\nlogging across MCP-based communication. Additionally, leveraging community or official tool registries that cryptographically\nsign MCP servers ensures trusted, version-controlled deployments. Restricting privileges through container isolation and limiting\ntokens to minimal scopes further minimizes the potential impact of a compromise.\n\nIt is also recommended that organizations conduct regular code reviews, penetration tests, and WAF rule updates, enabling\nthem to adapt quickly to evolving threats and newly discovered vulnerabilities. By collaborating with the broader AI security\ncommunity—sharing best practices, threat intelligence, and potential protocol extensions—developers and operators can\ncollectively foster safer, standardized MCP usage. Through this combination of robust governance, technical safeguards, and\nongoing collaboration, agentic AI systems can flourish without compromising on security or transparency.",
  "The authors would like to thank the *[AI Anytime community](https://aianytime.net/)* for their invaluable assistance in validations and performance\nreviews.",
  "This study does not contain any studies with human or animal subjects performed by any of the authors.",
  "The authors declare that they have no conflicts of interest to this work.",
  "This study primarily presents a conceptual framework and does not involve newly generated or analyzed datasets. Hence, no\ndata are available for public archiving.",
  "Sonu Kumar: Conceptualization, Methodology, Framework development, Results, Review.\nAnubhav Girdhar: Literature review, Architecture, Visualization, Review.\nRitesh Patil: Writing, Diagram creation, Review & Editing, Investigation, Validation, Results.",
  "-----\n\nDivyansh Tripathi: Writing, Diagram creation, Review & Editing.",
  "[1] Anthropic. (2024). Introducing the Model Context Protocol (MCP). Retrieved from [https://www.anthropic.com/news/model](https://www.anthropic.com/news/model-context-protocol)\n\n[context](https://www.anthropic.com/news/model-context-protocol)   - protocol\n\n[2] Cloudflare. (2025). MCP Connectors on Cloudflare Workers. Retrieved from [https://blog.cloudflare.com/building](https://blog.cloudflare.com/building-ai-agents-with-mcp-authn-authz-and-durable-objects) - ai - agents \nwith   - mcp   - authn   - [authz](https://blog.cloudflare.com/building-ai-agents-with-mcp-authn-authz-and-durable-objects)   - and   - durable   - objects\n\n[3] OpenAI. (2023). OpenAI Plugins for ChatGPT: Architecture and Security Considerations. Retrieved from\n\n                       [https://openai.com/index/chatgpt](https://openai.com/index/chatgpt-plugins) plugins\n\n[4] Permit.io. (2024). Fine-Grained Role-Based Access Control for AI Tools. Retrieved from [https://www.permit.io/ai](https://www.permit.io/ai-access-control) - access \n[control](https://www.permit.io/ai-access-control)\n\n                                                                              \n[5] Invariant Labs. (2025). MCP Security Notification: Tool Poisoning Attacks. Retrieved from [https://invariantlabs.ai/blog/mcp](https://invariantlabs.ai/blog/mcp-security-notification-tool-poisoning-attacks)\n\nsecurity   - [notification](https://invariantlabs.ai/blog/mcp-security-notification-tool-poisoning-attacks)   - tool   - poisoning   - attacks\n\n[6] Hou, X., Zhao, Y., Wang, S., & Wang, H. (2025). Model Context Protocol (MCP): Landscape, Security Threats, and Future\n\nResearch Directions. Retrieved from [https://arxiv.org/html/2503.23278](https://arxiv.org/html/2503.23278)\n\n                                                                              \n[7] Equixly. (2025-03-29). MCP Servers: The New Security Nightmare. Retrieved from [https://equixly.com/blog/2025/03/29/mcp](https://equixly.com/blog/2025/03/29/mcp-server-new-security-nightmare/)\n\nserver   - new   - [security](https://equixly.com/blog/2025/03/29/mcp-server-new-security-nightmare/)   - nightmare/\n\n[8] Gupta, M. (2025-04-03). MCP Servers Are Not Safe. Retrieved from https://medium.com/data - [science](https://medium.com/data-science-in-your-pocket/mcp-servers-are-not-safe-bfbc2bb7aef8) - in - your - pocket/mcp \nservers   - are   - [not](https://medium.com/data-science-in-your-pocket/mcp-servers-are-not-safe-bfbc2bb7aef8)   - safe   - bfbc2bb7aef8\n\n[9] Sarig, D. (2025-03-24). The Security Risks of Model Context Protocol (MCP). Retrieved from\n\nhttps://www.pillar.security/blog/the   - [security](https://www.pillar.security/blog/the-security-risks-of-model-context-protocol-mcp)   - risks   - of   - model   - context   - pro to c o l -m c p\n\n                                                                            \n[10] Open WebUI. (2025). mcpo: A Simple, Secure MCP-to-OpenAPI Proxy Server. Retrieved from [https://github.com/open](https://github.com/open-webui/mcpo)\n\n[webui/mcpo](https://github.com/open-webui/mcpo)",
  "-----",
  "**Xiang Zhang** *[†∗]* **Yongfeng Zhang** *[‡∗]*\n\n*†* Boston University *‡* Rutgers University *∗* AIOS Foundation\nxz0224@bu.edu yongfeng.zhang@rutgers.edu",
  "The internet is undergoing a historical transformation from the “Internet of Websites” to the “Internet of AgentSites.” While traditional Websites served as the\nfoundation for information hosting and dissemination, a new frontier is emerging\nwhere AgentSites serve as the hubs of the internet, where each AgentSite hosts\none or more AI agents that receive tasks, address them, and deliver actionable solutions, marking a significant shift in the digital landscape and representing the next\ngeneration of online ecosystems. Under this vision, AIOS, the AI Agent Operating\nSystem, serves as the server for the development, deployment and execution of AI\nagents, which is a fundamental infrastructure for the Internet of Agentsites.\n\nIn this paper, we introduce AIOS Server, a runtime framework designed to host\nagents and support large-scale collaboration among decentralized agents. AIOS\nServer provides a standardized communication protocol leveraging the Model\nContext Protocol (MCP) and JSON-RPC to enable structured agent-agent or humanagent interactions. Each AIOS node operates as an independent server, capable of\nhosting and executing agents, while supporting peer-to-peer coordination without\nreliance on centralized orchestration.\n\nBased on AIOS Server, we further present the world’s first practically deployed\nInternet of Agentsites (AIOS-IoA), including AgentHub for agent registration\nand management as well as AgentChat for interactive communication, at `https:`\n`//planet.aios.foundation` . Furthermore, we prototype an agent discovery\nmechanism based on Distributed Hash Tables (DHT) and a Gossip protocol, which\nserves as the search engine for the internet of agentsites, enabling scalable and\nresilient agent registry and lookup on this new internet.\n\nOur evaluation demonstrates that AIOS Server achieves low-latency communication, efficient task delegation, and robust coordination in agent networks. This\nwork provides a practical foundation for building the Internet of Agentsites —\na new paradigm where autonomous agents become first-class citizens of the\nweb. The implementation is available at `https://github.com/agiresearch/`\n`AIOS.Server` and will be integrated into the AIOS main branch at `https:`\n`//github.com/agiresearch/AIOS` .",
  "The rapid progress of large language models (LLMs) has led to the emergence of autonomous\nagents capable of planning, reasoning, and interacting with humans and other agents in structured\nenvironments [ 17, 22, 21, 18 ]. These agents increasingly act beyond static response generation,\ndemonstrating memory, tool usage, and long-term task execution abilities. However, most existing\nagent-based systems remain confined within centralized platforms, limiting openness, interoperability,\nand scalability in multi-agent ecosystems.\n\n*†* Department of Computer Science, Metropolitan College, Boston University, Boston, MA 02215.\n\n\n-----\n\nTo address these limitations, this paper introduces AIOS Server — a runtime framework designed\nto host autonomous agents and facilitate large-scale agent communication across the internet. Each\nAIOS server hosts one or more autonomous AI agents, constituting an Agentsite — just like the\nway that Nginx server hosts Websites. Agentsites are further connected through the internet for\nhuman-agent and agent-agent communication, establishing the Internet of Agentsites, akin to the\nInternet of Websites that constitute the world wide web (WWW).\n\nAIOS Server provides a standardized communication protocol combining the MCP (Model Context\nProtocol) and JSON-RPC (JavaScript Object Notation - Remote Procedure Call), enabling structured\ninteractions between agents, humans, and external services. Each AIOS Server node operates\nas an independent execution environment, supporting dynamic agent deployment, peer-to-peer\ncommunication, and decentralized coordination without relying on centralized control.\n\nBuilding on the AIOS Server, we further present the world’s first practically deployed Internet of\nAgentsites (AIOS-IoA) — an open ecosystem where distributed agents, hosted across heterogeneous\nsites (AgentSites), interact and collaborate at internet scale. The AIOS-IoA architecture includes\nAgentHub for agent registration and management, and AgentChat for interactive human-agent\ncommunication, both available at `https://planet.aios.foundation` .\n\nTo enable scalable and resilient agent discovery across this decentralized network, we design and\nprototype an agent search mechanism based on Distributed Hash Tables (DHT) and a Gossip protocol.\nThis decentralized search engine provides efficient registry, lookup, and discovery functionalities,\nallowing agents to interact across distributed environments in a robust and fault-tolerant manner.\n\nIn summary, our key contributions are:\n\n- We propose AIOS Server, a decentralized runtime framework enabling structured communication\nand coordination among autonomous agents.\n\n- We implement the Internet of Agentsites (AIOS-IoA), providing the first practical deployment of\nan open, agent-centric web ecosystem.\n\n- We design and evaluate a DHT-based decentralized agent registration and discovery mechanism,\nsupporting scalable search and registry across agent networks.\n\n- We empirically evaluate AIOS Server in real-world deployment settings, demonstrating low-latency\ncommunication, efficient task delegation, and robust peer-to-peer coordination.\n\nThis work builds on recent advances in agent system\ninfrastructures [ 12, 7, 4 ], memory-enhanced LLM\nagents [ 21, 18 ], and agent communication protocols\n\n[ 1, 5, 11 ]. AIOS Server emphasizes modularity, interoperability, and robustness as foundational elements for open and scalable agent ecosystems. This\nwork contributes a practical foundation for building\nthe next-generation Internet of Agentsites, where autonomous agents operate as first-class citizens of the\nweb, capable of decentralized collaboration, search,\nand interaction.\n\nRecent works such as [ 3 ] explored the idea of the internet of agents as a multi-agent framework in terms\nof agent team up and collaboration. However, our\nwork is different in that we do not focus on agent Figure 1: Global view showing agentsites deteam up and collaboration, but on the physical, decen- ployed in London, Singapore, and Tokyo.\ntralized and real-world deployment of agents across\nthe globe (as shown in Figure 1), just like the physical Internet of Websites, where each website\nruns on a server at a certain location on the planet, and servers are connected through the internet to\nconstitute the world wide web (WWW). As a result, our infrastructure is denoted as the Internet of\nAgentsites for differentiation and for highlighting the analogy with the Internet of Websites.\n\n2\n\n\n-----",
  "**2.1** **LLM-Powered Agent Systems**\n\nRecent advances in large language models (LLMs) have enabled the creation of autonomous agents\nthat can interpret tasks, use tools, and maintain state over long interactions [ 17, 22, 25, 23 ]. These\nagents are no longer limited to single-turn responses—they can reason, remember, and act in structured\nworkflows [ 21, 18 ]. Frameworks such as Camel [ 9 ], OpenAGI [ 6 ], AIOS [ 12 ], AutoGen [ 19 ]\nand MetaGPT [ 7 ] define agent execution workflows and pipelines, where agents assume roles to\nwork on task decomposition and decision-making either individually or collaboratively. Memoryaugmented systems, including A-Mem and Workflow Memory [ 21, 18 ], provide persistent state\ntracking, improving contextual understanding and coherence. LLM-based agents now actively\nsupport applications in software development [ 15 ], web interaction [ 8, 4 ], and interactive simulations\n\n[16], extending their practical utility across domains.\n\n**2.2** **Agent Communication and Protocol Design**\n\nEffective coordination in agent systems requires clear, interpretable communication. Structured\ninteraction protocols—such as ReAct [ 22 ] and intent-based dialogue models—enable agents to\nplan, reason, and act based on conversational or environmental context. Recent works explore\ndebate and discussion as communication primitives to improve factuality and reasoning depth\n\n[ 5, 11, 3, 14, 19 ]. Message-oriented protocols like JSON-RPC are widely adopted to support\nstructured, machine-readable interaction between agent components or subsystems. Emerging\nplatforms such as AgentVerse [ 2 ], OS-Copilot [ 20 ], and Formal-LLM [ 10 ] emphasize modular\ncommunication layers, enabling agents to interact with tools, APIs, and other services in well-defined\nformats.\n\n**2.3** **Infrastructure and Decentralized Execution**\n\nTraditional AI agents often depend on centralized backends for registration, execution, and orchestration. This architecture limits scalability, introduces single points of failure, and reduces system\nadaptability. Recent efforts propose decentralized execution environments that distribute agent hosting and discovery across independent nodes. Platforms like AIOS (AI Agent Operating System)\n\n[ 12 ] highlight the need for developer-friendly, general-purpose agent development, hosting and\nexecution frameworks. Our system builds on this foundation by introducing a runtime that integrates\nagent communication, registration, and discovery into a fully distributed infrastructure. Security\nand evaluation remain key concerns. Benchmarking frameworks such as ASB [ 24 ] and GAIA [ 13 ]\nprovide standardized settings for assessing agent robustness, safety, and coordination capabilities at\nscale.",
  "AIOS Server operates as a layered architecture that facilitates communication between human users\nand autonomous agents. Figure 2 illustrates the overall architecture of the AIOS server system.\n\nEach AIOS server node operates independently, hosting agents and managing tasks. Core components\ninclude an agent manager, system monitor, task processor, and node client. AIOS server nodes interact\nwith an agent registry node for agent registration, discovery, task assignment, and health monitoring.\nThe registry node maintains the metadata of AIOS server nodes and provides an interface for users\nto interact and delegate tasks to the agents on various AIOS servers. This node registration design\nenhances the safety and security of the Internet of Agentsites since malicious agents can be discovered\nat the registry node to prevent harms to the network. Note that there can be more than one registry\nnodes in the network and each AIOS server node can decide which registry node(s) to register itself.\nFurthermore, a web-based monitoring interface at the registry provides real-time visibility into node\nperformance, geographic distribution, and system metrics.\n\nIn the following sections, we will introduce the agent communication, registration, discovery, and\nexecution protocols step by step.",
  "We start by designing the AIOS Communication Protocol, which facilitates structured interactions in\nagent-based systems, encompassing *(i)* Human-Agent Communication Protocol and *(ii)* Agent-Agent\nCommunication Protocol. To ensure interoperability and scalability, we implement based on the\n\n3\n\n\n-----\n\nFigure 2: AIOS Server architecture with layers for messaging, agents, and services.\n\nModel Context Protocol (MCP) v1.2.1 [1], a standardized framework for integrating Large Language\nModels (LLMs) with external tools and data sources.\n\nMCP follows a client-server architecture, where structured requests and responses enable seamless\nAI-driven workflows. LLMs often require access to external computation, structured data, and APIs\nto improve reasoning and task execution. Traditional integrations rely on *ad hoc* solutions, limiting\nscalability and security. MCP addresses these limitations by providing:\n\n- *Interoperability* : A standardized interface compatible with multiple LLM providers;\n\n- *Modular architecture* : Decoupled deployment of models, tools, and data sources;\n\n- *Secure data handling* : Controlled access aligned with infrastructure constraints.\n\nWe present an MCP v1.2.1 implementation for agent-agent and human-agent communication, using\nJSON-RPC for structured request-response exchanges. We explore its role in AI workflow orchestration, inter-agent collaboration, and scalable task delegation within the AIOS-IoA framework.\n\n**4.1** **Human-Agent Communication Protocol**\n\nThe Human-Agent Communication Protocol enables structured interaction between humans and AI\nagents, where human users can issue tasks, request information, and receive structured responses. The\nagents running on AIOS server operate as intelligent assistants that interpret user requests, perform\ncomputations, and return results in a standardized format.\n\nThe MCP-based communication workflow follows a structured protocol to ensure efficient and\nscalable interactions. The process consists of four key steps:\n\n1. *Task Initialization* : A structured JSON-RPC request is issued to an MCP-compliant agent.\n\n2. *Processing* : The agent interprets the request and executes the assigned task.\n\n3. *Response Generation* : The agent returns a structured response.\n\n4. *Iterative Refinement (Optional)* : The requester may refine the query, triggering further interactions.\n\nFigure 3 shows how human users interact with agents through standardized workflow. Requests are\nsubmitted as structured prompts and agents return contextual responses via the MCP protocol. The\ncommunication workflow possesses the following key features: 1) *Standardized Message Format*,\nwhich ensures structured, machine-readable exchanges; 2) *Context-Aware Execution*, where agents\nprocess requests with system and conversational context awareness; 3) *Multi-Turn Capability*, which\nsupports iterative refinements and follow-up queries; 4) *Progress and Error Handling*, which enables\nstructured status tracking and exception management. For better illustration, we provide examples of\nuser request in Appendix A.2, and examples of AI agent response in Appendix A.4.\n\n1 `https://modelcontextprotocol.io/introduction`\n\n4\n\n\n-----\n\nFigure 3: Human-Agent Protocol: Users communicate with AI agents using structured requests.\n\nFigure 4: Agent-Agent Protocol: Structured messaging between autonomous agents.\n\n**4.2** **Agent-Agent Communication Protocol**\n\nThe Agent-Agent Communication Protocol facilitates interactions between AI agents in a decentralized, distributed computing environment. Agents communicate dynamically to delegate tasks, share\ndata, and execute workflows collaboratively. The communication workflow consists of four key steps:\n\n1. *Agent Discovery* : Agents dynamically identify peers via a distributed registry.\n\n2. *Task Delegation* : An agent delegates a task to another capable agent.\n\n3. *Task Execution* : The receiving agent processes the task.\n\n4. *Response Handling* : The requesting agent integrates the results into its workflow.\n\nFigure 4 illustrates the agent-to-agent communication process. Agents exchange structured messages\nusing JSON-RPC to delegate and coordinate tasks. The protocol enables multi-stage workflows\nacross distributed nodes. The communication workflow possesses the following key features: 1)\n*Decentralized Agent Lookup*, which is realized through dynamic agent discovery via distributed\nregistries; 2) *Intent-Based Messaging*, where agents can specify their roles (query, delegate, collaborate) when sending messages through the internet; 3) *Hierarchical Task Execution*, which enables\nmulti-step workflows across agents; 4) *Message Routing*, which is naturally supported by IP-based\nmessage routing and delivery through the internet. Similarly, examples of task delegation request are\nprovided in Appendix B.2, and examples of agent response are provided in Appendix B.4.\n\n**4.3** **Human-Agent vs. Agent-Agent Communication**\n\nFor better understanding of the similarity and difference between the human-agent and agent-agent\ncommunication protocols, we summarize the key features of the two protocols in Table 1.\n\n**4.4** **Communication Examples**\n\nFigure 5 and Figure 6 illustrate the primary communication modes in the AIOS server system:\nagent-agent communication and human-agent communication.\n\n5\n\n\n-----\n\n|Feature|Human-Agent Communication|Agent-Agent Communication|\n|---|---|---|\n|Initiator|Human user|AI agent|\n|Message Flow|Request →Response|Request →Task Delegation →Response|\n|Interaction Type|Direct command execution|Autonomous collaboration|\n|Capabilities|Single-task execution|Multi-task delegation|\n|Routing|Direct|Dynamic peer-to-peer|\n|Use Case|AI assistants, task execution|Distributed AI, multi-agent workflows|\n\n\nTable 1: Comparison of human-agent and agent-agent communication protocols\n\nFigure 5 depicts the agent-to-agent messaging workflow. The initiating agent constructs a request\nvia the Request Builder and sends it through a protocol layer that supports RPC or WebSocket. The\nmessage is authenticated and encrypted before being routed to the target agent. The receiving agent\nprocesses the request and returns a structured response via the same secure channel.\n\nIn contrast, Figure 6 presents the human-agent interaction pipeline. Human users can interact through\nmultiple interfaces (API, CLI, or Web UI). The protocol layer formats, validates and enriches the\nmessage with context before passing it to the agent layer, where internal tools or LLMs execute the\ntask. Responses are formatted and routed back through the same protocol layer.\n\nThese interaction flows demonstrate the modularity and interoperability of the AIOS server communication protocols, supporting machine-to-machine coordination and user-facing automation.",
  "Scalable agent communication requires effective methods for agent discovery and management. AIOS\nserver addresses this challenge by implementing a decentralized agent registration and discovery\nmechanism, ensuring a safe, fault-tolerant, and adaptive agent ecosystem.\n\nBesides nodes that host agents, there are one or more agent registry nodes in the network. When\nan agent node is launched, it needs to register itself on one or more registry nodes, so that it can\nbe discovered and receive tasks in the network. After registration, each agent node periodically\nadvertises its availability using a structured metadata which includes the following fields:\n\n- `\"agent_id\"` : Unique identifier of the agent, typically in the format `namespace/agent_name` .\n\n- `\"description\"` : List of functional tags or capabilities offered by the agent.\n\n- `\"last_seen\"` : Timestamp (in UTC) indicating the most recent broadcast from the agent.\n\n\n\n6\n\n\n-----\n\nThis decentralized agent registration and discovery mechanism eliminates reliance on a single\nregistration service and improves system resilience, meanwhile enhance the safety of the network\nsince agent information can be found on one or more registry nodes to prevent malicious agents in\nthe network. We provide the design and implementation details of the mechanism in the following.\n\n**5.1** **AIOS Server as an Autonomous Node**\n\nAIOS Server functions as independent, self-regulating entities capable of dynamic task delegation\nand workload distribution. Each server node serves a dual role: it acts as both a *service provider*\nby hosting agents and a *dynamic client* by accessing external services. Each node supports: *Service*\n*Hosting* : AIOS server node exposes API endpoints for agent-based task execution, and *Remote*\n*Invocation* : Nodes delegate tasks to other AIOS agents when needed. This structure allows for\nadaptive workload balancing and efficient inter-node communication.\n\nWhen a node receives a task request, it follows the following *Agent Execution Workflow* :\n\n1. *Local Execution* : The task is executed locally if a suitable agent is available.\n\n2. *Task Delegation* : If no local agent is available, the task is delegated to another AIOS server node\nvia an *adaptive routing* mechanism.\n\n3. *Task Completion* : The designated agent processes the request and returns the result.\n\n4. *Result Integration* : The originating node receives and integrates the response.\n\nEach node periodically reports its state and active agents. Examples for the structure of AIOS node\nstatus report is shown in Appendix C.1, and the structure of AIOS node task assignment is shown in\nAppendix C.2.\n\nThe AIOS autonomous node architecture provides several benefits: 1) *Scalability* : Nodes operate\nindependently, enabling seamless system expansion; 2) *Fault Tolerance* : Failure of a single node does\nnot impact system functionality; 3) *Load Balancing* : Tasks are dynamically allocated based on node\ncapacity; 4) *Decentralized Execution* : AIOS nodes reduce reliance on static configurations.\n\n**5.2** **Decentralized Registration with Distributed Hash Table and Gossip Protocol**\n\nWe design and implement a decentralized registration system to support scalable, fault-tolerant\nagent discovery in the ecosystem. This system integrates a Distributed Hash Table (DHT) and a\nGossip-based synchronization protocol, enabling AIOS agents to register, discover, and monitor each\nother across a peer-to-peer (P2P) network without relying on centralized services.\n\n7\n\n\n-----\n\nFigure 7: Decentralized agent discovery and metadata propagation pipeline. The system operates\nin four stages: (1) agents are launched with local agent nodes; (2) metadata is stored and replicated\nacross neighboring nodes via DHT; (3) presence and state changes are propagated using the Gossip\nprotocol; (4) other agent nodes synchronize state and notify agents of network updates.\n\nThe prototype architecture integrates two core components to support decentralized agent registration\nand synchronization:\n\n- *Kademlia-based Distributed Hash Table (DHT)* : Provides structured, key-based metadata storage\nand lookup with logarithmic complexity *O* (log *n* ) across *n* nodes.\n\n- *Gossip-based Synchronization Protocol* : Enables periodic, peer-to-peer propagation of agent\npresence and status updates with eventual consistency guarantees.\n\nThese components are exposed to the AIOS server runtime through an abstraction layer, allowing\nseamless integration with higher-level agent workflows. This design separates protocol logic from\ntask execution, enabling modular deployment and interoperability.\n\nFigure 7 presents the end-to-end metadata flow in the decentralized system, highlighting the hybrid\ninterplay between DHT-based storage and Gossip-driven state dissemination. Note that although\nthere are different types of nodes in the system, all of the nodes are basically running the same AIOS\nserver code base, they are just taking different roles by activating different functionalities in the sever.\nThe architecture avoids centralized coordination while ensuring scalability, fault tolerance, and high\nmetadata availability under dynamic network conditions.\n\nThe overall process unfolds through four stages:\n\n1. *Agent Launch* : Agents are launched on local AIOS server node and publish their capability\nmetadata to the node.\n\n2. *Agent Registration on DHT Storage* : The DHT nodes store and replicate metadata across the DHT\noverlay for fault-tolerant lookup.\n\n3. *Gossip Dissemination* : Gossip nodes periodically exchange presence and status deltas with a\nrandom subset of peers.\n\n4. *State Synchronization* : Other agent nodes apply received updates and notify their agents of\ntopology or role changes.\n\nThis decentralized design enables robust, scalable agent discovery and coordination. It remains\nresilient to node churn and transient failures, offering a practical foundation for distributed, multiagent collaboration in real-world deployment environments. Detailed implementation logic and\nsource code examples are provided in Appendix D.\n\n8\n\n\n-----\n\n|Feature|Centralized AIOS|Decentralized AIOS|\n|---|---|---|\n|Agent Registration|Managed by a central server|Distributed across AIOS nodes|\n|Fault Tolerance|Single point of failure|Resilient via peer-to-peer network|\n|Scalability|Limited by central server capacity|Horizontally scalable|\n|Task Delegation|Static assignment|Dynamic inter-node routing|\n|Message Routing|Centralized relay|Multi-hop decentralized routing|\n|Example Use Case|Small-scale AI automation|Large-scale distributed AI collaboration|\n\n\nTable 2: Comparison of Centralized and Decentralized AIOS Architectures\n\n**5.3** **Functional Capabilities and Design Trade-offs**\n\nIn summary, the decentralized registration system supports the following key functions: 1) *Agent*\n*Registration and Lookup* : Agents publish their identity and capability metadata to the DHT using\nglobally unique keys. Peers can efficiently retrieve these records to locate suitable collaborators; 2)\n*Dynamic Node Management* : New nodes can join the system anytime, synchronize routing tables, and\nbegin participating in the registration network with minimal configuration; 3) *Fault-Tolerant Metadata*\n*Replication* : The system replicates agent records across multiple DHT nodes to maintain availability\nduring node failures or temporary disconnections; 4) *Real-Time Presence Dissemination* : Using the\nGossip protocol, AIOS nodes periodically exchange state information to track agent liveness and\ncapability updates in near real-time.\n\nThis design enables several important advantages of the system: 1) *High Availability* : No single point\nof failure—nodes can join or leave without disrupting global service; 2) *Scalability* : Performance\nscales logarithmically with network size due to the DHT structure; 3) *Self-Organization* : Nodes\nform an adaptive topology, requiring no centralized coordination; 4) *Resilient Status Tracking* :\nGossip-based synchronization ensures soft-state convergence even under unreliable connectivity.\n\n**5.4** **Distributed Agent Hub**\n\nFigure 8 illustrates the architecture of the distributed AgentHub system. Each AIOS agent node\nmaintains a set of local agents and a local cache, reporting its status to the central registry nodes.\nThe central registry nodes manage a global view of agent availability through a registry database\nand monitors node health using a dedicated checker. A synchronization manager propagates updates\nacross the network, ensuring consistency between nodes. This design supports decentralized agent\ndiscovery, fault-tolerant registration, and real-time coordination across geographically distributed\nnodes. Table 2 shows a comparison between the centralized client-server mode and the decentralized\ninternet of agentsite mode of AIOS.",
  "We evaluate the AIOS server communication framework under local and cloud-based deployments.\nExperiments assess three core metrics: latency, throughput, and agent registration efficiency in a\ndecentralized setting.\n\n**6.1** **Experimental Setup**\n\nAll tests were performed using structured JSON-RPC requests under controlled concurrency levels.\nTwo environments were evaluated:\n\n- **Local** : Simulated on a macOS machine (Apple M-series CPU) running multiple AIOS nodes.\n\n- **Cloud** : Deployed to a public endpoint at `https://planet.aios.foundation` .\n\nEach test involved 50, 100, and 200 total requests issued through 5, 10, and 20 concurrent users,\nrespectively.\n\n**6.2** **Communication Performance Results**\n\nAIOS server achieved 100% response success in all scenarios. Latency scaled predictably with load,\nwhile throughput increased steadily. The system demonstrated reliable performance in both local and\ncloud environments. Despite infrastructure differences, latency remained under 200ms in all cases.\nThroughput increased with load, reaching up to 229 requests per second in the cloud deployment.\nThis confirms the framework’s scalability and consistency across platforms.\n\n9\n\n\n-----\n\n|Environment|Load|Avg. Latency (s)|Throughput (req/s)|\n|---|---|---|---|\n|Local Local Local|50 reqs 100 reqs 200 reqs|0.061 0.104 0.165|80.2 116.9 163.1|\n|Cloud Cloud Cloud|50 reqs 100 reqs 200 reqs|0.143 0.145 0.145|32.2 100.0 229.3|\n\n\nTable 3: AIOS server communication performance under local and cloud deployments\n\n**6.3** **Node Discovery Performance**\n\nThe agent discovery protocol was tested using distributed AIOS nodes and evaluated based on\nregistration latency. Across 3, 5, and 7 nodes, all agents successfully registered within milliseconds.\nAverage registration time remained consistent at 1ms, with a maximum delay of 2ms. These results\nconfirm the efficiency and stability of the DHT and gossip-based synchronization mechanism. Overall,\nthe AIOS server protocol demonstrates low-latency, high-throughput communication and rapid agent\nregistration across diverse environments. These findings confirm its robustness for decentralized\nagent communication.\n\n**6.4** **System Visualization and Demonstration**\n\nThis section presents interface-level illustrations of the AIOS server infrastructure. These visuals\nsupport the system’s functional claims, highlighting its distributed design, agent orchestration, and\nhuman-agent interaction.\n\n10\n\n\n-----\n\n**6.4.1** **Global Node Distribution**\n\nFigures 9a and 9b display the global distribution of active AIOS nodes. Each point represents an\nautonomous server instance hosting LLM agents. These nodes are geographically dispersed and form\na decentralized agent network.\n\n\n(a) Pacific view showing active nodes in San Francisco.\n\n\n(b) Eurasian view showing nodes in London, Singapore, and Tokyo.\n\n\nFigure 9: Global nodes map showing nodes at different locations across the Internet of Agentsites.\n\n**6.4.2** **Node Overview Interface**\n\nFigure 10 provides a snapshot of the AIOS node dashboard. Each node card shows its current resource\nusage, platform type, and available agents. All nodes are synchronized and support real-time task\nexecution.\n\nFigure 10: Distributed node dashboard showing CPU/memory usage, platform type, agent availability.\n\n**6.4.3** **Node Detail and Interaction Interface**\n\nFigure 11 presents the detailed view of an individual AIOS node. Users can inspect system performance, choose specific agents, and issue tasks to the agents hosted on the node via the UI.\n\n11\n\n\n-----\n\nFigure 11: Interface of the San Francisco node with task input, resource usage, and agent selector.\n\n**6.4.4** **Task Execution and Result Logging**\n\nFigure 12 shows a completed task handled by the `academic_agent` . The interface logs the complete JSON-RPC request and the corresponding AI-generated response. This validates the agent’s\nautonomous reasoning capability and system traceability.",
  "This work presents AIOS server, a foundational infrastructure for the internet of agentsites, where\neach server node can host agents to enable task solving at global scale. One key advantage of AIOS\nserver lies in its modular, message-oriented architecture, which supports flexible agent orchestration\nacross heterogeneous environments. The structured communication protocol (MCP + JSON-RPC)\nfurther facilitates interoperability between agents and external services. Evaluation demonstrates\nthat the AIOS server protocol achieves low latency, high throughput, and efficient decentralized\nagent registration under local and cloud-based deployments. These results validate its viability for\nreal-world applications, particularly in single-agent or loosely coupled multi-agent systems.\n\nFuture development will focus on extending AIOS to support multi-agent task orchestration and\nimproving its robustness in more complex, failure-prone settings, including: 1) *Inter-node commu-*\n*nication optimization* : Reducing serialization overhead and supporting asynchronous messaging to\nimprove throughput under high concurrency; 2) *Adaptive load balancing* : Developing real-time node\nprofiling and routing strategies based on resource availability and agent capabilities; 3) *Resilience*\n*under partial failure* : Introducing fallback mechanisms, timeout-based task migration, and state\ncheckpointing to improve fault tolerance; 4) *DHT registry validation at scale* : Integrating the DHT +\nGossip registry into a larger distributed deployment with hundreds of nodes and evaluating consistency and convergence under dynamic network conditions; 5) *Security and trust modeling* : Designing\nlightweight authentication and authorization schemes for peer-to-peer agent communication.\n\n12\n\n\n-----\n\nFigure 12: Agent response log showing a completed task with structured output.",
  "[1] Chi-Min Chan et al. Chateval: Towards better llm-based evaluators through multi-agent debate.\n\narXi v preprin t arXiv:2308.07201, 2023.\n\n[2] Weize Chen et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent\nbehaviors. arXi v preprin t arXiv:2305.12112, 2023.\n\n[3] Weize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian, Chenyang Zhao, Cheng Yang,\nRuobing Xie, Zhiyuan Liu, and Maosong Sun. Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence, 2024.\n\n[4] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and\nYu Su. Mind2web: Towards a generalist agent for the web. In Advance s i n Neura l Information\n\nProcessin g System s (NeurIPS), volume 36, 2024.\n\n[5] Yilun Du et al. Improving factuality and reasoning in language models through multiagent\ndebate. arXi v preprin t arXiv:2305.14325, 2023.\n\n[6] Yingqiang Ge, Wenyue Hua, Kai Mei, Juntao Tan, Shuyuan Xu, Zelong Li, and Yongfeng\nZhang. Openagi: When llm meets domain experts. Advance s i n Neura l Informatio n Processing\nSystems, 36, 2023.\n\n[7] Sirui Hong et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv\n\npreprin t arXiv:2308.00352, 2023.\n\n[8] Iat Long Iong, Xiao Liu, Yuxuan Chen, Hanyu Lai, Shuntian Yao, Pengbo Shen, Hao Yu, Yuxiao\nDong, and Jie Tang. Openwebagent: An open toolkit to enable web agents on large language\nmodels. In Proceeding s o f th e 62n d Annua l Meetin g o f th e Associatio n fo r Computational\nLinguistic s (Volum e 3 : Syste m Demonstrations), pages 72–81, 2024.\n\n13\n\n\n-----\n\n[9] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel:\nCommunicative agents for\" mind\" exploration of large language model society. Advance s in\nNeura l Informatio n Processin g Systems, 36:51991–52008, 2023.\n\n[10] Zelong Li et al. Formal-llm: Integrating formal language and natural language for controllable\nllm-based agents. arXi v preprin t arXiv:2402.00798, 2024.\n\n[11] Tian Liang et al. Encouraging divergent thinking in large language models through multi-agent\ndebate. arXi v preprin t arXiv:2305.19118, 2023.\n\n[12] Kai Mei, Xi Zhu, Wujiang Xu, Wenyue Hua, Mingyu Jin, Zelong Li, Shuyuan Xu, Ruosong Ye,\nYingqiang Ge, and Yongfeng Zhang. Aios: Llm agent operating system, 2024.\n\n[13] Grégoire Mialon et al. Gaia: A benchmark for general ai assistants. arXi v preprint\narXiv:2311.12983, 2023.\n\n[14] Xianghe Pang, Shuo Tang, Rui Ye, Yuxin Xiong, Bolun Zhang, Yanfeng Wang, and Siheng\nChen. Self-alignment of large language models via multi-agent social simulation. In ICLR\n202 4 Worksho p o n Larg e Languag e Mode l (LLM ) Agents, 2024.\n\n[15] Chen Qian et al. Communicative agents for software development. arXi v preprint\narXiv:2307.07924, 2023.\n\n[16] Guanzhi Wang et al. Voyager: An open-ended embodied agent with large language models.\n\narXi v preprin t arXiv:2305.16291, 2023.\n\n[17] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen,\nJiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous\nagents. Frontier s o f Compute r Science, 18(6):186345, 2024.\n\n[18] Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory.\n\narXi v preprin t arXiv:2409.07429, 2024.\n\n[19] Qingyun Wu et al. Autogen: Enabling next-gen llm applications via multi-agent conversation\nframework. arXi v preprin t arXiv:2308.08155, 2023.\n\n[20] Zhiyong Wu et al. Os-copilot: Towards generalist computer agents with self-improvement.\n\narXi v preprin t arXiv:2402.07456, 2024.\n\n[21] Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. A-mem:\nAgentic memory for llm agents. arXi v preprin t arXiv:2502.12110, 2025.\n\n[22] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. arXi v preprin t arXiv:2210.03629,\n2022.\n\n[23] Dingyao Yu, Kaitao Song, Peiling Lu, Tianyu He, Xu Tan, Wei Ye, Shikun Zhang, and Jiang\nBian. Musicagent: An ai agent for music understanding and generation with large language\nmodels, 2023.\n\n[24] Hanrong Zhang, Jingyuan Huang, Kai Mei, Yifei Yao, Zhenting Wang, Chenlu Zhan, Hongwei\nWang, and Yongfeng Zhang. Agent security bench (asb): Formalizing and benchmarking attacks\nand defenses in llm-based agents. arXi v preprin t arXiv:2410.02644, 2024.\n\n[25] Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu,\nJintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang,\nNingyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya Sachan. Agents: An open-source\nframework for autonomous language agents, 2023.\n\n14\n\n\n-----",
  "**A.1** **Message Schema Explanation**\n\nThe following explains each field of the human-agent communication request:\n\n- `\"jsonrpc\"` : Protocol version identifier (e.g., \"2.0\").\n\n- `\"id\"` : Unique identifier used to match requests and responses.\n\n- `\"method\"` : Name of the remote method invoked (e.g., `\"aios/delegateTask\"` ).\n\n- `\"params\"` : Contains task-specific parameters:\n\n**–** `\"sender\"` : Entity initiating the request.\n\n**–**\n`\"recipient\"` : Target AI agent receiving the request.\n\n**–**\n`\"messages\"` : User-provided messages forming the input.\n\n**–** `\"maxTokens\"` : Token limit for the response output.\n\n**A.2** **Example: User Request**\n\n\n\n**A.3** **Message Schema Explanation (Response)**\n\n- `\"jsonrpc\"` : Protocol version.\n\n- `\"id\"` : Corresponds to the original request ID.\n\n- `\"result\"` : Contains the response from the agent:\n\n**–** `\"sender\"` : Responding agent.\n\n**–**\n`\"recipient\"` : Original human user.\n\n**–** `\"content\"` : Task result or response content.\n\n**–** `\"model\"` : Language model used to generate response.\n\n**–**\n`\"stopReason\"` : Indicates how response was terminated.\n\n15\n\n\n-----\n\n**A.4** **Example: AI Agent Response**\n\n\n\n16\n\n\n-----",
  "**B.1** **Message Schema Explanation (Request)**\n\n- `\"jsonrpc\"` : Protocol version.\n\n- `\"id\"` : Identifier of the task.\n\n- `\"method\"` : Operation being requested (e.g., `\"aios/delegateTask\"` ).\n\n- `\"params\"` :\n\n**–** `\"intent\"` : Purpose of the request (e.g., data extraction).\n\n**–** `\"sender\"` : Requesting agent and role.\n\n**–**\n`\"recipient\"` : Target agent and role.\n\n**–** `\"task\"` : Task metadata and parameters.\n\n**B.2** **Task Delegation Request**\n\n\n\n**B.3** **Message Schema Explanation (Response)**\n\n- `\"jsonrpc\"` : Protocol version.\n\n- `\"id\"` : Same ID as the original request.\n\n- `\"result\"` :\n\n**–** `\"sender\"` : Agent responding to the task.\n\n**–**\n`\"recipient\"` : Agent that issued the task.\n\n**–** `\"content\"` : Result of the task, including output.\n\n**–** `\"isError\"` : Indicates if the task failed.\n\n17\n\n\n-----\n\n**B.4** **Agent Response**\n\n\n\n18\n\n\n-----",
  "**C.1** **AIOS Node Status Report**\n\n**Field Explanation:**\n\n- `\"node_id\"` : Unique identifier for the node.\n\n- `\"node_name\"` : System name of the node.\n\n- `\"timestamp\"` : UTC timestamp of the report.\n\n- `\"system_info\"` : Hardware and usage statistics.\n\n- `\"available_agents\"` : List of agents currently deployed on the node.\n\n\n\n**C.2** **AIOS Task Assignment Format**\n\n**Field Explanation:**\n\n- `\"task_id\"` : Task identifier.\n\n- `\"assigned_agent\"` : Name of the agent responsible for the task.\n\n- `\"status\"` : Current execution status (e.g., running, completed).\n\n\n\n\n19\n\n\n-----",
  "This section presents key excerpts from the prototype implementation of AIOS’s decentralized agent\nregistration and discovery system. The system combines a Kademlia-based Distributed Hash Table\n(DHT) for structured key-value storage with a lightweight Gossip protocol for periodic presence\nsynchronization.\n\n**D.1** **DHT Class for Agent Registration**\n\nThis class handles agent metadata registration into the DHT overlay, enabling global discoverability\nvia unique identifiers. Metadata such as node address and timestamp is encoded before being\ndistributed.\n\nListing 1: DHT Agent Registry Class\n```\nclass DHT:\n   def __init__(self, ip, port, node_id=None, k=20):\n     self.node_id = node_id or NodeID ()\n     self.ip = ip\n     self.port = port\n     self.local_node = Node(self.node_id, ip, port)\n     self.routing_table = RoutingTable (self.node_id, k)\n     self.data_store = {}\n   def register_agent (self, agent_id, metadata):\n     key = f \"agent :{ agent_id}\"\n     metadata[ \"last_update\" ] = time.time ()\n     metadata[ \"node_id\" ] = str (self.node_id)\n     metadata[ \"node_ip\" ] = self.ip\n     metadata[ \"node_port\" ] = self.port\n     return self.store(key, metadata)\n   def find_agent(self, agent_id):\n     key = f \"agent :{ agent_id}\"\n     return self.lookup(key)\n\n```\n**D.2** **Gossip Protocol for Presence Synchronization**\n\nThis module implements a gossip-based presence tracking protocol. Each node periodically propagates its knowledge of peer agents to a sampled subset of neighbors, balancing coverage and\noverhead.\n\nListing 2: Presence Gossip Protocol\n```\nclass GossipProtocol(asyncio. DatagramProtocol ):\n   def __init__(self, node_id, port =8001):\n     self.node_id = node_id\n     self.port = port\n     self.peers = {}\n     self.message_cache = {}\n     self.callbacks = {}\n   def _propagate_message (self, message):\n     if message.ttl <= 1:\n        return\n     new_message = GossipMessage (\n        message.sender_id,\n        message.message_type,\n        message.data,\n        message.timestamp,\n        message.ttl - 1\n     )\n     live_peers = [p for p in self.peers.values ()\n             if p[ \"state\" ] != NodeState.DEAD]\n\n```\n20\n\n\n-----\n\n```\n     if not live_peers:\n        return\n     target_count = min ( len (live_peers), max (3, int ( len (live_peers)\n        ** 0.5)))\n     targets = random.sample(live_peers, target_count)\n     for peer in targets:\n        addr = (peer[ \"ip\" ], peer[ \"port\" ])\n        self._send_message (new_message, addr)\n\n```\n**D.3** **Agent Directory Service**\n\nThis service provides high-level API access for agent capability-based queries. It relies on the\nunderlying gossip protocol to maintain updated lists of agents and their advertised features.\n\nListing 3: Agent Presence Directory\n```\nclass GossipAgentDirectoryService :\n   def __init__(self, node_id=None, host= \"127.0.0.1\", port =8001,\n          seed_nodes=None):\n     self.node_id = node_id or str (uuid.uuid4 ())\n     self. presence_service = AgentPresenceService (self.node_id,\n       host, port)\n     self.seed_nodes = seed_nodes or []\n   async def start(self):\n     await self. presence_service .start ()\n     for node_id, host, port in self.seed_nodes:\n        self. presence_service .add_peer(node_id, host, port)\n   def register_agent (self, agent_id, capabilities=None):\n     return self. presence_service . register_agent (agent_id,\n       capabilities)\n   def find_agents_by_capability (self, capability):\n     agents = self. presence_service . get_agents_by_capability (\n       capability)\n     return [a.to_dict () for a in agents]\n\n```\n**D.4** **Gossip Integrator for AIOS System**\n\nThis wrapper class integrates the gossip-based directory service into the broader AIOS runtime. It\nensures that the decentralized components are initialized correctly and available via a consistent\ninterface.\n\nListing 4: Gossip Service Integrator\n```\nclass GossipIntegrator :\n   def __init__(self, config=None):\n     self.config = config or Config ()\n     self.service = None\n     self.node_id = self.config.get( \"p2p.node_id\", default=None)\n     self.host = self.config.get( \"p2p.gossip.host\", default= \"\n       127.0.0.1\" )\n     self.port = self.config.get( \"p2p.gossip.port\", default =8001)\n   async def initialize(self):\n     self.service = GossipAgentDirectoryService (\n        node_id=self.node_id,\n        host=self.host,\n        port=self.port,\n        seed_nodes=self.seed_nodes\n     )\n     await self.service.start ()\n\n```\n21\n\n\n-----\n\n```\ndef register_agent (self, agent_id, capabilities=None):\n   if not self.service:\n     return False\n   return self.service. register_agent (agent_id, capabilities )\n\n```\n22\n\n\n-----",
  "*Adversarial AI Security reSearch (A2RS)*\n\n*Intuit*\n\nidan habler@intuit.com",
  "*Agentic AI Security*\n*DistributedApps.ai*\n\nken.huang@distributedapps.ai",
  "*Google Cloud*\n*Google*\n\npskulkarni@google.com",
  "*Proactive Security*\n*Amazon Web Services*\n\nvineesa@amazon.com\n\n\n***Abstract*** **—As Agentic AI systems evolve from basic workflows**\n**to complex multi-agent collaboration, robust protocols such as**\n**Google’s Agent2Agent (A2A) become essential enablers. To foster**\n**secure adoption and ensure the reliability of these complex**\n**interactions, understanding the secure implementation of A2A**\n**is essential. This paper addresses this goal by providing a**\n**comprehensive security analysis centered on the A2A protocol.**\n**We examine its fundamental elements and operational dynamics,**\n**situating it within the framework of agent communication**\n**development. Utilizing the MAESTRO framework, specifically**\n**designed for AI risks, we apply proactive threat modeling to**\n**assess potential security issues in A2A deployments, focusing on**\n**aspects such as Agent Card management, task execution integrity,**\n**and authentication methodologies.**\n**Based on these insights, we recommend practical secure**\n**development methodologies and architectural best practices**\n**designed to build resilient and effective A2A systems. Our analysis**\n**also explores how the synergy between A2A and the Model Context**\n**Protocol (MCP) can further enhance secure interoperability.**\n**This paper equips developers and architects with the knowledge**\n**and practical guidance needed to confidently leverage the A2A**\n**protocol for building robust and secure next-generation agentic**\n**applications.**\n***Index Terms*** **—Agentic AI, Google Agent2Agent, Agent-to-**\n**Agent Communication, A2A Protocol, Security, Threat Modeling,**\n**MAESTRO, MCP, Interoperability, Secure Development**\n\nI. I NTRODUCTION\n\nThe emergence of intelligent, autonomous agents marks a\npivotal shift in how AI systems are developed, deployed, and\nscaled. As these agents increasingly interact across organizational and technological boundaries, the need for secure,\ninteroperable communication becomes critical. This paper\nbegins by exploring the foundation of this transformation: the\nrise of Agentic AI and the protocols that enable it.\n\n1 These authors contributed equally to this work.\n2 This work is not related to the author’s position at Intuit\n3 This work is not related to the author’s position at DistributedApp.ai\n5 This work is not related to the author’s position at Google\n4 This work is not related to the author’s position at Amazon Web Services.\n\n\n*A. The Rise of Agentic AI and the Need for Secure Interoper-*\n*ability*\n\nAs artificial intelligence systems evolve from isolated,\ntask-specific models to dynamic, multi-agent ecosystems, we\nare witnessing the emergence of Agentic AI—intelligent\nagents capable of autonomous decision-making, tool use, and\ncollaboration with other agents and humans. These agents\ndo not merely respond to prompts; they initiate actions,\ndelegate subtasks, coordinate with peers, and adapt to new\ngoals in real time. From AI research assistants that plan\nliterature reviews to supply chain agents negotiating logistics\nacross organizations, agentic AI is rapidly becoming the\nbackbone of next-generation intelligent applications. However, as these agents interact and compose workflows across\norganizational, geographical, and trust boundaries, the need\nfor secure, standardized interoperability becomes paramount.\nWithout a shared protocol for identity, authentication, task\nexchange, and auditability, agent interactions are prone to\nfragmentation, redundancy, and most critically—security vulnerabilities. Threats such as impersonation, data exfiltration,\ntask tampering, and unauthorized privilege escalation can\nquickly arise in loosely governed agent ecosystems. To address\nthis, secure interoperability protocols like Google’s Agent-toAgent (A2A) specification offer a promising foundation. A2A\nprovides a declarative, identity-aware framework for enabling\nstructured, secure communication between agents—whether\nhuman-authored or AI-powered. Such protocols enable agents\nto share descriptive information about their capabilities, which\nis essential for facilitating effective interaction, discovery, and\ninteroperability. This exchanged information, especially when\nreceived from potentially untrusted peers, must be handled\nwith care and rigorously validated to prevent manipulation\ntechniques like prompt injections. Realizing the full potential\nof agentic AI will depend not only on such protocol standards,\nbut on robust implementations, rigorous threat modeling, and\ncontinuous security adaptation. To aid developers in building\n\n\n-----\n\nsecure systems, we also offer a repository containing secure\nA2A coding examples [1] .\nThis paper explores the security architecture of A2A, identifies critical risks through the MAESTRO threat modeling lens\n\n[1], [2] and proposes mitigation strategies and implementation\nbest practices to ensure that agentic systems remain not just\nintelligent—but trustworthy by design.\n\n*B. Google A2A: A Foundational Protocol and its Context*\n\nThe Agent-to-Agent (A2A) protocol, introduced by Google,\nrepresents a significant step forward in enabling structured,\nsecure, and interoperable communication between autonomous\nagents. Designed with composability and trust in mind, A2A\nallows agents to discover each other via standardized AgentCards, authenticate using modern cryptographic protocols,\nand exchange tasks in a declarative, auditable manner. Its\narchitecture reflects the growing demand for modular AI\nsystems that can scale across organizations, tools, and domains—while remaining adaptable to both human and machinedriven workflows. A2A’s emergence is timely, as the AI\necosystem increasingly shifts toward open-ended, agent-driven\napplications in areas like research, enterprise automation,\ncybersecurity, and scientific collaboration. Positioned at the\nintersection of protocol engineering and AI orchestration,\nA2A provides the plumbing needed to build reliable, multiagent ecosystems—where interoperability and security are not\nafterthoughts, but built-in foundations.\n\n*C. Paper Objectives: Analyzing A2A, Proposing Enhancements,*\n*and Guiding Secure Implementation*\n\nThis paper sets out to examine the Agent-to-Agent (A2A)\nprotocol in both its theoretical design and practical deployment,\nwith the overarching goal of enabling secure and trustworthy\nagentic AI systems. First, we analyze the A2A protocol\nthrough the lens of the MAESTRO threat modeling framework,\nidentifying a comprehensive set of risks that emerge in multiagent environments—including spoofing, task replay, privilege\nescalation, and prompt injection. Building on this analysis,\nwe propose targeted security enhancements to strengthen the\nprotocol and its implementations, ranging from cryptographic\ncontrols and schema enforcement to secure session handling\nand fine-grained authorization. Finally, the paper outlines a set\nof implementation best practices for building hardened A2A\nservers and clients, offering guidance on secure communication,\nauthentication workflows, logging, and abuse prevention. By\nbridging the gap between protocol specification and real-world\ndeployment, this paper aims to equip developers, researchers,\nand architects with actionable insights for designing secure-bydefault agentic ecosystems.\n\nII. L ITERATURE R EVIEW\n\n*A. Agent to Agent Communication Literature Review*\n\nAgent-to-agent communication is fundamental to multi-agent\nsystems (MAS), enabling autonomous entities to coordinate\n\n1 [https://github.com/kenhuangus/a2a-secure-coding-examples](https://github.com/kenhuangus/a2a-secure-coding-examples)\n\n\nand collaborate to solve complex problems [3]. Efficient and\nsecure communication protocols are critical for sophisticated\nagent interactions across heterogeneous platforms. This review\nsynthesizes the historical development, protocols, theoretical\nfoundations, applications, challenges, and future directions of\nagent communication, with a focus on secure A2A multi-agent\n\nsystems.\nEarly agent communication languages (ACLs) emerged\nfrom the Knowledge Sharing Effort (KSE) in the early 1990s\n\n[4]. KQML, the pioneering ACL, defined a multi-layered\narchitecture including content, message, and communication\nlayers using speech act theory through performatives to specify\nmessage intentions [3]. Building upon KQML, the FIPA ACL\nsought more rigor with a semantic framework based on modal\nlogic, feasibility preconditions, and rational effects [5], [6].\nThese ACLs are based on speech act theory, formal semantics,\nand the Belief-Desire-Intention (BDI) model [7].\nKey protocols include KQML with its LISP-like syntax,\nand FIPA ACL which refines message structure and semantics\nwith defined preconditions and outcomes [3]. Protocols define\ninteraction sequences between agents, including request, query,\ncontract net, auction, and subscription [5]. Recent efforts like\nGoogle’s A2A protocol aim to enable seamless interoperability\nbetween AI agents across different frameworks by standardizing\ntask negotiation, capability sharing, and secure communication\n\n[8].\nCommunication architectures in MAS can be direct (full,\npartial, or dynamic networks) or mediated (facilitator-based,\nblackboard systems, or global control) [9]. Multi-agent reinforcement learning (MARL) allows agents to learn when, how,\nand what to communicate using differentiable, reinforcement,\nsupervised, or regularized communication learning methods\n\n[10].\nApplications of agent communication range across enterprise knowledge management, customer service, supply chain\nmanagement, healthcare, smart grids, autonomous vehicles,\ne-commerce, and disaster response [5], [9]. Security, trust,\nand the potential manipulation of feedback loops present\ninherent challenges. Integration with existing standards, scaling\ncommunication for efficiency, and handling non-stationarity in\nlearning processes are open research areas [4], [5], [11].\nFuture research should focus on multimodal communica\ntion, structured communication networks, robust centralized\ncomponents, interpretable emergent languages, and integration\nwith LLMs. Initiatives like A2A and MCP are aiming for\nprotocol convergence for standardized agent communication,\nalthough many questions regarding how this may emerge are\nstill unanswered. Effective communication remains crucial for\n\nthe success of agent systems. Security must be considered\nthroughout their evolution.\n\n*B. Traditional PKI and Transport Security*\n\nEarly implementations of secure agentic systems leveraged\npublic key infrastructure (PKI), with each agent provisioned\na certificate and private key signed by a trusted certificate\nauthority. This approach, frequently combined with SSL/TLS\n\n\n-----\n\nat the transport layer, guaranteed encrypted channels for agent\ncommunication [12]. While PKI solutions remain fundamental\nto securing multi-agent messages, they often required complex\ncertificate management, especially as agent populations grew\nor changed dynamically. Agent platforms such as JADE (Java\nAgent DEvelopment Framework) attempted to simplify this via\nsecurity extensions that integrated PKI-based authentication\n\n[13].\n\n*C. SOAP, WS-Security, and Enterprise Service Buses*\n\nPrior to the widespread adoption of RESTful services,\nenterprise-grade agentic applications often employed SOAP\nas the foundational protocol. The WS-Security suite [14]\nprovided message-level encryption, signatures, and token-based\nauthentication for agent-to-service or agent-to-agent interactions. These frameworks were robust but verbose; they were\nwell-suited to closed enterprise environments yet burdensome\nfor lightweight agentic systems or real-time AI workflows. Still,\nthey established fundamental principles of end-to-end message\nconfidentiality, integrity, and non-repudiation at a time when\nmicroservices and ephemeral containers were not standard\npractice.\n\n*D. OAuth 2.0, JSON Web Tokens (JWT), and Service Accounts*\n\nWith the shift to cloud and user-centric web applications,\nOAuth 2.0 became the de facto framework for delegated\nauthorization [15]. However, OAuth 2.0’s flows primarily\nfocus on interactive user login. This made it challenging for\nautonomous agents to securely obtain and renew tokens without\nuser intervention. JSON Web Tokens (JWT) [16] introduced\na lightweight format for securely transmitting agent identity\nand privileges. Many practitioners adopted service accounts or\n“robot accounts” with static credentials to simulate user roles\n\nin an agent scenario, though this introduced key-rotation and\nsecret-management complexities.\n\n*E. Zero-Trust Architectures and BeyondCorp*\n\nAs multi-agent systems expanded across hybrid cloud\nenvironments, Zero-Trust principles emphasized continuous\nvalidation of identity and authorization, even for internal\nrequests [17]. Solutions like Google’s BeyondCorp architecture\napplied these principles, requiring every agent or service to\nauthenticate with strong, verifiable credentials irrespective of\nnetwork location [18]. This paved the way for “never trust,\nalways verify” policies, which align closely with the needs of\nautonomous AI agents that may operate across heterogeneous\nnetwork boundaries.\n\n*F. Limitations of Historical Approaches*\n\nThe historical approaches to securing agentic AI systems\nshared several common limitations:\n\n*•* **Manual Credential Management:** Significant manual effort\nwas required to provision, rotate, and revoke agent credentials.\n\n*•* **Adapting User-Centric Models:** Substantial work was\nneeded to adapt inherently user-centric security workflows\nto machine-only environments.\n\n\n\n*•* **Fragmented Implementations:** The lack of uniform standards or cloud-native support for fully autonomous agents\nresulted in inconsistent security implementations.\n\n*•* **Scaling Challenges:** As agent populations grew, the operational complexity of maintaining secure communication\nincreased exponentially.\nThese early innovations and their limitations highlighted the\nfundamental importance of establishing robust identity and trust\nmechanisms for distributed AI, while also underscoring the\nneed for simpler, more automated credential exchange processes\nspecifically designed for autonomous agent interactions.\n\nIII. U NDERSTANDING G OOGLE ’ S A2A P ROTOCOL\n\n*A. Protocol Overview*\n\nThe A2A protocol facilitates communication between two\nprimary types of agents:\n\n*•* **Client Agent:** Responsible for formulating and communicating tasks.\n\n*•* **Remote Agent:** Responsible for acting on those tasks to\nprovide information or take action.\nBuilt on established web standards including HTTP, JSONRPC, and Server-Sent Events (SSE), A2A prioritizes compatibility with existing systems while maintaining a security-first\napproach. Its design follows several key principles:\n\n*•* **Agentic-first:** Agents operate independently and communicate explicitly to exchange information, without shared\nmemory or tools by default.\n\n*•* **Standards-compliant:** Utilization of widely adopted web\ntechnologies minimizes developer friction.\n\n*•* **Secure by default:** Integrated authentication and authorization measures safeguard sensitive data and transactions.\n\n*B. Communication Flow*\n\nThe A2A protocol defines a structured communication flow\nbetween agents that consists of several stages:\n\n1) **Discovery:** The client fetches the target agent’s\nAgent Card (from /.well-known/agent.json ) via\nHTTPS to learn about its capabilities, endpoint, and\nauthentication requirements.\n2) **Initiation:** After authenticating using a method specified\nin the Agent Card, the client sends a JSON-RPC request\nover HTTPS to the server’s a2aEndpointUrl using\none of two methods:\n\n*•* tasks.send : Used for potentially synchronous tasks\nor initiating tasks where immediate streaming isn’t\nrequired.\n\n*•* tasks.sendSubscribe : Used for long-running\ntasks requiring streaming updates, establishing a persistent HTTPS connection for Server-Sent Events (SSE).\n3) **Processing & Interaction:**\n\n*•* *Non-Streaming (* *tasks.send* *):* The server processes\nthe task and returns the final Task object in the HTTP\n\nresponse.\n\n*•* *Streaming* *(* *tasks.sendSubscribe* *):* The\nserver sends SSE messages over the persistent\nconnection, including TaskStatusUpdateEvent\n\n\n-----\n\n(containing the updated Task object) and\nTaskArtifactUpdateEvent (containing\ngenerated Artifact objects).\n4) **Input Required:** If additional input is needed, the server\nsignals this via the response or an SSE event, and the\nclient sends subsequent input using the same taskId.\n5) **Completion:** The task transitions to a terminal state\n(completed, failed, or canceled), communicated via the\nfinal response or an SSE event.\n6) **Push** **Notifications** **(Optional):** Servers supporting\nthe pushNotifications capability can send asynchronous updates to a client-provided webhook URL,\nregistered using tasks.pushNotification.set.\n\n*C. Discoverability Mechanism*\n\nA central innovation of the A2A protocol is its robust\ndiscoverability mechanism, which enables agents to efficiently\nlocate and leverage each other’s capabilities.\n*1) AgentCard Structure:* The AgentCard serves as the foundation of the A2A protocol’s discoverability mechanism. This\nstructured JSON metadata file, located at the standardized path\n/.well-known/agent.json, functions as a machinereadable ”business card” describing an agent’s capabilities\nand interfaces. The AgentCard contains:\n\n*•* Basic agent identification (name, version, provider).\n\n*•* HTTPS endpoint URL for A2A communication\n(a2aEndpointUrl).\n\n*•* Detailed function catalogs with parameter schemas.\n\n*•* Required authentication methods using OpenAPI 3.x Security\nScheme objects (e.g., apiKey, http bearer, oauth2,\nopenIdConnect).\n\n*•* Capability descriptions that define available functions.\n\n*•* Hosted/DNS information detailing accessibility.\nThis standardized location creates a predictable discovery\npattern across the entire A2A ecosystem, similar to how\nrobots.txt and sitemap.xml function for web crawlers.\nThe AgentCard’s JSON structure ensures both human readability and machine parsability, facilitating seamless integration\nbetween disparate agent systems.\n\n*D. Core Concepts*\n\nThe A2A protocol is built around several fundamental\nconcepts that structure agent interactions:\n*1) AgentCard:* As described earlier, the Agent Card is a\npublic JSON metadata file that acts as a machine-readable\nbusiness card. The quality and accuracy of Agent Cards can\nvary, which directly impacts discovery reliability and security\nposture advertisement.\n*2) A2A Server:* An agent exposing an HTTPS endpoint that\nimplements the A2A JSON-RPC methods. It receives requests,\nmanages task execution, and sends responses/updates.\n*3) A2A Client:* An application or another agent that consumes A2A services by sending JSON-RPC requests to a\nserver’s a2aEndpointUrl . Clients can dynamically add\nand remove servers after launch, offering flexibility in how\ncapabilities are discovered and utilized during runtime.\n\n\n*4) Task:* The fundamental unit of work in the A2A protocol, identified by a unique taskId provided by the client.\nTasks progress through a defined lifecycle represented by\nTaskStatus values:\n\n*•* TASK_STATUS_SUBMITTED\n\n*•* TASK_STATUS_WORKING\n\n*•* TASK_STATUS_INPUT_REQUIRED\n\n*•* TASK_STATUS_COMPLETED\n\n*•* TASK_STATUS_FAILED\n\n*•* TASK_STATUS_CANCELED\nThe protocol supports both short-lived request/response style\ninteractions and long-running asynchronous tasks, making it\nsuitable for complex workflows that may extend over days or\nweeks.\n\n*5) Message:* Represents a turn in the communication dialogue, containing a role (”user” for client-originated, ”agent”\nfor server-originated) and one or more Parts.\n*6) Part:* The basic unit of content within a Message or\nArtifact. Defined types include:\n\n*•* TextPart: For unstructured plain text communication.\n\n*•* FilePart: For transferring files, either via inline Base64\nencoded bytesContent or a URI reference.\n\n*•* DataPart: For structured JSON data, identified by a\nmimeType (e.g., application/json).\n*7) Artifact:* Represents outputs generated by the agent\nduring task execution (e.g., files, reports, structured data), also\ncontaining Parts.\n\nIV. T HREAT M ODELING G OOGLE A2A BASED A GENTIC AI\n\nA PPS WITH MAESTRO\n\nThis section will focus on typical agentic AI applications\nbuilt using Google A2A Applications. We will use the MAESTRO threat modeling framework [1] and the work documented\nin [2] to build application-specific threats for Agentic AI\napplications built using the Google A2A protocol. The next\nsection will dive deep into strategies to mitigate the threats.\n\n*A. Recap of MAESTRO Threat Modeling Methodology*\n\nTraditional threat modeling frameworks often fall short when\napplied to agentic AI systems. These systems can autonomously\nmake decisions, interact with external tools, and learn over\ntime – capabilities that introduce unique security risks. That’s\nwhy we’ll use the MAESTRO framework, a seven-layer\nthreat modeling approach specifically designed for agentic AI.\nMAESTRO offers a more granular and proactive methodology\nuniquely suited for the complexities of agentic systems like\nthose built using A2A. MAESTRO (Multi-Agent Environment,\nSecurity, Threat, Risk, and Outcome) provides a structured,\ngranular, and proactive methodology for identifying, assessing,\nand mitigating threats across the entire agentic AI lifecycle.\nMAESTRO in a Nutshell:\n\n*•* Extends Existing Frameworks: Builds upon established\nsecurity frameworks like STRIDE, PASTA, and LINDDUN,\nbut adds AI-specific considerations.\n\n*•* Layered Security: Recognizes that security must be addressed\nat every layer of the agentic architecture.\n\n\n-----\n\nFig. 1. Maestro Architecture - 7 Layers\n\n*•* AI-Specific Threats: Focuses on the unique threats arising\nfrom AI, such as adversarial machine learning and the risks\nof autonomous decision-making.\n\n*•* Risk-Based Approach: Prioritizes threats based on their\nlikelihood and potential impact.\n\n*•* Continuous Monitoring: Emphasizes the need for ongoing\nmonitoring and adaptation.\nThe Seven Layers of MAESTRO (See Figure 1):\n\n1) Foundation Models: The core AI models (e.g., LLMs)\nused by the agents.\n2) Data Operations: The data used by the agents, including\nstorage, processing, and vector embeddings.\n3) Agent Frameworks: The software frameworks and APIs\nthat enable agent creation and interaction (like the A2A\nprotocol).\n4) Deployment and Infrastructure: The underlying infrastructure (servers, networks, containers) that hosts the agents\nand API.\n\n5) Evaluation and Observability: The systems used to monitor, evaluate, and debug agent behavior.\n6) Security and Compliance: The security controls and\ncompliance measures that protect the entire system.\n7) Agent Ecosystem: The environment where multiple agents\ninteract, including marketplaces, collaborations, and potential conflicts.\n\n*B. Common A2A Multi-Agent System Threats*\n\nLeveraging MAESTRO threat modeling methodology, we\nidentified potential threats for A2A multi-agent systems, which\nare illustrated in Figure 2 and detailed below:\n*1) Agent Card Spoofing:* **MAESTRO Layers: 3 (Agent**\n**Frameworks), 4 (Deployment & Infrastructure)**\nAn attacker publishes a forged /.well-known/agent.json (Agent\nCard) at a malicious or typosquatting domain. When an A2A\nClient performs agent discovery, it may trust this fake card and\nsend sensitive A2A Tasks to a rogue A2A Server. This can result\nin task hijacking, data exfiltration, and agent impersonation.\n*2) A2A Task Replay:* **MAESTRO Layers: 3 (Agent Frame-**\n**works), 2 (Data Operations)** If an attacker captures a valid\ntasks/send request and replays it to the A2A Server, the\n\n\nsame A2A Task may be executed multiple times. Without replay\nprotection, this leads to duplicate or unauthorized actions.\n*3) A2A Message Schema Violation:* **MAESTRO Layer:**\n**2 (Data Operations)** A malicious A2A Client may craft\nmalformed A2A Messages or Parts to exploit weak schema\nvalidation in the A2A Server, potentially causing code injection,\nprivilege escalation, or denial of service.\n*4) A2A Server Impersonation:* **MAESTRO Layer: 4**\n**(Deployment & Infrastructure)** Through DNS spoofing or\nnetwork attacks, an adversary redirects A2A Client traffic to a\nfake A2A Server. The attacker can serve forged Agent Cards\nand Task results, undermining trust and stealing data.\n*5) Cross-Agent Task Escalation:* **MAESTRO Layers: 7**\n**(Agent Ecosystem), 3 (Agent Frameworks)** A malicious agent\nenumerates available Agent Cards and attempts to escalate\nprivileges by submitting A2A Tasks with forged credentials,\nbreaching trust boundaries and accessing unauthorized data.\n*6) Artifact Tampering via A2A Artifacts:* **MAESTRO Lay-**\n**ers: 2 (Data Operations), 3 (Agent Frameworks)** Attackers\nintercept or modify Artifacts exchanged during A2A Task\nexecution, injecting malicious content or corrupting results.\n*7) Insider Threat/Logging Evasion via A2A Task Manipu-*\n*lation:* **MAESTRO Layers: 6 (Security & Compliance), 3**\n**(Agent Frameworks)** A privileged user or agent manipulates\nTask state transitions or disables logging on the A2A Server,\nconcealing unauthorized actions.\n*8) Supply Chain Attack via A2A Dependencies:* **MAESTRO**\n**Layers: 4 (Deployment & Infrastructure), 6 (Security &**\n**Compliance)** A compromised or vulnerable dependency in the\nA2A Server or Client can enable remote code execution or\n\ncredential theft.\n\n*9) Authentication & Identity Threats:* **MAESTRO Layers:**\n**3 (Agent Frameworks), 4 (Deployment & Infrastructure),**\n**6 (Security & Compliance), 7 (Agent Ecosystem)** A2Abased systems use OAuth/OIDC with JWT tokens for identity\nvalidation. Threats include:\n\n*•* Forged or stolen JWT tokens allowing unauthorized access\nto A2A Tasks or Agent Cards.\n\n*•* Weak JWT validation (e.g., missing signature check, improper audience/issuer claims).\n\n*•* Token replay or use of expired tokens.\n\n*•* Insecure storage or transmission of tokens.\n*10) Poisoned AgentCard:* **MAESTRO Layers: 1 (Founda-**\n**tion Model), 2 (Data Operations)** An attacker embeds malicious instructions using prompt injection techniques within the\nfields of an AgentCard (e.g., AgentSkill id, name, descriptions,\ntags or examples). When another agent automatically ingests\nand processes this AgentCard data as part of its discovery\nor interaction flow, it may execute these hidden instructions.\nThis exploits the trust placed in AgentCard content (Data\nOperations) and the automated processing of this content using\nthe foundation model during its planning. As a result, the\nagent’s goals being hijacked, sensitive data being revealed, or\ninternal security protocols being bypassed, highlighting the\nneed to treat AgentCard content as untrusted input requiring\ncareful validation and sanitization.\n\n\n-----\n\nFig. 2. List of Common A2A Multi-Agent System Threats Identified by MAESTRO Threat Modeling Methodology\n\n\n*C. Additional Security Considerations*\n\nIn addition to the above specific controls, we think following\nadditional security aspects should be considered:\n\n*•* Supply chain dependencies must be scanned and verified.\n(Layers 4, 6)\n\n*•* Incident response and recovery plans must be in place.\n(Layers 6, 7)\n\n*•* Use open-source libraries for validation, authentication, and\nmonitoring. (Layers 2, 3, 6)\n\n*•* Prefer declarative security configurations (YAML/JSON) for\ninfrastructure. (Layer 4)\n\n*•* Integrate security testing (unit, integration, adversarial) into\nCI/CD pipelines. (Layers 3, 6)\n\n*•* Document all agent capabilities and trust boundaries in the\nAgent Card. (Layers 3, 7)\n\nV. C ASE S TUDY\n\nWe will consider two case studies to understand the threat\n\nmodeling of the agentic system.\n\n*A. Case Study 1: Collaborative Document Processing*\n\nIn this scenario, multiple A2A Clients (from different\nvendors) discover and interact with an enterprise A2A Server to\nco-edit, summarize, and review documents. Each client retrieves\nthe Agent Card, authenticates, and launches A2A Tasks via\ntasks/send.\n\n1) **Layer 1: Foundation Models** — Prompt injection attacks\ncan occur when adversarial input is embedded in A2A\nMessage Parts, causing the LLM to behave unexpectedly.\n2) **Layer 2: Data Operations** — Attackers may leak\nsensitive data through A2A Artifacts or tamper with A2A\nTask state.\n\n3) **Layer 3: Agent Frameworks** — Vulnerable to Agent\nCard spoofing and replayed tasks/send requests, especially\nif malformed Agent Cards are accepted.\n4) **Layer 4: Deployment & Infrastructure** — Risks include\ndenial-of-service attacks via flooding the A2A Server\nwith tasks, or lateral movement using compromised Agent\nCards.\n\n\n5) **Layer 5: Evaluation & Observability** — Log tampering\nor insufficient auditing of A2A Task transitions can let\nattacks go undetected.\n6) **Layer 6: Security & Compliance** — Credential theft\nfrom Agent Cards or bypassing policies via misconfigured\nfields.\n7) **Layer 7: Agent Ecosystem** — Enumeration of Agent\nCards and Sybil attacks with fake cards can undermine\ntrust in federated A2A deployments.\n\n*1) Cross-Layer Vulnerabilities:*\n\n*•* Compromised Agent Card enables attacker to hijack Task\nexecution (Layer 3 *→* Layer 2).\n\n*•* Weak authentication on A2A Server allows replayed Task\nrequests (Layer 3 *→* Layer 6).\n\n*•* Insufficient logging of Task state changes enables undetected\nattacks (Layer 5 *→* Layer 7).\n*2)* ***Risk Assessment*** *:*\n\n*•* Likelihood: **High** (open Agent Card discovery, multi-vendor\nfederation)\n\n*•* Impact: **High** (data loss, compliance breach, reputational\ndamage)\n\n*B. Case Study 2: Distributed Data Analysis*\n\nHere, A2A Clients in different departments analyze sensitive\ndatasets by launching A2A Tasks to a central A2A Server,\naggregating results via Artifacts.\n\n1) Layer 1: Foundation Models — Model inversion attacks\nmay occur when adversarial input is embedded in A2A\nMessage Parts, allowing attackers to extract sensitive data\nfrom LLMs.\n\n2) Layer 2: Data Operations — Data poisoning in A2A\nArtifacts, unauthorized aggregation of Task results, tool\npoisoning, agent card poisoning, or tampering with distributed Task state are key threats.\n3) Layer 3: Agent Frameworks — Susceptible to Task replay\nvia tasks/send, Agent Card spoofing, and schema violations\nin Task or Message objects.\n4) Layer 4: Deployment & Infrastructure — Risks include\nnetwork eavesdropping on A2A Task traffic and A2A\nServer compromise.\n\n\n-----\n\n5) Layer 5: Evaluation & Observability — Insufficient\nanomaly detection in Task audit logs or log forgery in\nTask status events can enable undetected attacks.\n\n6) Layer 6: Security & Compliance — Data privacy violations due to misconfigured Agent Cards or weak\nencryption of Task data.\n7) Layer 7: Agent Ecosystem — Data silo bridging via Agent\nCard enumeration and policy conflicts between federated\nA2A Servers can lead to unauthorized data flows.\n\n*1) Cross-Layer Vulnerabilities::*\n\n*•* Poisoned A2A Artifacts corrupting analytics (Layer 2 *→*\nLayer 1)\n\n*•* Weak Task orchestration enabling replay or hijacking (Layer\n3 *→* Layer 4)\n\n*•* Cross-department Agent Card trust failures (Layer 7 *→* Layer\n6)\n*2) Risk Assessment:*\n\n*•* Likelihood: **Medium-High** (complexity, distributed trust)\n\n*•* Impact: **High** (business intelligence compromise, regulatory\nrisk)\n\n*C. Threat Evolution*\n\nThreats to A2A-based multi-agent systems evolve as the\nprotocol, Agent Card registry, and deployment patterns change.\nRegular threat modeling via the MAESTRO methodology, and\nupdates to identified threats, are essential to address new attack\ntechniques and changes in the A2A ecosystem.\n\nVI. M ITIGATING S ECURITY T HREATS IN A2A-B ASED\n\nM ULTI -A GENT S YSTEMS\n\nDrawing from the MAESTRO threat modeling methodology,\nSection- IV-B outlined several potential threats facing A2Abased MAS. This section delves into specific security controls\nand best practices to address these threats, as well as additional\nsecurity considerations from Section- IV-C . Moreover, it recontextualizes the case studies of Sections- V-A and V-B in\n\nlight of those mitigations.\n\n*A. Addressing Specific Threats and Enhancing Security*\n\nThe following subsections detail mitigation strategies for\neach threat identified in Section- IV-B, incorporating the additional considerations of Section-IV-C:\n\n*1) Addressing Agent Card Spoofing:* Addressing SectionIV-B 1 Agent Card Spoofing, where an attacker publishes a\nforged /.well-known/agent.json (Agent Card) at a malicious domain, poses a significant risk of task hijacking, data exfiltration,\nand agent impersonation.\n**Mitigation Strategies:**\n\n*•* **Digital Signatures on Agent Cards:** Use digital signatures\nvia a trusted Certificate Authority (CA) to ensure authenticity\nand integrity.\n\n*•* **Secure Agent Card Resolution:** Use HTTPS with certificate\nvalidation and optionally, certificate pinning.\n\n*•* **Agent Card Registry and Validation:** Use a trusted registry\nor directory for validation.\n\n\n\n*•* **Reputation-Based Trust:** Implement a reputation system\nfor rating Agent Cards.\n\n*•* **Agent Card Sanitization:** Sanitize AgentCard content before\nusing it with Foundational Models.\n*2) Preventing A2A Task Replay:* Addressing Section- IV-B 2\n**Mitigation Strategies:**\n\n*•* Include a unique **nonce** in each tasks/send request.\n\n*•* Use **timestamp verification** with an acceptable time window.\n\n*•* Use **Message Authentication Codes (MACs)** .\n\n*•* Design tasks to be **idempotent** .\n\n*•* Implement **session management** to track tasks.\n*3) Preventing A2A Message Schema Violations:* Addressing\nSection-IV-B3\n\n**Mitigation Strategies:**\n\n*•* Enforce **strict schema validation** .\n\n*•* Sanitize all input from clients.\n\n*•* Use **Content Security Policies (CSP)** .\n\n*•* Execute tasks with **least privilege** .\n*4) Preventing A2A Server Impersonation :* Addressing\nSection-IV-B4\n\n**Mitigation Strategies:**\n\n*•* Use **Mutual TLS (mTLS)** .\n\n*•* Use **DNSSEC** to protect against spoofing.\n\n*•* Implement **certificate pinning** .\n\n*•* Apply **monitoring and intrusion detection** .\n*5) Preventing Cross-Agent Task Escalation:* Addressing\nSection-IV-B5\n\n**Mitigation Strategies:**\n\n*•* Enforce strict **authentication and authorization** .\n\n*•* **Validate credentials** for every task.\n\n*•* Implement **audit logging** .\n\n*•* Follow **least privilege** principles.\n\n*•* Use **secure discovery** for Agent Cards.\n*6) Mitigating Artifact Tampering via A2A Artifacts:* Addressing Section-IV-B6\n**Mitigation Strategies:**\n\n*•* Use **digital signatures** on artifacts.\n\n*•* Apply **hashing and checksums** .\n\n*•* Use **encryption** .\n\n*•* Ensure **secure storage and transmission** .\n*7) Preventing Insider Threat/Logging Evasion via A2A Task*\n*Manipulation:* Addressing Section-IV-B7\n**Mitigation Strategies:**\n\n*•* Implement **RBAC** .\n\n*•* Apply **audit logging with integrity checks** .\n\n*•* Ensure **separation of duties** .\n\n*•* Conduct **security audits** .\n\n*•* Require **MFA** for privileged roles.\n*8) Addressing Supply Chain Attacks via A2A Dependencies:*\nAddressing Section-IV-B8\n**Mitigation Strategies:**\n\n*•* Use **dependency scanning** .\n\n*•* Apply **dependency pinning** .\n\n*•* Generate and maintain a **Software Bill of Materials**\n\n**(SBOM)** .\n\n*•* Conduct **vendor security assessments** .\n\n\n-----\n\n*•* Follow **secure development practices** .\n*9) Mitigating Authentication & Identity Threats:* Addressing\nSection-IV-B9\n\n**Mitigation Strategies:**\n\n*•* Apply **strong JWT validation** .\n\n*•* Use **secure token storage** .\n\n*•* Implement **token rotation** .\n\n*•* Use **mTLS for API access** .\n\n*•* Follow **OAuth 2.0 best practices**, including PKCE.\n*10) Mitigating Poisoned AgentCard:* Addressing Section\nIV-B10\n\n**Mitigation Strategies:**\n\n*•* Apply **input sanitization** to all AgentCard content.\n\n*•* Use a **whitelist of allowed characters** .\n\n*•* Implement **escaping/encoding** of special characters.\n\n*•* Enforce **Content Security Policy (CSP)** .\n\n*•* Validate structure with **schema checks and type constraints** .\n\n*B. Additional Security Considerations*\n\nAddressing Section-IV-C\n\n*•* Continuously scan and verify supply chain dependencies.\n\n*•* Develop and maintain incident response and recovery plans.\n\n*•* Evaluate security posture of open-source libraries.\n\n*•* Use declarative security configurations (YAML/JSON).\n\n*•* Integrate security testing into CI/CD pipelines.\n\n*•* Document capabilities and trust boundaries in Agent Cards.\n\n*C. Applying Mitigations to Case Studies (Sections* *V-A* *and*\n*V-B)*\n\n*1) Case Study 1: Collaborative Document Processing:*\n\n*•* Digitally sign all documents.\n\n*•* Enforce granular access control.\n\n*•* Apply DLP techniques.\n\n*•* Sanitize Agent Cards before using with FMs.\n\n*•* Validate and authenticate all task submissions.\n\n*2) Case Study 2: Distributed Data Analysis:*\n\n*•* Implement differential privacy.\n\n*•* Use federated learning.\n\n*•* Apply secure multi-party computation (SMPC).\n\n*•* Sanitize Agent Cards before use.\n\n*•* Apply strict audit controls on artifact aggregation.\n\nVII. S ECURE A PPLICATION D EVELOPMENT S TRATEGIES\n\n*A. Securely Implementing with Current A2A Features*\n\nThis involves endpoint hardening, augmenting authentication\nmechanisms beyond the basic requirements if necessary, and\nrigorous input/output validation.\n\n*B. Leveraging Enhanced A2A Features for Advanced Security*\n\nFuture enhancements or complementary technologies could\ninclude DID-based authentication, granular policy enforcement\nframeworks, and mechanisms for establishing data provenance.\n\n\nVIII. K EY C ONTROL M EASURES FOR S AFE A2A\n\nD EPLOYMENT\n\nThis section covers essential controls for deploying A2Abased applications securely:\n\n*•* Endpoint security (TLS, network controls).\n\n*•* Strong authentication and authorization.\n\n*•* Comprehensive input validation and sanitization.\n\n*•* Principle of least privilege for agent capabilities and data\n\naccess.\n\n*•* Robust monitoring, logging, and alerting.\n\n*•* Secure software development lifecycle (SSDLC) practices.\n\n*•* Incident response planning and execution.\nFor secure A2A coding examples, please refer to the Github\nrepository [2] .\n\nIX. I MPLEMENTING A2A S ERVER S ECURELY\n\nIn this section, we will focus on suggesting specific security\ncontrols to implement for the A2A Server deployment, aiming\nto enhance its resilience against potential threats. A high-level\noverview of these threats is illustrated in Figure 3.\n\n*A. Securing the AgentCard*\n\nThe AgentCard ( /.well-known/agent.json ) is a\ncritical security element as it exposes information about your\nagent’s capabilities and authentication requirements.\n\n*B. Securing the AgentCard*\n\nThe AgentCard is a critical security element in the A2A\nprotocol as it exposes information about an agent’s capabilities\nand authentication requirements.\n\n1) **AgentCard Location and Access Controls**\n\n*•* Host the AgentCard file\n(/.well-known/agent.json) with appropriate\naccess controls.\n\n*•* Implement rate limiting to prevent enumeration attacks.\n\n*•* Consider using content security headers to prevent\nunauthorized embedding or framing.\n2) **AgentCard Content Security**\n\n*•* Include only necessary information about your agent’s\ncapabilities.\n\n*•* Specify detailed authentication requirements using OpenAPI 3.x Security Scheme objects.\n\n*•* Validate the AgentCard content regularly to ensure it\ndoesn’t expose sensitive information.\n\n*•* Keep authentication details accurate and up to date.\n\n*C. Authentication and Authorization*\n\n*•* **Establish Server Identity:** Authenticate the server’s identity\nvia digital certificates provided by trustworthy Certificate\nAuthorities. Employ TLS to secure connections, enabling\nclients to authenticate the server’s authenticity during the\nhandshake and mitigate man-in-the-middle attacks.\n\n*•* **Declare Authentication Protocols:** Explicitly declare the\nsupported authentication methods (e.g., OAuth, OIDC, API\nKeys) in the Agent Card.\n\n2 [https://github.com/kenhuangus/a2a-secure-coding-examples](https://github.com/kenhuangus/a2a-secure-coding-examples)\n\n\n-----\n\nFig. 3. Best Practices For Secured A2A Server\n\n\n\n*•* **Authenticate Each Request:** Require authentication for each\nincoming HTTP request. Ensure that every request has proper\ncredentials (e.g., tokens, certificates). Reject requests that\nlack the valid credentials by utilizing suitable HTTP status\ncodes (e.g., 401 Unauthorized, 403 Forbidden).\n\n1) **Protecting Sensitive Actions and Data**\nIt is recommended that A2A server will protect sensitive\ninformation by managing authorization to both ‘skills’ and\n‘tools’:\n\n*•* **Skills:** Agents are required to advertise their skills\nthrough an Agent Card, showcasing their expertise. It\nis recommended for agents to grant permission for\neach skill or grant permission on a per-skill basis using\nspecific scopes, enabling different access levels (e.g.,\nread-only skills).\n\n*•* **Tools:** Agents are required to limit access to sensitive\ndata and actions by securing using controlled tools.\nTherefore, when agentic flow requests data, the agent\nwill grant permissions based on this.\n2) **API Keys**\nFor simpler implementations, API keys may be used:\n\n*•* Generate strong, random API keys following cryptographic best practices.\n\n*•* Implement key rotation policies to regularly update API\nkeys.\n\n*•* Store API keys securely and never expose them in\nclient-side code.\n\n3) **JWT Authentication**\nFor stateless authentication:\n\n*•* Implement robust JWT validation including signature\nverification.\n\n*•* Set appropriate token expiration times to limit the impact\nof token theft.\n\n*•* Include only necessary claims in the JWT payload.\n\n*D. Secure Communication*\n\n1) **Transport Layer Security (TLS)**\n\n*•* Enforce HTTPS for all A2A communications with\n\nproper TLS configuration (TLS1.3).\n\n*•* Regularly renew TLS certificates and disable insecure\nciphers.\n2) **Data Protection in Transit**\n\n\n\n*•* Ensure all A2A messages are encrypted during transmission.\n\n*•* Validate certificate chains to prevent man-in-the-middle\nattacks.\n\n*•* Consider implementing certificate pinning for critical\nconnections.\n\n3) **Data Protection at Rest**\n\n*•* Encrypt sensitive data stored by your A2A server.\n\n*•* Secure storage of any persistent data from agent\ninteractions.\n\n*•* Implement proper key management for encryption keys.\n\n*E. Input Validation and Request Processing*\n\n1) **Message Validation**\n\n*•* Validate all incoming messages against the A2A protocol schema.\n\n*•* Implement robust input sanitization to prevent injection\nattacks.\n\n*•* Verify message formats, sizes, and content types before\nprocessing.\n2) **URI Validation**\n\n*•* Strictly validate any URIs included in messages to\nprevent Server-Side Request Forgery (SSRF) attacks.\n\n*•* Implement allow-lists for acceptable URI schemes and\ndomains.\n\n*•* Avoid fetching content from untrusted or user-supplied\nURIs.\n\n3) **Processing File Parts**\nWhen handling FilePart content in A2A messages:\n\n*•* Scan all uploaded files for malware.\n\n*•* Validate file types and enforce size limits.\n\n*•* Store uploaded files securely with appropriate access\ncontrols.\n\n*F. Server Implementation Best Practices*\n\n1) **Error Handling and Logging**\n\n*•* Implement comprehensive error handling that doesn’t\nexpose sensitive information.\n\n*•* Maintain detailed security logs for authentication attempts, authorization decisions, and security events.\n\n*•* Ensure logs are protected and cannot be tampered with.\n\n*•* Implement monitoring and alerting for suspicious activities.\n\n\n-----\n\n2) **Rate Limiting and DoS Protection**\n\n*•* Implement rate limiting on all A2A endpoints to prevent\nabuse.\n\n*•* Consider using exponential backoff for failed authentication attempts.\n\n*•* Monitor for and mitigate denial of service attacks\ntargeting your A2A server.\n3) **Secure Development Practices**\n\n*•* Follow secure coding guidelines specific to your implementation language.\n\n*•* Conduct regular security code reviews.\n\n*•* Implement application security testing as part of your\ndevelopment workflow.\n\n*•* Keep all dependencies updated to address security\nvulnerabilities.\n\n*G. Protocol-Specific Security Considerations*\n\n1) **Streaming and SSE (Server-Sent Events)**\nFor implementations using tasks/sendSubscribe and ServerSent Events (SSE):\n\n*•* Implement proper authentication for SSE connections.\n\n*•* Maintain secure state management for long-lived connections.\n\n*•* Monitor for and mitigate resource exhaustion attacks.\n2) **Push Notifications**\nWhen implementing the optional push notifications capability:\n\n*•* Validate webhook URLs rigorously to prevent SSRF\nattacks.\n\n*•* Implement signature verification for webhook payloads.\n\n*•* Use HTTPS for all webhook communications.\n\n*•* Implement retry policies with appropriate backoff.\n3) **Connection Management and Abuse Prevention**\nEffective connection management is essential to ensure\nthe scalability and resilience of A2A Server streaming\nfeatures such as tasks/sendSubscribe and ServerSent Events (SSE). To prevent abuse and resource exhaustion, the server should enforce various strategies depicted\nbelow.\n\n4) **Connection Quotas:** connection quotas per client or IP,\nclose idle connections through timeout mechanisms, and\nuse periodic keep-alive pings to detect and clean up stale\nsessions.\n\n5) **Connection limits:** Set hard limits on the number of\nactive SSE connections per client ID/IP to prevent resource\nexhaustion.\n\n6) **Idle Timeout & Keep-Alive Pings:** Enforce connection\nidle timeouts. Use periodic keep-alive pings and terminate\nstale sessions.\n\n7) **Backpressure-Aware Streaming:** Drop non-critical event\nmessages for lagging clients or apply backoff strategies\nto prevent memory build-up.\n\n*H. Server Hardening*\n\nWhen deploying A2A server, it is essential to use a hardened\nenvironment following security best practices.\n\n\n\n*•* Implement network segmentation to isolate the A2A server.\n\n*•* Use Web Application Firewalls (WAF) to protect against\ncommon web attacks.\n\n*•* Regularly apply security updates to server components.\n\n*•* Implement comprehensive monitoring of your A2A server.\n\n*•* Develop incident response procedures for security breaches.\n\n*•* Conduct regular security assessments and penetration tests.\n\n*•* Establish a security incident management process.\n\nX. MCP AND A2A: A S YNERGY\n\nThe Agent-to-Agent (A2A) protocol and Model Context\nProtocol (MCP) represent complementary frameworks that\ntogether create a robust foundation for sophisticated agentic\nsystems. Rather than competing standards, these protocols\noperate at different layers of the AI interaction stack, enabling\nboth horizontal coordination between peer agents and vertical\nintegration with specialized tools and data sources. Table I\npresents a comparative analysis of these two critical protocols\nin Agentic AI.\nWhen deployed together, these protocols create an efficient\nhierarchical workflow system. An agent initially receives a\ncomplex task from either a human user or another agent through\nA2A protocols. To successfully complete this assignment, the\nagent often needs specialized capabilities beyond its own scope.\nUsing A2A’s discovery mechanism, it identifies and delegates\nspecific subtasks to purpose-built agents with relevant expertise,\nsuch as Claim Agents or Rental Car Agents. These specialized\nagents then leverage MCP to connect with structured data\nsources, computational tools, or external systems required\nto fulfill their assigned responsibilities. The completed work\nflows back through the agent hierarchy via A2A’s structured\ntask management framework, enabling seamless integration\nof results from multiple specialized systems into a cohesive\nsolution delivered to the original requestor. Figure 4 illustrates\nthe steps in such a use case.\nThe flow between a Claim Agent and Rental Car Agent\nwould utilize the A2A protocol for agent-to-agent coordination\nand task management, where the Claim Agent (acting as the\nMain A2A Client) would discover the Rental Car Agent’s\ncapabilities through the well-known agent registry, authenticate,\nand delegate specific rental-related tasks with unique taskIds.\nMeanwhile, the MCP framework enables each specialized agent\nto extend its capabilities by connecting to external tools and\ndatabases - the Claim Agent might access policy databases\nwhile the Rental Car Agent connects to vehicle availability\nsystems. These frameworks complement each other by allowing\nthe Claim Agent to maintain high-level coordination of the\ninsurance claim process (tracking status updates and compiling\nfinal results for the user) while the Rental Car Agent uses MCP\nto perform specialized functions like querying vehicle inventories and processing rental agreements, ultimately delivering\nan integrated solution where claim processing and rental car\narrangements happen seamlessly within a unified workflow.\nThis architectural approach promotes several key design\nprinciples. It enables functional modularity, allowing organizations to develop specialized agents with deep domain\n\n\n-----\n\nTABLE I\n\nC OMPARATIVE A NALYSIS OF A2A AND MCP\n\n**Feature** **Google Agent2Agent (A2A)** **Anthropic Model Context Protocol (MCP)**\n\nStandardize connection between AI models/agents\n**Purpose** Enable interoperability between diverse AI agents and external tools/data\n\nAgent-to-Agent collaboration, delegation, Agent-to-Tool/Resource access, context\n**Focus**\nmessaging provisioning\n\nMCP Client (Agent/Host) *↔* MCP Server\n**Primary Interaction** Client Agent *↔* Remote Agent\n(Tool/Data)\n\nAgent Cards (discovery), Task object (lifecycle), Tools, Resources, Prompts (exposed by server),\n**Key Mechanisms**\nMessages, Artifacts Client-Server requests\n\nHorizontal Integration Vertical Integration\n**Ecosystem Role**\n(Agent Network Communication) (Agent Capability Enhancement)\n\nFig. 4. End to End Agents collaboration utilizing A2A and MCP\n\n\n-----\n\nexpertise. It supports compositional flexibility, as different agent\ncombinations can be assembled to address diverse problems. It\nmaintains clear separation of concerns between peer-level agent\ncoordination and tool-level resource access. Most importantly,\nit creates an extensible ecosystem where new capabilities can\nbe added incrementally without redesigning the entire system.\nThe multi-protocol architecture introduces important security\nconsiderations at integration boundaries. Potential attack vectors\nmay cross protocol boundaries—for example, an authentication\nvulnerability in the A2A layer could be exploited to gain\nunauthorized MCP access, while sensitive information exposed\nthrough inadequately protected MCP connections might enable\nsubsequent A2A-based compromises. Therefore, robust security\nstrategies must address not only each protocol individually\nbut also the critical interfaces where they interconnect. This\ncomprehensive approach ensures continuous protection throughout the entire agent workflow chain, with special attention to\nauthentication token handling, task delegation permissions, and\nsecure data transmission between the Claim Agent, Rental Car\nAgent, and their respective external systems. [19]\n\nXI. C ONCLUSION\n\n*A. Recap of A2A’s Role, Security Challenges, and Enhancement*\n*Potential*\n\nThe Agent-to-Agent (A2A) protocol plays a foundational\nrole in enabling secure, composable, and scalable multi-agent\nsystems. By standardizing how agents discover, authenticate, and communicate with one another, A2A facilitates a\nnew paradigm of autonomous collaboration—ranging from\ndocument co-authoring agents to federated analytics across\ndistributed datasets. Its declarative nature and emphasis on\nexplicit capabilities (via AgentCards) empower developers to\nbuild trust-aware, modular workflows that reflect real-world\nboundaries and roles. However, with these capabilities come\nsignificant security challenges. The distributed and dynamic\nnature of multi-agent systems introduces threats. Furthermore,\nnovel risks such as prompt injection into AgentCards and\ncross-agent escalation highlight the need for context-aware\nthreat modeling—such as that provided by the MAESTRO\nframework. To realize the full potential of A2A-based systems,\nsecurity must be deeply integrated into both protocol design and\nserver implementation. From strong cryptographic controls and\nzero-trust agent authentication to schema validation, logging\nintegrity, and resilient streaming protocols, robust security\ncontrols are essential. As multi-agent ecosystems grow in\ncomplexity, future enhancements to A2A should emphasize\nadaptive trust, continuous policy enforcement, and secure-bydefault configurations. By proactively addressing today’s threats\nand anticipating tomorrow’s, A2A can serve as a resilient\nbackbone for intelligent, secure, and trustworthy agentic AI\napplications.\n\n*B. Future Directions for Secure Agent Collaboration*\n\nLooking ahead, the secure collaboration of autonomous\nagents hinges on continued progress in protocol standardization and the widespread adoption of security best practices.\n\n\nInitiatives like A2A and MCP are crucial, but their evolution\nmust prioritize security considerations, potentially incorporating\nlessons from Zero Trust architectures adapted for agentspecific contexts. Future work should focus on developing\nstandardized methods for expressing and enforcing complex\nauthorization policies between agents, establishing robust\nmechanisms for agent identity verification and reputation\nmanagement, and creating frameworks for secure multi-agent\ncoordination, especially when crossing administrative or trust\nboundaries. Furthermore, addressing the security implications\nof integrating Large Language Models within agentic workflows\nand ensuring the resilience of agent communication against\nsophisticated attacks will be paramount for building trustworthy\nand reliable multi-agent systems. The development of comprehensive threat models, like MAESTRO [1], and shared\nsecurity implementation guidelines will be vital for guiding\ndevelopers in building the next generation of secure agentic\nAI applications.\n\nR EFERENCES\n\n[1] K. Huang, “Agentic AI threat modeling framework:\nMAESTRO,” [https://cloudsecurityalliance.org/blog/2025/02/06/](https://cloudsecurityalliance.org/blog/2025/02/06/agentic-ai-threat-modeling-framework-maestro)\n[agentic-ai-threat-modeling-framework-maestro, Cloud Security Alliance,](https://cloudsecurityalliance.org/blog/2025/02/06/agentic-ai-threat-modeling-framework-maestro)\nfeb 2025.\n\n[2] [K. Huang and I. Habler, “Threat modeling Google’s A2A protocol,” https:](https://kenhuangus.substack.com/p/threat-modeling-googles-a2a-protocol)\n[//kenhuangus.substack.com/p/threat-modeling-googles-a2a-protocol,](https://kenhuangus.substack.com/p/threat-modeling-googles-a2a-protocol)\n2025.\n\n[3] T. Finin, R. Fritzson, D. McKay, and R. McEntire, “KQML\nas an agent communication language,” in *Proceedings* *of* *the*\n*Third* *International* *Conference* *on* *Information* *and* *Knowledge*\n*Management* . ACM, 1994, pp. 456–463. [Online]. Available:\n[https://dl.acm.org/doi/10.1145/191246.191322](https://dl.acm.org/doi/10.1145/191246.191322)\n\n[4] Y. Labrou and T. Finin, “History, state of the art and challenges\nfor agent communication languages,” *Informatik/Informatique*,\n[2000. [Online]. Available: https://ebiquity.umbc.edu/paper/html/id/231/](https://ebiquity.umbc.edu/paper/html/id/231/History-State-of-the-Art-and-Challenges-for-Agent-Communication-Languages)\n[History-State-of-the-Art-and-Challenges-for-Agent-Communication-Languages](https://ebiquity.umbc.edu/paper/html/id/231/History-State-of-the-Art-and-Challenges-for-Agent-Communication-Languages)\n\n[5] [FIPA.org, “FIPA agent communication language specifications,” http:](http://www.fipa.org/repository/aclspecs.html)\n[//www.fipa.org/repository/aclspecs.html, n.d.](http://www.fipa.org/repository/aclspecs.html)\n\n[6] B. Chaib-draa and F. Dignum, “Trends in agent communication\nlanguage,” *Computational Intelligence*, vol. 18, no. 2, pp. 89–101,\n[2002. [Online]. Available: https://onlinelibrary.wiley.com/doi/abs/10.](https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-8640.00184)\n[1111/1467-8640.00184](https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-8640.00184)\n\n[7] B. Gaudou, A. Herzig, D. Longin, and M. Nickles, “A new semantics\nfor the FIPA agent communication language based on social attitudes,”\nin *ECAI 2006: 17th European Conference on Artificial Intelligence* .\n[IOS Press, 2006, pp. 245–249. [Online]. Available: https://www.irit.fr/](https://www.irit.fr/publis/LILAC/Conf_internationales/2006_Gaudou_et_al_ECAI.pdf)\npublis/LILAC/Con f [internationales/200](https://www.irit.fr/publis/LILAC/Conf_internationales/2006_Gaudou_et_al_ECAI.pdf) 6 Gaudo u e t al ECAI.pdf\n\n[8] Google Developer Blog, “Announcing the agent2agent\nprotocol (A2A),” [https://developers.googleblog.com/en/](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/)\n[a2a-a-new-era-of-agent-interoperability/, 2025.](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/)\n\n[9] A. Dorri, S. S. Kanhere, and R. Jurdak, “Multi-agent systems: A survey,”\n*IEEE Access*, vol. 6, pp. 28 573–28 593, 2018. [Online]. Available:\n[https://ieeexplore.ieee.org/document/8352646/](https://ieeexplore.ieee.org/document/8352646/)\n\n[10] C. Zhu, M. Dastani, and S. Wang, “A survey of multi-agent deep\nreinforcement learning with communication,” *Autonomous Agents*\n*and Multi-Agent Systems*, vol. 38, no. 1, 2024. [Online]. Available:\n[https://link.springer.com/article/10.1007/s10458-023-09633-6](https://link.springer.com/article/10.1007/s10458-023-09633-6)\n\n[11] Google Developer Blog, “[specific blog post title - placeholder],” [Specific\nURL - Placeholder], 2024.\n\n[12] M. Bellare and P. Rogaway, “Optimal asymmetric encryption,” *Journal*\n*of Cryptology*, vol. 9, no. 2, pp. 137–159, 1995.\n\n[13] JADE Documentation, “Security extensions,” TILab, Telecom Italia.\n\n[14] OASIS, “Web services security: SOAP message security 1.0 (WS-Security\n2004),” 2004.\n\n[15] D. Hardt, “The OAuth 2.0 authorization framework,” IETF, RFC 6749,\n[October 2012. [Online]. Available: https://tools.ietf.org/html/rfc6749](https://tools.ietf.org/html/rfc6749)\n\n\n-----\n\n[16] M. B. Jones, J. Bradley, and N. Sakimura, “JSON web token\n[(JWT),” IETF, RFC 7519, May 2015. [Online]. Available: https:](https://tools.ietf.org/html/rfc7519)\n[//tools.ietf.org/html/rfc7519](https://tools.ietf.org/html/rfc7519)\n\n[17] S. Rose, O. Borchert, S. Mitchell, and S. Connelly, “Zero trust\narchitecture,” National Institute of Standards and Technology, NIST\nSpecial Publication 800-207, August 2020. [Online]. Available:\n[https://doi.org/10.6028/NIST.SP.800-207](https://doi.org/10.6028/NIST.SP.800-207)\n\n[18] N. Ward and Y. He, “Adopting BeyondCorp for multi-agent AI in\nenterprise networks,” 2019.\n\n[19] V. S. Narajala and I. Habler, “Enterprise-Grade Security for\nthe Model Context Protocol (MCP): Frameworks and Mitigation\nStrategies,” *arXiv preprint arXiv:2504.08623*, 2025. [Online]. Available:\n[https://arxiv.org/abs/2504.08623](https://arxiv.org/abs/2504.08623)\n\n\n-----"
]